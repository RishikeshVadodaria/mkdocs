{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#final-year-engineering-notes","title":"Final Year Engineering Notes \ud83d\udcda","text":"<p>\ud83d\ude80 Comprehensive Study Materials for Engineering Students </p> <p>Welcome to my Final Year Engineering Notes Repository, a well-organized collection of study materials designed to help students grasp complex engineering concepts with ease. Whether you're preparing for exams or looking for structured reference materials, you'll find everything you need here.  </p>"},{"location":"#why-this-repository","title":"Why This Repository?","text":"<p>\u2705 Well-Structured &amp; Organized \u2013 Notes are categorized for easy navigation. \u2705 Concise &amp; Clear \u2013 Key concepts are explained in a straightforward manner. \u2705 Linked References \u2013 Includes hyperlinks for additional context and further reading. \u2705 Visual Learning \u2013 Diagrams and Mermaid.js flowcharts enhance understanding. \u2705 Open to All \u2013 Publicly accessible for students worldwide.  </p>"},{"location":"#best-practices-in-documentation","title":"Best Practices in Documentation \ud83d\udee0\ufe0f","text":"<p>To ensure high-quality notes, the following principles are followed: \ud83d\udccc Audience-Focused \u2013 Tailored for engineering students and self-learners. \ud83d\udccc Concise &amp; Structured \u2013 Bullet points and headings improve readability. \ud83d\udccc Hyperlinked Content \u2013 Easy navigation with internal and external links. \ud83d\udccc Technical Writing Standards \u2013 Adheres to best practices in documentation.  </p>"},{"location":"#enhance-your-technical-writing-skills","title":"Enhance Your Technical Writing Skills \u270d\ufe0f","text":"<p>Want to write better documentation? Consider Google\u2019s free technical writing courses, covering: \u2705 Writing clear, concise sentences. \u2705 Using active voice for better readability. \u2705 Markdown best practices for formatting.  </p>"},{"location":"#explore-the-notes-repo","title":"Explore the Notes Repo! \ud83d\udd0d","text":"<p>Click below to access the notes and start learning: \ud83d\udcc2 View Notes{:target=\"_blank\"}</p>"},{"location":"RL_Unit1/","title":"RL Unit1","text":""},{"location":"RL_Unit1/#unit-1","title":"Unit 1","text":"<pre><code>graph LR\n    subgraph Input\n    A[Data with Labels] --&gt; B[Supervised Learning]\n    C[Data without Labels] --&gt; D[Unsupervised Learning]\n    E[States + Actions] --&gt; F[Reinforcement Learning]\n    end\n\n    B --&gt; G[Mapping]\n    D --&gt; H[Classes]\n    F --&gt; I[Action]\n\n    B -.-&gt;|Error| B\n    F -.-&gt;|Reward| F</code></pre>"},{"location":"RL_Unit1/#supervised-learning","title":"Supervised Learning","text":"<p>Definition: Learning from labeled data where the model maps inputs to outputs.</p> <ul> <li>Input: Labeled data (features + target labels)</li> <li>Process: Learning from labeled examples</li> <li>Output: Prediction model</li> <li>Feedback: Error measurement against known labels</li> </ul>"},{"location":"RL_Unit1/#applications","title":"Applications","text":"<ul> <li>Classification: Spam detection, Image recognition</li> <li>Regression: Price prediction, Sales forecasting</li> </ul>"},{"location":"RL_Unit1/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Definition: Learning patterns from unlabeled data.</p> <ul> <li>Input: Unlabeled data</li> <li>Process: Pattern/structure discovery</li> <li>Output: Data grouping/structure</li> <li>Feedback: Internal validation metrics</li> </ul>"},{"location":"RL_Unit1/#applications_1","title":"Applications","text":"<ul> <li>Clustering: Customer segmentation</li> <li>Dimensionality Reduction: Feature extraction</li> </ul>"},{"location":"RL_Unit1/#reinforcement-learning-rl","title":"Reinforcement Learning (RL)","text":"<p>Definition: Learning through trial and error to maximize rewards.</p> <ul> <li>Input: States + possible actions</li> <li>Process: Trial and error learning</li> <li>Output: Action policy</li> <li>Feedback: Rewards/penalties</li> </ul>"},{"location":"RL_Unit1/#applications_2","title":"Applications","text":"<ul> <li>Game AI: Chess, Go</li> <li>Robotics: Navigation, Control</li> </ul>"},{"location":"RL_Unit1/#common-algorithms","title":"Common Algorithms","text":"Learning Type Algorithms Supervised Linear Regression, Random Forest, Neural Networks Unsupervised K-means, PCA, Autoencoders Reinforcement Q-Learning, Policy Gradient, DQN"},{"location":"RL_Unit1/#reinforcement-learning-rl_1","title":"Reinforcement Learning (RL)","text":"<p>Reinforcement learning is learning what to do\u2014how to map situations to actions\u2014so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may a\u21b5ect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics\u2014trial-and-error search and delayed reward\u2014are the two most important distinguishing features of reinforcement learning.</p>"},{"location":"RL_Unit1/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Reward and Punishment: Encourages repeating good actions and avoiding bad ones.</li> <li>Trial and Error: Learns by trying different methods.</li> <li>Learning Over Time: Improves through continuous experience.</li> <li>Long-Term Rewards: Actions influence future rewards.</li> </ul>"},{"location":"RL_Unit1/#example-applications","title":"Example Applications","text":"<ul> <li>Chess</li> <li>Maze Solving</li> <li>Industrial Robot Arms</li> <li>Path Planning</li> <li>Sweeper Robots</li> </ul>"},{"location":"RL_Unit1/#how-rl-differs-from-supervised-learning","title":"How RL Differs from Supervised Learning","text":"Feature Supervised Learning Reinforcement Learning Training Data Has labeled answers No labeled answers; learns from experience Decision Making Independent of past decisions Dependent on past decisions Learning Method Trained with a dataset Learns through trial and error"},{"location":"RL_Unit1/#elements-of-rl","title":"Elements of RL","text":"<pre><code>graph LR\n    A[Agent] --&gt;|Action| B[Environment]\n    B --&gt;|State| A\n    B --&gt;|Reward| A</code></pre>"},{"location":"RL_Unit1/#agent","title":"Agent","text":"<ul> <li>Definition: An entity that interacts with the environment.</li> <li>Examples: Robot, human, software program.</li> </ul>"},{"location":"RL_Unit1/#environment","title":"Environment","text":"<ul> <li>Definition: The external system in which the agent operates.</li> <li>Examples: Physical world, game simulation.</li> </ul>"},{"location":"RL_Unit1/#learning-process","title":"Learning Process","text":"<ol> <li>The agent moves from the initial state to the goal state.</li> <li>The agent continually asks, \"What is the best action in each state?\"</li> </ol>"},{"location":"RL_Unit1/#advantages-of-reinforcement-learning","title":"Advantages of Reinforcement Learning","text":"<ul> <li>\u2705 No need for predefined instructions or human intervention. </li> <li>\u2705 Can adapt to both static and dynamic environments. </li> <li>\u2705 Solves a wide range of problems (decision-making, prediction, optimization). </li> <li>\u2705 Improves with experience and fine-tunes over time.</li> </ul>"},{"location":"RL_Unit1/#disadvantages-of-reinforcement-learning","title":"Disadvantages of Reinforcement Learning","text":"<ul> <li>\u274c Performance depends on the quality of the reward function. </li> <li>\u274c Designing and tuning RL models can be complex.</li> </ul> <p>Note</p> <p>Reinforcement Learning is most suitable when: - The problem environment is complex and uncertain, making traditional programming methods ineffective. - Feedback is sparse, delayed, and dependent on multiple decisions. - Decision-making (actions) follows a feedback loop.</p> <p>Why Is Reinforcement Learning Difficult?</p> <p>The toughest parts of Reinforcement Learning are: - Mapping the Environment. - Including All Possible Actions.</p> <p>Core Concepts</p> <ul> <li>Goal-Oriented Learning: The agent learns by trying to achieve a goal.</li> <li>Learning from Consequences: The agent learns from the consequences of its actions.</li> <li>Active Research Area: RL is one of the most active fields in Artificial Intelligence (AI).</li> </ul>"},{"location":"RL_Unit1/#rl-algorithm-steps","title":"RL Algorithm Steps","text":"<pre><code>graph TD;\n    A[Agent Observes Environment] --&gt; B[Agent Performs Action];\n    B --&gt; C[Agent Moves to New State];\n    C --&gt; D[Agent Receives Reward];\n    D --&gt; E[Agent Evaluates Action - Good or Bad];\n    E --&gt; F[Agent Adjusts Strategy to Maximize Reward];\n\n</code></pre>"},{"location":"RL_Unit1/#learning-and-planning","title":"Learning and Planning","text":""},{"location":"RL_Unit1/#two-fundamental-problems-in-sequential-decision-making","title":"Two Fundamental Problems in Sequential Decision Making","text":""},{"location":"RL_Unit1/#reinforcement-learning-rl_2","title":"Reinforcement Learning (RL):","text":"<ul> <li>The environment is initially unknown.</li> <li>The agent interacts with the environment.<ul> <li>The agent improves its policy.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#planning","title":"Planning:","text":"<ul> <li>A model of the environment is known.</li> <li>The agent performs computations with its model (without any external interaction).</li> <li>The agent improves its policy, also known as deliberation, reasoning, introspection, pondering, thought, search.</li> </ul>"},{"location":"RL_Unit1/#model-of-the-environment","title":"Model of the Environment:","text":"<ul> <li>A model mimics the behavior of the environment. With the help of the model, one can make inferences about how the environment will behave. For example, if a state and an action are given, the model can predict the next state and reward.</li> <li>The model is used for planning, providing a way to take a course of action by considering all future situations before actually experiencing those situations.</li> <li>Approaches for solving RL problems with the help of the model are termed model-based approach.</li> <li>An approach without using a model is called a model-free approach.</li> </ul>"},{"location":"RL_Unit1/#types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based","title":"Types of Reinforcement Learning Algorithms ( on the basis of model based)","text":"<p>There are various algorithms used in reinforcement learning such as Q-learning, policy gradient methods, Monte Carlo method and many more. All these algorithms can be classified into two broad categories - </p>"},{"location":"RL_Unit1/#model-free-reinforcement-learning","title":"Model-free Reinforcement Learning :","text":"<ul> <li>It is a category of reinforcement learning algorithms that learns to make decisions by interacting with the environment directly, without creating a model of the environment's dynamics.</li> <li>The agent performs different actions multiple times to learn the outcomes and creates a strategy (policy) that optimizes its reward points. This is ideal for changing, large or complex environments.</li> <li>Not applicable for some scenario like self driving car.</li> </ul>"},{"location":"RL_Unit1/#model-based-reinforcement-learning","title":"Model-based Reinforcement Learning:","text":"<ul> <li>This category of reinforcement learning algorithms involves creating a model of the environment's dynamics to make decisions and improve performance.</li> <li>Ideal for environments that are static and well-defined, where real-world environment testing is difficult.</li> </ul>"},{"location":"RL_Unit1/#key-differences-between-model-free-and-model-based-reinforcement-learning","title":"Key Differences Between Model-free and Model-based Reinforcement Learning","text":"Feature Model-Free RL Model-Based RL Learning Approach Direct learning from environment Indirect learning through model building Efficiency Requires more real-world interactions More sample-efficient Complexity Simpler implementation More complex due to model learning Environment Utilization No internal model Builds and uses a model Adaptability Slower to adapt to changes Faster adaptation with accurate model Computational Requirements Less intensive More computational resources needed Examples Q-Learning, SARSA, DQN, PPO Dyna-Q, Model-Based Value Iteration"},{"location":"RL_Unit1/#rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state","title":"RL Framework - The RL Process: A Loop of State, Action, Reward, and Next State","text":""},{"location":"RL_Unit1/#main-characteristics-of-rl","title":"Main Characteristics of RL","text":"<ul> <li>No supervisor while training.</li> <li>Environment is generally stochastic for real-world applications.</li> <li>Model of the environment can be incomplete.</li> <li>Feedback (Negative/Positive Reward) can be delayed or partial.</li> <li>The agent uses experience from the past to improve its performance over time.</li> <li>Actions that have fetched more rewards are preferred.</li> <li>The agent tries various actions and prefers those that are best or have fetched more rewards.</li> <li>RL uses Markov Decision Process (MDP) framework to define the interaction between a learning agent and its environment.</li> </ul>"},{"location":"RL_Unit1/#reinforcement-learning-rl-problem-challenges-in-rl","title":"Reinforcement Learning (RL) Problem - Challenges in RL","text":""},{"location":"RL_Unit1/#trade-off-between-exploration-and-exploitation","title":"Trade-off between Exploration and Exploitation:","text":"<ul> <li>To obtain rewards, an RL agent must prefer actions that it has tried in the past and found effective (Exploit).</li> <li>However, to discover such actions, it must try actions it has not selected before (Explore).</li> </ul> <p>Note</p> <p>Neither exploration nor exploitation can be pursued exclusively without failing at the task.</p>"},{"location":"RL_Unit1/#fundamental-components-of-rl","title":"Fundamental Components of RL","text":"<ul> <li>Policy: Defines the agent\u2019s behavior.</li> <li>Reward Function: Provides feedback on actions.</li> <li>Value Function: Evaluates future rewards.</li> <li>Model of the Environment: Simulates how the environment works.</li> </ul>"},{"location":"RL_Unit1/#policy","title":"Policy:","text":"<p>A policy is a strategy or set of rules that defines the actions the agent should take in a given state.</p> <ul> <li>The policy can be deterministic (one action for a state) or stochastic (probabilistic actions for a state).</li> <li>The goal is to find an optimal policy that maximizes the total expected reward.</li> </ul> <p>Example:</p> <ul> <li>A robot navigating a maze may follow a policy that says, \"Always turn left unless there's an obstacle, then turn right.\"</li> </ul> <p>Note</p> <p>A policy is like a person's habit or plan of action, such as the decision to exercise every morning or take an umbrella when it's cloudy.</p>"},{"location":"RL_Unit1/#value-function","title":"Value function:","text":"<p>Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.  - Rewards determine the immediate, intrinsic desirability of environmental states.  - Values indicate the long-term desirability of states after considering the states likely to follow and the rewards available in those states.  - Example: - A state might always yield a low immediate reward but still have a high value because it is followed by states that yield high rewards. </p> <p>Note</p> <p>Rewards are somewhat like pleasure (if high) and pain (if low). - Values correspond to a more refined and farsighted judgment of how pleased or displeased we are by the environment.</p>"},{"location":"RL_Unit1/#reward-function","title":"Reward Function:","text":"<p>The reward function provides feedback on the actions the agent takes, indicating whether an action was good or bad.</p> <ul> <li>It assigns a numeric value to the agent's actions, which the agent uses to evaluate the desirability of its actions in a given state.</li> <li>The goal of the agent is to maximize the cumulative reward over time.</li> </ul> <p>Example:</p> <ul> <li>In a game, winning a round might give a reward of +10, while losing gives a reward of -1.</li> </ul> <p>Note</p> <p>The reward function is like the feedback a person gets from their actions, such as feeling happy after a good deed or guilty after a mistake.</p>"},{"location":"RL_Unit1/#model-of-the-environment_1","title":"Model of the Environment:","text":"<p>The model of the environment simulates how the environment behaves, helping the agent predict the outcomes of actions.</p> <ul> <li>This model can be used for planning future actions by simulating potential outcomes.</li> <li>A model-free approach directly learns from experience, while a model-based approach uses a model to predict actions' results before performing them.</li> </ul> <p>Example:</p> <ul> <li>A self-driving car may use a model to simulate various driving scenarios and plan its route accordingly.</li> </ul> <p>Note</p> <p>The model of the environment is like a mental map that a person forms, which helps them predict the likely outcomes of their actions, such as deciding to avoid a route with heavy traffic.</p>"},{"location":"RL_Unit1/#types-of-reinforcement-learning","title":"Types of Reinforcement Learning","text":"<p>There are three main types of Reinforcement Learning (RL): - Value-Based - Policy-Based - Model-Based</p> <p>Each approach has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem you are trying to solve.</p>"},{"location":"RL_Unit1/#value-based-reinforcement-learning","title":"Value-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns to estimate the value of each state or action based on the rewards it receives.</li> <li>This value is known as Q-values.</li> <li>The agent then selects the actions with the highest Q-value in each state to maximize its long-term reward.</li> <li>The most commonly used algorithm for value-based reinforcement learning is Q-learning.</li> </ul>"},{"location":"RL_Unit1/#policy-based-reinforcement-learning","title":"Policy-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns an optimal policy, which is a mapping from states to actions, without calculating the value function.</li> <li>The policy is updated based on the rewards received by the agent, with the goal of maximizing the expected reward over time.</li> <li>The most common algorithm used for policy-based reinforcement learning is the REINFORCE algorithm.</li> </ul>"},{"location":"RL_Unit1/#model-based-reinforcement-learning_1","title":"Model-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns a model of the environment, which it can use to simulate different scenarios and plan its actions accordingly.</li> <li>The model can learn through supervised or unsupervised learning, and the agent can use it to predict the outcome of its actions before taking them.</li> <li>The most common model-based reinforcement learning algorithm is the Dyna algorithm.</li> </ul>"},{"location":"RL_Unit1/#formal-presentation-of-rl-fundamentals","title":"Formal Presentation of RL Fundamentals","text":""},{"location":"RL_Unit1/#1-state-s-and-action-a","title":"1. State (\\(s\\)) and Action (\\(a\\))","text":"<ul> <li>Current state: \\(s_t\\)</li> <li>Next state: \\(s_{t+1}\\)</li> <li>Action: \\(a\\), an action performed by the agent to move from state \\(s_t\\) to \\(s_{t+1}\\).</li> <li>State space: The set of all possible states the agent can be in.</li> </ul>"},{"location":"RL_Unit1/#2-reward-r-or-rs-a","title":"2. Reward (\\(r\\) or \\(R(s, a)\\))","text":"<ul> <li>The result of taking action \\(a\\) at state \\(s\\).</li> <li>Actions affect not only the immediate reward but also the next states and all subsequent rewards.</li> </ul>"},{"location":"RL_Unit1/#3-episode","title":"3. Episode","text":"<ul> <li>A sequence of states and actions until reaching a terminal state.</li> </ul>"},{"location":"RL_Unit1/#4-transition-probability-ps-s-a","title":"4. Transition Probability (\\(P(s' | s, a)\\))","text":"<ul> <li>The probability of reaching state \\(s'\\) when taking action \\(a\\) at state \\(s_t\\).</li> </ul>"},{"location":"RL_Unit1/#5-policy-pis-a","title":"5. Policy (\\(\\pi(s, a)\\))","text":"<ul> <li>A mapping of each state to an action, determining how the agent acts at each state.</li> <li>Types of Policies:<ul> <li>Deterministic: Always selects the same action for a given state.</li> <li>Stochastic: Selects actions based on probability distribution.</li> <li>\\(\\pi(a | s) = P(A_t = a | S_t = s)\\).</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#6-return-g_t","title":"6. Return (\\(G_t\\))","text":"<ul> <li>The total future reward from state \\(s_t\\).</li> <li>\\(Gt=rt+\u03b3rt+1+\u03b32rt+2+\u22ef+\u03b3T\u22121rTG_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-1} r_T\\)</li> <li>Discount factor (\\(\\gamma\\)):<ul> <li>Determines the importance of future rewards.</li> <li>Higher \\(\\gamma\\) \u2192 more focus on long-term rewards.</li> <li>Lower \\(\\gamma\\) \u2192 more focus on immediate rewards.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#7-value-function-vs","title":"7. Value Function (\\(V(s)\\))","text":"<ul> <li>The expected return from starting at state \\(s\\).</li> <li>Also called the State-Value Function:  </li> </ul> <p>Formula</p> <p>\\(V(s)=E[Gt\u2223st=s]=E[rt+\u03b3rt+1+\u03b32rt+2+\u22ef+\u03b3T\u22121rT\u2223st=s]V(s) = \\mathbb{E}[G_t | s_t = s] = \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-1} r_T \\mid s_t = s \\right]\\)</p> <ul> <li>Breakdown:<ul> <li>Immediate reward: \\(r_t\\).</li> <li>Discounted value of successor states.</li> <li>Represents the long-term desirability of state \\(s\\).</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#8-optimal-policy-pis","title":"8. Optimal Policy (\\(\\pi^*(s)\\))","text":"<ul> <li>The best possible policy for a given state, maximizing expected future rewards.</li> </ul>"},{"location":"RL_Unit1/#9-optimal-value-functions","title":"9. Optimal Value Functions","text":"<ul> <li>Optimal State-Value Function:<ul> <li>Maximum value function over all policies: \\(V\u2217(s)=max\u2061\u03c0V\u03c0(s)V^*(s) = \\max_{\\pi} V_{\\pi}(s)\\)</li> </ul> </li> <li>Optimal Action-Value Function (\\(Q^*(s, a)\\)):<ul> <li>Maximum action-value function over all policies: \\(Q\u2217(s,a)=max\u2061\u03c0Q\u03c0(s,a)Q^*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)\\)</li> <li>Represents the best possible expected return for taking action \\(a\\) in state \\(s\\).</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#two-fundamental-tasks-of-reinforcement-learning","title":"Two Fundamental Tasks of Reinforcement Learning","text":""},{"location":"RL_Unit1/#1-prediction-task","title":"1. Prediction Task","text":"<ul> <li>We have a policy:<ul> <li>The goal is to evaluate the policy by estimating the state-value or Q-value of running actions within a given policy.</li> <li>Evaluate the future.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#2-control-task","title":"2. Control Task","text":"<ul> <li>We don't know the policy, and the goal is:<ul> <li>To find the optimal policy aiming to collect maximum rewards.</li> <li>Optimize the future.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#tabular-solution-methods","title":"Tabular Solution Methods","text":""},{"location":"RL_Unit1/#core-idea","title":"Core Idea","text":"<ul> <li>In their simplest form, RL algorithms assume that the state and action spaces are small enough for approximate value functions to be represented as arrays or tables.</li> <li>These methods can often find exact solutions (i.e., optimal value function and optimal policy).</li> </ul>"},{"location":"RL_Unit1/#fundamental-classes-of-methods-for-solving-finite-mdps","title":"Fundamental Classes of Methods for Solving Finite MDPs","text":"<ol> <li> <p>Dynamic Programming (DP)</p> <ul> <li>Requires a complete and accurate model of the environment.</li> <li>Mathematically well-developed.</li> <li> <p>Monte Carlo Methods</p> </li> <li> <p>No model required and conceptually simple.</p> </li> <li>Not well suited for step-by-step incremental computation.</li> <li> <p>Temporal Difference (TD) Learning</p> </li> <li> <p>Requires no model and is fully incremental.</p> </li> <li>More complex to analyze but efficient.</li> <li>Differences exist in efficiency and speed of convergence.</li> </ul> </li> </ol> <p>Each method has its own strengths and weaknesses.</p>"},{"location":"RL_Unit1/#immediate-reinforcement-learning-vs-full-reinforcement-learning","title":"Immediate Reinforcement Learning vs. Full Reinforcement Learning","text":""},{"location":"RL_Unit1/#immediate-reinforcement-learning-immediate-rl","title":"Immediate Reinforcement Learning (Immediate RL)","text":"<ul> <li>Policy Update Frequency<ul> <li>Updates the policy or value function after every action.</li> <li>The agent learns and adapts in real time as it interacts with the environment.</li> </ul> </li> <li>Learning Approach<ul> <li>Online Learning: Updates are made continuously and incrementally after each interaction.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#immediate-rl-vs-full-rl","title":"Immediate RL vs. Full RL","text":"Feature Immediate RL Full RL Reward Timing Immediate rewards after each action. Delayed rewards, requiring long-term strategy. Decision Making Faster, as actions are evaluated instantly. Requires profound understanding of the environment. Example Bandit Problem Chess, Go, or strategic planning tasks."},{"location":"RL_Unit1/#explore-exploit-dilemma-in-immediate-rl","title":"Explore-Exploit Dilemma in Immediate RL","text":"<ul> <li>The agent must explore different actions to identify near-optimal actions.</li> <li>Once enough exploration is done, it exploits the best-known action.</li> <li>The challenge: How much to explore before exploiting?</li> </ul>"},{"location":"RL_Unit1/#examples-of-reinforcement-learning-in-real-life","title":"Examples of Reinforcement Learning in Real Life","text":""},{"location":"RL_Unit1/#immediate-rl-examples","title":"Immediate RL Examples","text":"<ul> <li>Giving treats for homework completion.</li> <li>Earning points in a game.</li> <li>Receiving applause after a performance.</li> <li>Receiving praise for completing a task.</li> <li>Getting paid directly after work.</li> <li>Eating immediately after feeling hungry.</li> <li>Social media notifications.</li> </ul>"},{"location":"RL_Unit1/#delayed-reinforcement-examples","title":"Delayed Reinforcement Examples`","text":"<ul> <li>Saving money for future goals.</li> <li>Completing a degree for career advancement.</li> <li>Physical fitness and exercise.</li> <li>Learning a musical instrument.</li> <li>Learning a new language.</li> </ul>"},{"location":"RL_Unit1/#suitability-of-immediate-rl","title":"Suitability of Immediate RL","text":"<ul> <li>Real-time applications: Suitable where quick decision-making is needed, such as:<ul> <li>Tic-Tac-Toe: The agent updates its strategy after each move.</li> <li>Self-driving cars: The control system updates the driving policy in real time.</li> </ul> </li> </ul>"},{"location":"RL_Unit1/#general-reinforcement-learning-rl","title":"General Reinforcement Learning (RL","text":"<ul> <li> <p>Policy Update Frequency</p> <ul> <li>Updates can be made after accumulating a batch of experiences or at the end of an episode.</li> <li> <p>Learning Approach</p> </li> <li> <p>Online and Offline Learning</p> <ul> <li>Online RL: Updates occur during interaction with the environment.</li> <li>Offline RL: The agent gathers experience first and updates the policy afterward.</li> </ul> </li> </ul> </li> </ul>"},{"location":"RL_Unit1/#state-action-value-function-qs-a","title":"State-Action Value Function (\\(Q(s, a)\\))","text":"<ul> <li>The state-action value function (or Q-function) specifies how good it is for an agent to take a particular action \\(a\\) in a given state \\(s\\) under a policy \\(\\pi\\).</li> <li>Denoted as: \\(Q(s,a)=E[Gt\u2223St=s,At=a]Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a]\\)</li> <li>Represents the expected cumulative reward of taking action \\(a\\) in state \\(s\\).</li> </ul>"},{"location":"RL_Unit1/#reinforcement-learning-rl-fundamentals","title":"Reinforcement Learning (RL) Fundamentals","text":""},{"location":"RL_Unit1/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<ul> <li>A simple rule to explain complex behaviors.</li> <li>Intuition: Prediction of the outcome at time t+1t+1t+1 is better than at time ttt. The later prediction is used to adjust the earlier prediction.</li> <li>Has had a profound impact on behavioral psychology and neuroscience.</li> </ul>"},{"location":"RL_Unit1/#optimal-control","title":"Optimal Control","text":"<ul> <li>A branch of mathematical optimization.</li> <li>Goal: Design a controller that maximizes or minimizes an objective function.</li> <li>Key Concept: Finding a control policy that optimizes the cumulative reward or minimizes the cost over time.</li> <li>Deals with dynamical systems, determining the best sequence of actions to achieve an optimal outcome.</li> </ul>"},{"location":"RL_Unit1/#dynamic-programming-dp-in-rl","title":"Dynamic Programming (DP) in RL","text":"<ul> <li>A mathematical approach to solving optimization problems by breaking them down into simpler subproblems.</li> <li>In Markov Decision Processes (MDPs), DP methods help find optimal policies by solving Bellman equations.</li> </ul>"},{"location":"RL_Unit1/#two-primary-dp-methods","title":"Two Primary DP Methods","text":"<ol> <li>Policy Iteration:<ul> <li>Alternates between evaluating a policy and improving it.</li> </ul> </li> <li>Value Iteration:<ul> <li>Iteratively updates the value function directly to find the optimal policy.</li> </ul> </li> </ol>"},{"location":"RL_Unit1/#on-policy-vs-off-policy-reinforcement-learning","title":"On-Policy vs. Off-Policy Reinforcement Learning","text":"<p>In Reinforcement Learning (RL), learning strategies can be classified into on-policy and off-policy methods. These approaches define how an agent interacts with the environment and learns optimal behavior.  </p>"},{"location":"RL_Unit1/#on-policy-learning","title":"On-Policy Learning","text":"<ul> <li>The agent learns while following its own policy.  </li> <li>Explores and exploits simultaneously.  </li> <li>Typically used in algorithms like SARSA (State-Action-Reward-State-Action).  </li> </ul> <p>\u2705 Example: - Learning to ride a bike by trial and error\u2014adjusting balance while practicing.  </p>"},{"location":"RL_Unit1/#off-policy-learning","title":"Off-Policy Learning","text":"<ul> <li>The agent learns from data generated by other policies (not just its own).  </li> <li>More flexible as it allows learning from past experiences.  </li> <li>Used in algorithms like Q-learning, where the agent updates its policy using the best-known actions.  </li> </ul> <p>\u2705 Example: - Learning to ride a bike by watching others rather than directly practicing.  </p>"},{"location":"RL_Unit1/#unit-2","title":"Unit 2","text":""},{"location":"RL_Unit1/#the-one-armed-and-multi-armed-bandit-problems","title":"The One-Armed and Multi-Armed Bandit Problems","text":"<p>The One-Armed Bandit Problem and its extension, the k-Armed (Multi-Armed) Bandit Problem, are fundamental concepts in reinforcement learning and decision-making under uncertainty.  </p>"},{"location":"RL_Unit1/#one-armed-bandit-problem","title":"\ud83c\udfb0 One-Armed Bandit Problem","text":"<p>\u2705 Definition: - Refers to a slot machine with a single lever (arm). - Each spin has a certain probability of winning, but this probability is unknown. - The outcome is uncertain, and a player must decide whether to keep playing or stop.  </p> <p>\u2705 Key Challenge: - The probability distribution of rewards is unknown and cannot be determined with a limited number of trials.  </p> <p>\u2705 Real-World Analogy: - A gambler playing one slot machine without knowing its payout rate.  </p>"},{"location":"RL_Unit1/#multi-armed-k-armed-bandit-problem","title":"\ud83c\udfb0 Multi-Armed (k-Armed) Bandit Problem","text":"<p>\u2705 Definition: - A gambler faces multiple slot machines (each with different and unknown payout probabilities). - The goal is to maximize total winnings by choosing which machine (arm) to play.  </p> <p>\u2705 Core Concept in Reinforcement Learning: - Balances exploration (trying different machines) vs. exploitation (sticking to the best-known machine). - Each arm has a unique probability distribution, which is stationary (remains constant over time).  </p> <p>\u2705 Mathematical Model: - An agent selects between N different actions (arms). - Each arm provides a reward drawn from an unknown probability distribution. - The goal is to maximize cumulative reward over multiple trials.  </p>"},{"location":"RL_Unit1/#why-is-it-called-a-bandit","title":"\ud83d\udd0d Why is it Called a \"Bandit\"?","text":"<ul> <li>The term \"bandit\" refers to a thief.  </li> <li>Slot machines are called \"one-armed bandits\" because casinos configure them to ensure players eventually lose money.  </li> </ul>"},{"location":"RL_Unit1/#applications-of-the-multi-armed-bandit-problem","title":"\ud83d\udccc Applications of the Multi-Armed Bandit Problem","text":"<ul> <li>Online Advertising \u2013 Deciding which ads to display to maximize clicks.  </li> <li>Clinical Trials \u2013 Testing multiple treatments to determine the most effective one.  </li> <li>A/B Testing \u2013 Comparing website designs or marketing strategies.  </li> <li>Stock Trading \u2013 Choosing the best stocks to invest in over time.  </li> </ul> <p>The k-Armed Bandit Problem serves as a foundation for reinforcement learning algorithms, influencing decision-making strategies in uncertain environments.</p>"},{"location":"RL_Unit1/#k-armed-bandit-problem-in-reinforcement-learning","title":"k-Armed Bandit Problem in Reinforcement Learning","text":"<p>The k-Armed Bandit Problem is a fundamental challenge in reinforcement learning and decision theory, where an agent must choose between multiple actions (arms) to maximize total reward.  </p>"},{"location":"RL_Unit1/#key-concepts-in-k-armed-bandit-problem","title":"\ud83d\udccc Key Concepts in k-Armed Bandit Problem","text":"<ul> <li>Action Value (\\( q^*(a) \\)) </li> <li>The true expected reward for selecting action \\( a \\).  </li> <li>Defined as:     $$    q^*(a) = \\mathbb{E}[R_t | A_t = a]    $$</li> <li> <p>If the true values of all actions were known, the best approach would be to always select the action with the highest \\( q^*(a) \\).  </p> </li> <li> <p>Estimated Action Value (\\( Q_t(a) \\)) </p> </li> <li>The empirical mean of observed rewards from action \\( a \\).  </li> <li>Defined as:     $$  Q_t(a) = \\frac{1}{n_a} \\sum_{i=1}^{n_a} r_i   $$</li> <li>As more rewards are observed, \\( Q_t(a) \\) converges to \\( q^*(a) \\).  </li> </ul>"},{"location":"RL_Unit1/#expected-vs-estimated-value","title":"Expected vs. Estimated Value","text":"Concept Definition Formula Expected Value \\( q^*(a) \\) The longrun average reward for an action \\( q^*(a) = \\sum r \\cdot P(r,a) \\) Estimated Value \\( Q_t(a) \\) The empirical mean reward based on observed outcomes \\( Q_t(a) = \\frac{1}{n_a} \\sum_{i=1}^{n_a} r_i \\) <ul> <li>Expected Value is theoretical and based on an unknown probability distribution.  </li> <li>Estimated Value is calculated from real observations and improves over time.  </li> </ul>"},{"location":"RL_Unit1/#exploitation-vs-exploration-trade-off","title":"Exploitation vs. Exploration Trade-Off","text":"<ul> <li>Greedy Actions: </li> <li>Actions with the highest estimated value \\( Q_t(a) \\).  </li> <li> <p>Exploitation: Selecting the best-known action to maximize immediate reward.  </p> </li> <li> <p>Non-Greedy Actions: </p> </li> <li>Actions that do not have the highest estimated value.  </li> <li>Exploration: Trying different actions to improve estimates and potentially discover better rewards.  </li> </ul> <p>\ud83d\udd39 Key Dilemma: - Exploiting greedy actions maximizes short-term rewards. - Exploring new actions may lead to better long-term gains.  </p>"},{"location":"RL_Unit1/#optimistic-initial-values-in-reinforcement-learning","title":"Optimistic Initial Values in Reinforcement Learning","text":"<p>Optimistic initial values are a simple yet effective technique used to encourage exploration in reinforcement learning, particularly in multi-armed bandit problems and value-based learning methods.  </p>"},{"location":"RL_Unit1/#how-does-it-work","title":"\ud83d\udccc How Does It Work?","text":"<p>Instead of initializing action values to zero or a neutral estimate, we set them to a highly optimistic value.  </p> <p>For example, in a 10-armed bandit problem where the true action values (\\( q^*(a) \\)) are drawn from a normal distribution (mean = 0, variance = 1): - Instead of initializing all \\( Q_1(a) = 0 \\), we set them to +5 (a high optimistic value).  </p>"},{"location":"RL_Unit1/#why-does-optimistic-initialization-encourage-exploration","title":"\ud83d\udd0d Why Does Optimistic Initialization Encourage Exploration?","text":"<ol> <li>Initial Overestimation:  </li> <li> <p>Since the true rewards are lower than +5, the agent will be disappointed by its first few actions.  </p> </li> <li> <p>Forces the Agent to Try Other Actions:  </p> </li> <li> <p>As the agent realizes that the initial choices don\u2019t meet expectations, it explores alternative actions.  </p> </li> <li> <p>More Thorough Exploration in Early Steps:  </p> </li> <li> <p>The agent tries multiple actions before settling on the best one.  </p> </li> <li> <p>Converges to Optimal Action Over Time:  </p> </li> <li>Eventually, the estimates stabilize as the agent gathers more data.  </li> </ol>"},{"location":"RL_Unit1/#comparison-optimistic-vs-greedy-exploration","title":"\ud83d\udcc8 Comparison: Optimistic vs. \u03b5-Greedy Exploration","text":"Exploration Method How It Works Pros Cons Optimistic Initial Values Set initial action values high Encourages early exploration, works well in stationary environments May not adapt well in non-stationary environments \u03b5-Greedy Selects a random action with probability \u03b5 Works well in both stationary and non-stationary settings Requires tuning of \u03b5, exploration is uniform"},{"location":"RL_Unit1/#upper-confidence-bound-ucb-in-reinforcement-learning","title":"Upper Confidence Bound (UCB) in Reinforcement Learning","text":""},{"location":"RL_Unit1/#why-is-exploration-needed","title":"Why is Exploration Needed?","text":"<ul> <li>Action-value estimates always contain uncertainty.  </li> <li>Greedy actions (highest estimated value) may not be truly optimal.  </li> <li>\u03b5-Greedy exploration forces random selection but does not prioritize actions with high uncertainty.  </li> </ul>"},{"location":"RL_Unit1/#upper-confidence-bound-ucb-approach","title":"Upper Confidence Bound (UCB) Approach","text":"<p>Instead of exploring randomly, UCB selects actions by balancing: 1. Current estimate of reward (exploitation). 2. Uncertainty of the estimate (exploration).  </p>"},{"location":"RL_Unit1/#ucb-formula","title":"\ud83d\udccc UCB Formula","text":"<p>For an action \\( a \\), UCB selects the action that maximizes: [ Q_t(a) + c \\sqrt{\\frac{\\log t}{N_t(a)}} ] where: - \\( Q_t(a) \\) = Estimated value of action \\( a \\). - \\( t \\) = Total number of time steps. - \\( N_t(a) \\) = Number of times action \\( a \\) has been selected. - \\( c \\) = Exploration parameter (higher \\( c \\) encourages more exploration).  </p>"},{"location":"RL_Unit1/#how-ucb-works","title":"How UCB Works?","text":"<ul> <li>The square-root term measures the uncertainty in the estimate of \\( Q_t(a) \\).  </li> <li>Actions that have been chosen fewer times (\\( N_t(a) \\) is low) will have higher uncertainty, making them more likely to be selected.  </li> <li>As an action is selected more often, \\( N_t(a) \\) increases, reducing the exploration term.  </li> </ul>"},{"location":"RL_Unit1/#key-properties-of-ucb","title":"Key Properties of UCB","text":"<p>\u2705 Encourages exploration of uncertain actions \u2013 Prefers actions that haven't been tried enough. \u2705 Gradually reduces exploration \u2013 As all actions are explored, the agent shifts towards exploitation. \u2705 Logarithmic growth \u2013 Ensures that exploration is bounded over time but never stops completely.  </p>"},{"location":"RL_Unit1/#comparison-ucb-vs-greedy","title":"Comparison: UCB vs. \u03b5-Greedy","text":"Method Exploration Type Strengths Weaknesses \u03b5-Greedy Random exploration Simple, works well in practice Does not focus on uncertain actions UCB Uncertainty-based exploration Smart exploration, prioritizes less tried actions More complex, requires tuning \\( c \\)"},{"location":"RL_Unit1/#markov-decision-process-mdp-in-reinforcement-learning","title":"Markov Decision Process (MDP) in Reinforcement Learning","text":""},{"location":"RL_Unit1/#what-is-a-markov-decision-process-mdp","title":"\ud83d\udccc What is a Markov Decision Process (MDP)?","text":"<p>A Markov Decision Process (MDP) is a mathematical framework used to model decision-making problems where outcomes are partially random and partially controlled by an agent.  </p> <p>It extends a Markov Chain by incorporating actions and rewards, allowing an agent to interact with an environment sequentially to maximize long-term rewards.  </p>"},{"location":"RL_Unit1/#what-is-a-markov-chain","title":"\ud83d\udccc What is a Markov Chain?","text":"<p>A Markov Chain is a stochastic process that follows the Markov Property:  </p> <p>The probability of transitioning to the next state only depends on the current state and not on past states.  </p> <p>Mathematically, [ P(S_{t+1} | S_t, S_{t-1}, ..., S_0) = P(S_{t+1} | S_t) ]  </p> <p>\u2705 Key Feature: Memoryless property (Future states depend only on the present state, not history).  </p>"},{"location":"RL_Unit1/#what-is-a-transition-in-mdp","title":"\ud83d\udccc What is a Transition in MDP?","text":"<p>A transition defines the probability of moving from one state to another when taking a specific action.  </p> <p>Denoted as \\( P(s' | s, a) \\): - \\( s \\) = Current state - \\( a \\) = Action taken - \\( s' \\) = Next state - \\( P(s' | s, a) \\) = Probability of transitioning to \\( s' \\) after taking action \\( a \\) in state \\( s \\).  </p> <p>\u2705 Example: - If a robot is in a room and moves right, there is an 80% chance it reaches the next room, but a 20% chance it remains in the same room (if the door is stuck).  </p>"},{"location":"RL_Unit1/#how-is-mdp-used-in-reinforcement-learning-rl","title":"\ud83d\udccc How is MDP Used in Reinforcement Learning (RL)?","text":"<p>In Reinforcement Learning, MDP helps formulate the environment as a mathematical model, guiding the agent to learn optimal policies.  </p> <ul> <li>Agent interacts with an environment following MDP dynamics.  </li> <li>State transitions occur based on agent actions.  </li> <li>The goal is to learn a policy that maximizes cumulative rewards over time.  </li> </ul>"},{"location":"RL_Unit1/#components-of-an-mdp","title":"\ud83d\udccc Components of an MDP","text":"<p>An MDP is defined by the tuple \\( (S, A, P, R, \\gamma) \\):  </p> Component Definition \\( S \\) (State Space) Set of all possible states the agent can be in. \\( A \\) (Action Space) Set of actions the agent can take. **( P(s' s, a) ) (Transition Probability)** \\( R(s, a) \\) (Reward Function) Reward received after taking action \\( a \\) in state \\( s \\). \\( \\gamma \\) (Discount Factor) Determines the importance of future rewards (\\( 0 \\leq \\gamma \\leq 1 \\)). <p>\u2705 Example in RL: - Self-driving car:   - \\( S \\): Location of the car, speed, traffic signals.   - \\( A \\): Accelerate, brake, turn left, turn right.   - \\( P(s' | s, a) \\): Probability of reaching the next position based on current speed and action.   - \\( R(s, a) \\): Reward for avoiding obstacles and staying on track.   - \\( \\gamma \\): Balances short-term (safety) vs. long-term (reaching destination quickly).  </p>"},{"location":"RL_Unit1/#summary","title":"\ud83d\udccc Summary","text":"<ul> <li>Markov Chain models state transitions without actions or rewards.  </li> <li>MDP extends Markov Chains by adding actions and rewards, allowing decision-making.  </li> <li>MDP is the foundation of RL, providing a structured way for agents to learn optimal policies.  </li> </ul>"},{"location":"computer-vision/","title":"Computer vision","text":""},{"location":"computer-vision/#unit-1","title":"Unit 1","text":""},{"location":"computer-vision/#introduction","title":"Introduction","text":"<ul> <li>Image Processing: Image in \u2192 Image out.  </li> <li>Computer Vision: Image/Video in \u2192 Interpretation out.  </li> <li>Objective: Enable computers to see and interpret objects like humans.  </li> </ul>"},{"location":"computer-vision/#human-vs-computer-vision","title":"Human vs. Computer Vision","text":"<ul> <li>Humans: Quick recognition, intelligent decision-making.  </li> <li>Computers: Require complex processing, pattern recognition, and AI.  </li> </ul>"},{"location":"computer-vision/#features-of-human-vision","title":"Features of Human Vision","text":"<ul> <li>Stereo Vision: Two eyes perceive depth using different distances.  </li> <li>Texture &amp; Color: Identifies objects based on patterns and colors.  </li> <li>Object Recognition: Recognizes objects despite illumination, viewpoint, or expression changes.  </li> <li>Context Awareness: Infers key information from a scene.  </li> </ul>"},{"location":"computer-vision/#limitations-of-human-vision","title":"Limitations of Human Vision","text":"<ul> <li>Memory Constraints: Limited ability to recall vast image data.  </li> <li>Restricted Spectrum: Limited to visible light.  </li> <li>Optical Illusions: Misinterpretations of size, shape, and perspective.  </li> </ul>"},{"location":"computer-vision/#computer-vision-vs-image-processing","title":"Computer Vision vs. Image Processing","text":"<ul> <li>Image Processing: Low-level operations (compression, restoration, enhancement).  </li> <li>Computer Vision: High-level understanding (object recognition, scene interpretation).  </li> </ul>"},{"location":"computer-vision/#applications-of-computer-vision","title":"Applications of Computer Vision","text":"<ul> <li>Self-driving Cars: Navigation and obstacle detection.  </li> <li>Facial Recognition: Identity verification.  </li> <li>Augmented Reality: Merging virtual objects with reality.  </li> <li>Medical Imaging: Detecting anomalies in X-rays and MRIs.  </li> <li>Manufacturing: Detecting defective products.  </li> <li>Retail &amp; Banking: OCR, fraud detection, and identity verification.</li> </ul>"},{"location":"computer-vision/#levels-of-processing-for-computer-vision","title":"\ud83d\udcdc Levels of processing for computer Vision","text":""},{"location":"computer-vision/#computer-vision-processing-levels","title":"Computer Vision Processing Levels","text":""},{"location":"computer-vision/#low-level-processing","title":"Low-Level Processing","text":"<ul> <li>Enhances image quality and extracts basic features like edges, corners, textures.  </li> <li>Works on raw pixel data without understanding objects.  </li> <li>Steps for low level processing:  </li> <li>Image Preprocessing: Contrast enhancement, noise reduction.  </li> <li>Feature Extraction: Edge detection (Canny, Sobel filters).  </li> <li>Segmentation: Divides an image into meaningful parts.  </li> <li>Super-Resolution: Converts low-resolution to high-resolution images.  </li> </ul> <p>Important</p> <p>Low-level processing extracts fundamental features such as edges, lines, corners, and salient points from an image. These features serve as the building blocks for higher-level processing, enabling pattern recognition, object detection, and scene understanding.</p> <p>How Low-Level Processing Helps:</p> <ul> <li>\u2705 Feature Extraction: Identifies edges, textures, and key points in an image.</li> <li>\u2705 Segmentation: Divides images into meaningful regions and shapes.</li> <li>\u2705 Noise Reduction: Enhances image clarity by removing unwanted distortions.</li> <li>\u2705 Super-Resolution: Generates higher-resolution images from fewer pixels.</li> <li>\u2705 Depth Perception: Uses stereo vision (left &amp; right cameras) to create disparity maps for estimating depth.</li> </ul>"},{"location":"computer-vision/#mid-level-processing","title":"Mid-Level Processing","text":"<ul> <li>Uses patterns and object features for recognition.  </li> <li>Works on segmented objects to classify, track, and detect motion.  </li> <li>Examples:  </li> <li>Object Detection: Recognizes and labels objects.  </li> <li>Image Classification: Assigns categories to images.  </li> <li>Image Matching &amp; Stitching: Used in panoramas.  </li> <li>Object Tracking: Predicts movement using optical flow.  </li> <li>Clustering (K-Means): Segments objects into groups.  </li> </ul>"},{"location":"computer-vision/#high-level-processing","title":"High-Level Processing","text":"<ul> <li>Involves complex scene understanding and AI-based interpretations.  </li> <li>Integrates low-level features for semantic meaning.  </li> <li>Examples:  </li> <li>Semantic Segmentation: Classifies pixels into categories (e.g., road, car, tree).  </li> <li>Instance Segmentation: Differentiates between multiple objects of the same type.  </li> <li>Panoptic Segmentation: Combines semantic + instance segmentation.  </li> <li>Deep Learning for Segmentation: Uses CNNs &amp; RNNs for precise object recognition.  </li> <li>Theme Understanding: Identifies scene context (e.g., marketplace vs. garden).  </li> <li>Visual Question Answering (VQA): AI interprets images + text-based queries.  </li> </ul>"},{"location":"computer-vision/#applications-of-computer-vision_1","title":"Applications of Computer Vision","text":"<ul> <li>Self-Driving Cars: Detects lanes, pedestrians, and traffic signs.  </li> <li>Facial Recognition: Matches faces for identity verification.  </li> <li>Medical Imaging: Detects diseases in X-rays &amp; MRIs.  </li> <li>Manufacturing: Identifies defective products.  </li> <li>Retail &amp; Banking: Uses OCR, fraud detection, and customer authentication.</li> </ul>"},{"location":"computer-vision/#deep-learning-for-image-segmentation","title":"Deep Learning for Image Segmentation","text":"<p>\ud83d\udd39 Convolutional Neural Networks (CNNs) play a crucial role in image segmentation, enabling computers to understand objects at a pixel level.</p> <p>How Deep Learning Works for Image Segmentation:</p> <ul> <li>1\ufe0f\u20e3 Feature Extraction with CNNs: CNNs process images and detect key features for segmentation.</li> <li>2\ufe0f\u20e3 Object Localization with Region Proposal Network (RPN): Identifies potential object locations and generates bounding boxes.</li> <li>3\ufe0f\u20e3 Refining Features from the Region of Interest (ROI): CNN extracts features within the bounding box.</li> <li>4\ufe0f\u20e3 Instance Segmentation using Fully Convolutional Networks (FCN): The extracted features are passed into an FCN, which learns to segment the object from its surroundings.</li> <li>5\ufe0f\u20e3 Binary Mask Output: The FCN produces a binary mask, highlighting which pixels belong to the object and which do not.</li> </ul> <p>\ud83d\ude80 From raw images to object masks, deep learning models enhance precision in image segmentation by combining feature extraction, region detection, and pixel-level classification! \ud83d\ude80</p>"},{"location":"computer-vision/#difference-between-low-level-and-high-level-features","title":"Difference Between Low-Level and High-Level Features","text":"Feature Type Low-Level Features \ud83d\uddbc\ufe0f High-Level Features \ud83e\udde0 Content Deals with raw pixel data, detecting edges, textures, and simple patterns. Focuses on object understanding, classification, and relationships. Sensitivity More sensitive to noise, lighting, and orientation changes. More robust, can interpret complex scenes despite variations. Scale Extracted from local regions (e.g., edges in a small part of the image). Global features\u2014considers the entire image for context. Resources Requires fewer system resources, as it focuses on simple features. Uses advanced AI models, requiring higher computation power. Use Cases Applied in image segmentation, feature matching, and object detection. Used in image classification, object recognition, and scene understanding. <p>\ud83d\udd0d Low-level features lay the foundation by detecting shapes and patterns, while high-level features bring context and meaning to an image. Together, they power modern computer vision applications! \ud83d\ude80</p>"},{"location":"computer-vision/#edge-detection","title":"Edge Detection","text":""},{"location":"computer-vision/#sharpening-spatial-filters","title":"Sharpening Spatial Filters","text":"<p>\ud83d\uddbc\ufe0f Purpose: - Removes blurring and enhances edges in images. - Highlights intensity transitions using spatial differentiation. - Image gradients measure the rate of change in pixel intensity, crucial for detecting edges.  </p>"},{"location":"computer-vision/#image-gradients","title":"Image Gradients","text":"<p>\ud83d\udccc Fundamental for computer vision &amp; image processing \ud83d\udd39 Used for: \u2705 Edge detection \u2705 Finding object contours \u2705 Outlining shapes </p> <p>\ud83d\udd39 Computes: - Gradient Magnitude (Strength of the edge) - Gradient Orientation (Direction of the edge)  </p> <p>\u2728 Popular techniques built on image gradients: - Histogram of Oriented Gradients (HOG) - Scale-Invariant Feature Transform (SIFT) </p>"},{"location":"computer-vision/#edge-detection-using-image-gradients","title":"Edge Detection Using Image Gradients","text":"<p>\ud83d\udca1 Gradient computation is a key pre-processing step for edge detection.  </p>"},{"location":"computer-vision/#computing-image-gradients","title":"Computing Image Gradients","text":"<p>The gradient of an image is calculated using finite differences: - Gradient along the vertical direction (\\(G_y\\)):   $$ G_y = f(r+1, c) - f(r-1, c) $$ - Gradient along the horizontal direction (\\(G_x\\)):   $$ G_x = f(r, c+1) - f(r, c-1) $$  </p> <p>\ud83d\udccc Gradient masks (filters) for edge detection: </p> Filter Mask Vertical (\\(G_y\\)) Sobel Filter \\(\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\end{bmatrix}\\) Horizontal (\\(G_x\\)) Sobel Filter \\(\\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\)"},{"location":"computer-vision/#image-gradient-gradient-vector","title":"Image Gradient &amp; Gradient Vector","text":"<p>\ud83d\uddbc\ufe0f Purpose: - Computes rate of change in pixel intensity. - Gradient Magnitude (\\(M\\)) measures the strength of intensity change. - Gradient Orientation (\\(\\alpha\\)) determines direction of the edge. - The matrix size for magnitude and angle is the same as the original image.  </p> <p>\ud83d\udd39 Mathematical Representation: - Gradient Angle (\\(\\alpha\\)):   $$ \\alpha = \\tan^{-1} \\left(\\frac{g_y}{g_x} \\right) $$ - Gradient Magnitude (\\(M\\)):   $$ M = \\sqrt{g_x^2 + g_y^2} $$  </p> <p>\ud83d\udccc Where: - \\(g_x\\) = Gradient in the horizontal direction. - \\(g_y\\) = Gradient in the vertical direction. - \\(\\alpha\\) = Angle between the vertical axis and the edge direction.  </p>"},{"location":"computer-vision/#sobel-filter-for-edge-detection","title":"Sobel Filter for Edge Detection","text":"<p>\ud83d\udca1 Sobel filters compute image gradients using convolution masks.  </p>"},{"location":"computer-vision/#gradient-computation-with-sobel-operator","title":"Gradient Computation with Sobel Operator","text":"<p>\ud83d\udd39 Sobel Operator for Horizontal (\\(G_x\\)) and Vertical (\\(G_y\\)) Gradients: </p> Gradient Direction Filter Mask (Kernel) \\(G_x\\) (Horizontal Gradient) \\(\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\) \\(G_y\\) (Vertical Gradient) \\(\\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\) <p>\ud83d\udd39 Steps to Compute Edge Strength: 1\ufe0f\u20e3 Convolve the image with \\(G_x\\) to compute horizontal changes. 2\ufe0f\u20e3 Convolve the image with \\(G_y\\) to compute vertical changes. 3\ufe0f\u20e3 Compute gradient magnitude and angle using:    $$ M = \\sqrt{G_x^2 + G_y^2} $$    $$ \\alpha = \\tan^{-1} \\left(\\frac{G_y}{G_x} \\right) $$  </p> <p>\ud83d\udccc Why Sobel Filters? \u2705 Enhances edges by detecting gradients in both directions. \u2705 Smooths noise while emphasizing high-frequency intensity changes. \u2705 Used in edge detection algorithms like Canny Edge Detector.  </p>"},{"location":"computer-vision/#gaussian-filter-for-image-smoothing","title":"Gaussian Filter for Image Smoothing","text":"<p>\ud83d\udd39 The Gaussian filter is a smoothing filter that reduces noise and detail in an image. \ud83d\udd39 It applies a Gaussian function to weight pixels, giving higher importance to the center pixel and gradually reducing weights outward.  </p>"},{"location":"computer-vision/#mathematical-representation","title":"Mathematical Representation","text":"<p>The Gaussian function for a 2D image is: $$ G_\\sigma(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} $$ where: - \\( \\sigma \\) = Standard deviation (controls blurring strength) - \\( x, y \\) = Pixel coordinates  </p> <p>\ud83d\udccc Higher \\( \\sigma \\) \u2192 More blur \ud83d\udccc Lower \\( \\sigma \\) \u2192 Less blur, preserves more details </p>"},{"location":"computer-vision/#gaussian-filter-kernels","title":"Gaussian Filter Kernels","text":"Filter Size Kernel Matrix (Normalized) 3\u00d73 (\u03c3 = 1) \\( \\frac{1}{16} \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\) 5\u00d75 (\u03c3 = 1) \\( \\frac{1}{330} \\begin{bmatrix} 1 &amp; 4 &amp; 7 &amp; 4 &amp; 1 \\\\ 4 &amp; 20 &amp; 33 &amp; 20 &amp; 4 \\\\ 7 &amp; 33 &amp; 54 &amp; 33 &amp; 7 \\\\ 4 &amp; 20 &amp; 33 &amp; 20 &amp; 4 \\\\ 1 &amp; 4 &amp; 7 &amp; 4 &amp; 1 \\end{bmatrix} \\) 5\u00d75 (\u03c3 = 2) \\( \\frac{1}{34} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix} \\) <p>\ud83d\udccc Larger filter size \u2192 More blur &amp; smoothing \ud83d\udccc Smaller filter size \u2192 Preserves more details </p>"},{"location":"computer-vision/#laplacian-filter-for-edge-detection","title":"Laplacian Filter for Edge Detection","text":"<p>\ud83d\udd39 The Laplacian filter is a second-order derivative filter used for edge detection. \ud83d\udd39 It highlights regions of rapid intensity change by computing the second derivative of an image.  </p>"},{"location":"computer-vision/#mathematical-representation_1","title":"Mathematical Representation","text":"<p>The Laplacian operator is given by:  </p> \\[ \\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\\] <p>where \\( f(x, y) \\) is the image intensity at a given point.</p>"},{"location":"computer-vision/#laplacian-filter-kernels","title":"Laplacian Filter Kernels","text":"Filter Type Kernel Matrix 4-Neighbor Laplacian \\( \\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ -1 &amp; 4 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix} \\) 8-Neighbor Laplacian \\( \\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ -1 &amp; 8 &amp; -1 \\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix} \\) <p>\ud83d\udd39 The 4-neighbor Laplacian considers only direct neighbors, while the 8-neighbor Laplacian accounts for diagonal edges as well.</p>"},{"location":"computer-vision/#unit-2","title":"Unit 2","text":""},{"location":"computer-vision/#image-features-in-computer-vision","title":"Image Features in Computer Vision","text":"<p>\ud83d\udd39 Image features are key elements that help in object recognition, segmentation, and analysis.  </p>"},{"location":"computer-vision/#types-of-image-features","title":"Types of Image Features","text":"<p>\u2705 Edges \u2013 Identifies boundaries between objects. \u2705 Color \u2013 Extracts information based on pixel intensity. \u2705 Texture \u2013 Analyzes surface patterns and structures. \u2705 Object Boundaries \u2013 Detects outlines and contours of objects. \u2705 Object Shape \u2013 Defines geometric properties of an object.  </p> <p>\ud83d\udd39 Good Features Should Be: - \u2705 Unique &amp; Distinctive \u2013 Helps differentiate between objects. - \u2705 Non-redundant \u2013 Avoids duplicate or unnecessary information. - \u2705 Robust \u2013 Works well under noise and illumination changes. - \u2705 Global Representation \u2013 Captures scene-wide characteristics, not just local details.  </p>"},{"location":"computer-vision/#gradient-based-features","title":"Gradient-Based Features","text":"<p>Gradient-based techniques detect changes in pixel intensity, which highlight object edges and textures.  </p> <p>\ud83d\udd39 Popular Techniques: - DoG (Difference of Gaussian) - LoG (Laplacian of Gaussian) - HoG (Histogram of Oriented Gradients) - SIFT (Scale-Invariant Feature Transform) - SURF (Speeded-Up Robust Features) </p> <p>\ud83d\udccc Advantages: \u2705 Invariant to small shifts &amp; rotations \u2013 Ensures stability under transformations. \u2705 Localized histograms \u2013 Offers better spatial information compared to global histograms. \u2705 Contrast normalization \u2013 Reduces the impact of variable illumination.  </p>"},{"location":"computer-vision/#difference-of-gaussian-dog","title":"Difference of Gaussian (DoG)","text":"<p>\ud83d\udccc A feature enhancement technique used for blob detection &amp; SIFT descriptors.  </p>"},{"location":"computer-vision/#how-dog-works","title":"How DoG Works:","text":"<p>1\ufe0f\u20e3 Apply Gaussian Blur \u2013 Smoothens the image using two Gaussian filters with different sigma values (\u03c3\u2081 &amp; \u03c3\u2082). 2\ufe0f\u20e3 Subtract the Two Blurred Images \u2013 Enhances regions with specific frequency details. 3\ufe0f\u20e3 Suppress High-Frequency Details \u2013 Reduces random noise but preserves important structures.  </p> <p>\ud83d\udd39 Mathematical Representation: $$ DoG = G_{\\sigma_1} * I - G_{\\sigma_2} * I $$</p> <p>where: - \\( I \\) = Original grayscale image - \\( G_{\\sigma_1}, G_{\\sigma_2} \\) = Gaussian filters with different standard deviations  </p> <p>\ud83d\udccc Pros &amp; Cons: \u2705 Reduces noise while preserving edges \u2705 Enhances spatial features \u274c Reduces overall image contrast </p>"},{"location":"computer-vision/#laplacian-of-gaussian-log-edge-detection-feature-enhancement","title":"Laplacian of Gaussian (LoG) \u2013 Edge Detection &amp; Feature Enhancement","text":"<p>\ud83d\udd39 Laplacian of Gaussian (LoG) is a feature detection technique that combines: 1\ufe0f\u20e3 Gaussian Smoothing \u2013 Reduces noise in the image. 2\ufe0f\u20e3 Laplacian Operator \u2013 Detects edges and blobs by identifying intensity changes.  </p>"},{"location":"computer-vision/#how-log-works","title":"How LoG Works:","text":"<ol> <li>Apply a Gaussian filter to smooth the image and suppress noise.  </li> <li>Compute the second derivative (Laplacian) to highlight regions with rapid intensity changes (edges).  </li> <li>Detect zero-crossings in the Laplacian response to identify edges.  </li> </ol>"},{"location":"computer-vision/#mathematical-representation_2","title":"Mathematical Representation:","text":"<p>The LoG function is given by: $$ LoG(x, y) = \\nabla^2 G_{\\sigma} (x, y) * I(x, y) $$ where: - \\( G_{\\sigma} (x, y) \\) = Gaussian filter with standard deviation \\( \\sigma \\) - \\( \\nabla^2 \\) = Laplacian operator (second derivative) - \\( I(x, y) \\) = Input image </p>"},{"location":"computer-vision/#key-features-of-log","title":"Key Features of LoG:","text":"<p>\u2705 Combines smoothing &amp; edge detection in one step. \u2705 Detects both fine and coarse details depending on \\( \\sigma \\). \u2705 Useful for blob detection in feature descriptors like SIFT. \u274c Sensitive to noise \u2013 Requires pre-smoothing for better results.  </p> <p>\ud83d\ude80 LoG is commonly used in edge detection pipelines like the Marr-Hildreth operator and as a preprocessing step in Computer Vision applications! \ud83d\udd0d</p>"},{"location":"computer-vision/#histogram-of-oriented-gradients-hog-feature-descriptor-for-object-detection","title":"Histogram of Oriented Gradients (HoG) \u2013 Feature Descriptor for Object Detection","text":"<p>\ud83d\udd39 Histogram of Oriented Gradients (HoG) is a feature descriptor used for object detection and image classification by analyzing gradient orientations in localized regions of an image.  </p>"},{"location":"computer-vision/#step-by-step-hog-computation","title":"Step-by-Step HoG Computation","text":"<p>\u2705 Step 1: Resize Image - Resize the image to an integer multiple of 8 (nearest to the original size). - Ensures uniform cell division and efficient computation.  </p> <p>\u2705 Step 2: Divide Image into Cells - Split the image into small patches of equal size (e.g., 8\u00d78 pixels per cell). - Each cell will have its own gradient histogram.  </p> <p>\u2705 Step 3: Compute Gradients - Calculate the gradient magnitude and orientation using Sobel filters:   [   M = \\sqrt{G_x^2 + G_y^2}, \\quad \\theta = \\tan^{-1} \\left(\\frac{G_y}{G_x} \\right)   ]   where \\( G_x, G_y \\) are gradients along horizontal and vertical directions.  </p> <p>\u2705 Step 4: Compute Gradient Histograms (Per Cell) - For each 8\u00d78 cell, create a histogram of gradients (e.g., 9 bins for 0\u00b0-180\u00b0). - Assign gradient magnitudes to their corresponding orientation bins.  </p> <p>\u2705 Step 5: Construct Feature Vector - Normalize the histograms across neighboring blocks (e.g., 2\u00d72 cells per block) for better illumination invariance. - Flatten the computed HoG features into a single feature vector for classification.  </p> <p>\u2705 Step 6: Visualize HoG - HoG features are often visualized as a grid of arrows, where the length and direction represent gradient strength and orientation.  </p> <p>\u2705 Step 7: Classify Images - Use machine learning models (SVM, Deep Learning) to classify objects using the extracted HoG feature vector.  </p>"},{"location":"computer-vision/#mathematical-representation_3","title":"Mathematical Representation:","text":"<ul> <li> <p>Gradient Magnitude (\\(M\\)): $$   M = \\sqrt{G_x^2 + G_y^2} $$ </p> </li> <li> <p>Gradient Orientation (\\(\\theta\\)): $$   \\theta = \\tan^{-1} \\left(\\frac{G_y}{G_x} \\right) $$  where:  </p> </li> <li>\\( G_x, G_y \\) = Gradients in horizontal &amp; vertical directions.  </li> <li>\\( M \\) = Strength of edge response.  </li> <li>\\( \\theta \\) = Edge direction (0\u00b0\u2013180\u00b0 or 0\u00b0\u2013360\u00b0 bins).  </li> </ul>"},{"location":"computer-vision/#key-features-of-hog","title":"Key Features of HoG:","text":"<p>\u2705 Invariance to Illumination &amp; Shadows \u2013 Normalization removes intensity variations. \u2705 Captures Local Shape Information \u2013 Focuses on edges and textures rather than pixel intensity. \u2705 Robust to Small Translations &amp; Rotations \u2013 Uses histograms instead of raw gradients. \u2705 Widely Used in Object Detection \u2013 Forms the basis of Dalal-Triggs pedestrian detection and is used in SVM-based image recognition. \u274c Computationally Expensive \u2013 Requires dense gradient computations across the entire image.  </p> <p>\ud83d\ude80 HoG is widely used in Human &amp; Object Detection (e.g., Pedestrian Detection in self-driving cars) and Machine Learning-based Image Classification! \ud83d\udd0d</p>"},{"location":"computer-vision/#feature-descriptors-in-computer-vision","title":"Feature Descriptors in Computer Vision","text":"<p>\ud83d\udd39 Feature descriptors help identify key points, edges, and corners in an image. \ud83d\udd39 These descriptors are used for object detection, image matching, and recognition.  </p>"},{"location":"computer-vision/#types-of-feature-descriptors","title":"Types of Feature Descriptors","text":""},{"location":"computer-vision/#1-global-descriptors","title":"1. Global Descriptors \ud83c\udf0d","text":"<ul> <li>Represent the entire image.  </li> <li>Examples:   \u2705 Histogram of Oriented Gradients (HoG)   \u2705 Difference of Gaussian (DoG)   \u2705 Histogram of Optical Flow (HOF) </li> <li>Limitations: Struggle with occlusions and profile variations since they analyze the whole image.  </li> </ul>"},{"location":"computer-vision/#2-local-descriptors","title":"2. Local Descriptors \ud83d\udd0d","text":"<ul> <li>Describe small patches within an image.  </li> <li>More accurate &amp; robust for object detection, matching, and occlusion handling.  </li> <li>Examples:   \u2705 SIFT (Scale-Invariant Feature Transform)   \u2705 SURF (Speeded-Up Robust Features)   \u2705 LBP (Local Binary Pattern)   \u2705 BRISK (Binary Robust Invariant Scalable Keypoints)   \u2705 MSER (Maximally Stable Extremal Regions)   \u2705 FREAK (Fast Retina Keypoint) </li> </ul> <p>\ud83d\udccc Local descriptors outperform global ones in real-world applications like facial recognition and object tracking! \ud83d\ude80  </p>"},{"location":"computer-vision/#how-to-define-an-interest-point","title":"How to Define an Interest Point?","text":"<p>\ud83d\udd39 Interest points are key locations (e.g., edges, corners) where features can be extracted.  </p> <p>\u2705 Repeatability: - A feature should be detected consistently across multiple images, despite geometric &amp; photometric transformations.  </p> <p>\u2705 Saliency: - Features should be distinct and unique to avoid mismatches.  </p> <p>\u2705 Compactness: - Fewer features than the number of image pixels should effectively represent the image.  </p> <p>\u2705 Efficiency: - Fast computation is essential for real-time applications like tracking &amp; detection.  </p> <p>\u2705 Locality: - Features should occupy a small area and remain robust to clutter &amp; occlusion.  </p> <p>\u2705 Covariance: - Features should be detectable despite geometric &amp; photometric variations (e.g., rotation, lighting changes).  </p>"},{"location":"computer-vision/#sift-algorithm-scale-invariant-feature-transform","title":"SIFT Algorithm \u2013 Scale-Invariant Feature Transform","text":"<p>SIFT is a feature detection algorithm that extracts scale and rotation-invariant keypoints for object recognition, tracking, and image matching.  </p>"},{"location":"computer-vision/#step-1-construct-a-scale-space","title":"Step 1: Construct a Scale Space","text":"<p>\ud83d\udccc Why? - Real-world objects appear different at various distances (scales). - A feature must be detectable at multiple scales to be useful in recognition.  </p> <p>\ud83d\udd39 How it Works: 1. The original image is repeatedly blurred using a Gaussian filter. 2. Octaves are created by downsampling the image (reducing its size by half). 3. Within each octave, multiple blurred images are generated with increasing sigma values (\u03c3). 4. This scale-space representation ensures features are scale-independent.  </p> <p>\ud83d\udd39 Mathematical Formulation (Gaussian Blur): [ G(x, y, \\sigma) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} ] where: - \\( G(x, y, \\sigma) \\) = Gaussian function. - \\( \\sigma \\) = Standard deviation (controls blurring). - \\( x, y \\) = Pixel coordinates.  </p> <p>\ud83d\udd39 Example: - Octave 1: Original image + multiple blurred versions. - Octave 2: Image resized to half and blurred again. - Repeats for multiple octaves (typically 4-5 octaves).  </p> <p>\ud83d\udccc Outcome: - A collection of images at different scales and resolutions.  </p>"},{"location":"computer-vision/#step-2-compute-difference-of-gaussian-dog","title":"Step 2: Compute Difference of Gaussian (DoG)","text":"<p>\ud83d\udccc Why? - Identifies keypoints by enhancing edges and texture features. - The Gaussian Blur removes noise, and the DoG highlights changes in intensity.  </p> <p>\ud83d\udd39 How it Works: 1. Compute DoG images by subtracting two consecutive Gaussian-blurred images:    [    DoG(x, y, \\sigma) = G(x, y, k\\sigma) - G(x, y, \\sigma)    ]    where \\( k \\) is a constant (typically \\( k = \\sqrt{2} \\)). 2. This process is repeated across all octaves. 3. The resulting DoG images enhance edges, blobs, and texture details.  </p> <p>\ud83d\udccc Outcome: - A set of DoG images that highlight regions of interest (potential keypoints).  </p>"},{"location":"computer-vision/#step-3-keypoint-localization","title":"Step 3: Keypoint Localization","text":"<p>\ud83d\udccc Why? - Identify stable keypoints while removing weak or false detections.  </p> <p>\ud83d\udd39 How it Works: 1. Each pixel in the DoG images is compared with 26 neighboring pixels (8 in the same image, 9 in the scale above, and 9 in the scale below). 2. If a pixel is the local maximum or minimum, it is marked as a potential keypoint. 3. Low-contrast keypoints are discarded using a threshold (typically 0.03). 4. Edges are removed using the Hessian matrix determinant to avoid unstable keypoints.  </p> <p>\ud83d\udd39 Mathematical Filtering (Hessian Matrix): [ H = \\begin{bmatrix} I_{xx} &amp; I_{xy} \\ I_{xy} &amp; I_{yy} \\end{bmatrix} ] - Compute corner response:   [   \\frac{(\\text{Trace}(H))^2}{\\text{Det}(H)} &lt; 12.1   ]   If the value is greater than 12.1, the keypoint is rejected.  </p> <p>\ud83d\udccc Outcome: - A set of highly stable, contrast-rich keypoints that can be used for further processing.  </p>"},{"location":"mc-m2/","title":"Mobile Computing","text":"<p> hr-subflow.txt</p> <p>hi </p>"},{"location":"mc-viva/","title":"Mc viva","text":""},{"location":"mc-viva/#summary-of-mobile-generations","title":"Summary of Mobile Generations","text":"Generation Technology Used Speed Key Features 1G AMPS (Analog) 2.4 kbps Voice calls, poor quality, no data 2G GSM &amp; CDMA 144 kbps Digital voice, SMS, MMS, basic internet 2.5G GPRS 64 \u2013 144 kbps Always-on internet, emails, camera phones 2.75G EDGE Up to 384 kbps Faster browsing, video streaming 3G WCDMA, HSPA Up to 2 Mbps Video calls, multimedia, web-based apps 4G LTE, WiMAX 10 Mbps \u2013 1 Gbps HD streaming, VoLTE, broadband connectivity 5G mmWave, MIMO Up to 20 Gbps Ultra-fast speeds, IoT, AI-powered networks"},{"location":"mc-viva/#mobile-architecture","title":"Mobile Architecture","text":""},{"location":"mc-viva/#experiment-1","title":"Experiment 1","text":""},{"location":"mc-viva/#1-how-will-user-interfaces-evolve-beyond-touch-and-voice-could-thought-based-interaction-become-a-reality-what-role-might-augmented-reality-play-in-the-future-of-mobile-computing-interfaces","title":"1. How will user interfaces evolve beyond touch and voice? Could thought-based interaction become a reality? What role might augmented reality play in the future of mobile computing interfaces?","text":"<p>Thought-Based Interaction includes Brain-Computer Interfaces (BCIs), which are a promising area of research. Advances in neural decoding and wearable EEG devices suggest that thought-based interactions could become a reality, allowing users to control devices directly with their thoughts. Early applications might include accessibility tools for people with disabilities, gaming, and hands-free control in AR/VR environments.</p> <p>Augmented Reality (AR) could revolutionize interfaces by overlaying contextual information onto the real world. With AR glasses and contact lenses, mobile computing might seamlessly blend physical and digital spaces, enabling hands-free interactions, holographic displays, and real-time guidance for tasks.</p>"},{"location":"mc-viva/#2-could-we-develop-a-truly-decentralized-mobile-network-that-operates-without-any-central-infrastructure","title":"2. Could we develop a truly decentralized mobile network that operates without any central infrastructure?","text":"<p>A fully decentralized mobile network is conceivable using technologies like:</p> <ul> <li>Mesh Networking: Devices communicate directly with each other, forming a peer-to-peer network without relying on centralized infrastructure.</li> <li>Blockchain: Securely manage identity, transactions, and bandwidth sharing in a decentralized system.</li> <li>Edge Computing: Offload processing to local devices or micro-data centers, reducing dependence on centralized servers.</li> </ul>"},{"location":"mc-viva/#3-how-could-mobile-networks-be-designed-to-remain-functional-during-major-disasters-or-in-extreme-environments-like-underwater-or-in-space","title":"3. How could mobile networks be designed to remain functional during major disasters or in extreme environments like underwater or in space?","text":""},{"location":"mc-viva/#mobile-networks-for-extreme-conditions","title":"Mobile Networks for Extreme Conditions","text":"<ul> <li>Disaster Scenarios: Networks like FirstNet and mesh-based systems can provide emergency communication during disasters. Deployable base stations and satellite uplinks are critical.</li> <li>Underwater Communication: Acoustic and optical communication technologies are being developed for underwater mobile networks, albeit with limited range and speed compared to terrestrial systems.</li> <li>Space Communication: Inter-satellite communication networks (ISL) and high-throughput satellites like Starlink can create reliable networks in space.</li> </ul>"},{"location":"mc-viva/#4-how-might-6g-technology-evolve-beyond-5g-and-what-new-applications-could-emerge-from-its-potential-capabilities","title":"4. How might 6G technology evolve beyond 5G, and what new applications could emerge from its potential capabilities?","text":"<p>Beyond 5G, 6G aims to deliver terabit-level speeds, ultra-low latency (&lt;1ms), and high energy efficiency. 6G could offer speeds up to 100 times faster than 5G.</p>"},{"location":"mc-viva/#features-and-potential-applications","title":"Features and Potential Applications:","text":"<ul> <li>Holographic communications for immersive telepresence.</li> <li>High-precision IoT for smart cities and autonomous systems.</li> <li>Real-time, AI-driven decision-making in sectors like healthcare and finance.</li> <li>Advanced AR/VR experiences with tactile feedback and ultra-high-resolution visuals.</li> </ul>"},{"location":"mc-viva/#5-what-are-the-potential-implications-of-mobile-computing-on-personal-privacy-and-how-can-we-balance-convenience-with-security-in-future-mobile-technologies","title":"5. What are the potential implications of mobile computing on personal privacy, and how can we balance convenience with security in future mobile technologies?","text":""},{"location":"mc-viva/#implications","title":"Implications:","text":"<p>Increased data collection by mobile devices raises concerns about surveillance, misuse of personal information, and identity theft.</p>"},{"location":"mc-viva/#solutions","title":"Solutions:","text":"<ul> <li>Privacy-by-Design: Embedding privacy principles into technology development.</li> <li>Decentralized Identity Systems: Users retain control over their digital identities.</li> <li>Encryption: End-to-end encryption for all communications and data storage.</li> <li>Regulation and Transparency: Stronger data protection laws and transparent policies on data usage.</li> <li>User Education: Empower users with tools and knowledge to manage their data and privacy settings effectively.</li> </ul>"},{"location":"mc-viva/#network-architecture","title":"Network architecture","text":"<p>Questions of Curiosity: 1.  Virtualized Networks and Cloud-Native Architectures: With virtualization and cloud-native architectures, components like the Base Station Controller (BSC) and Mobile Switching Center (MSC) could be moved to the cloud, offering more flexibility, scalability, and easier management of resources without the need for dedicated physical hardware. 2.  Artificial Intelligence Integration: AI could be integrated into these components to predict traffic patterns, optimize resource allocation, automate network management, and improve decision-making, leading to more efficient and responsive network operations. 3.  Support for Emerging Technologies: To support IoT and machine-to-machine communication, these components will evolve to handle massive device connections, low-latency communication, and efficient data routing, ensuring that networks can manage a higher volume of devices and more diverse traffic. 4.  Security Challenges: The distributed nature of these components increases vulnerability to cyberattacks and data breaches. Future implementations could address this by incorporating advanced encryption, real-time monitoring, and decentralized security protocols to ensure data integrity and network protection.</p>"},{"location":"mc-viva/#cellular-architecture","title":"\ud83d\udce1 Cellular Architecture","text":"<p>The cellular network is structured in a hierarchical way to provide efficient communication across large geographical areas.  </p> <p> </p> <p>\ud83d\udd39 Key Components: \u2705 Mobile Device (User Equipment) \u2013 Phones, tablets, or IoT devices. \u2705 Base Transceiver Station (BTS) \u2013 Handles wireless communication with mobile devices. \u2705 Base Station Controller (BSC) \u2013 Manages multiple BTS and assigns frequencies. \u2705 Mobile Switching Center (MSC) \u2013 Connects mobile calls and manages handovers. \u2705 Public Switched Telephone Network (PSTN) \u2013 Traditional wired telephone network. \u2705 Packet Data Network (Internet) \u2013 Allows mobile data access and browsing. \u2705 Cell Towers \u2013 Divides the service area into small cells to provide coverage.  </p>"},{"location":"mc-viva/#mode-of-communications","title":"Mode of Communications","text":"<pre><code>graph TD;\n\n    subgraph Circuit-Switched Network\n        A((User A)) -- Dedicated Path --&gt; B((User B))\n    end\n\n    subgraph Packet-Switched Network\n        A1((User C)) -- Packet 1 --&gt; C1[Router] --&gt; D1((User D))\n        A1 -- Packet 2 --&gt; C2[Router] --&gt; D1\n        A1 -- Packet 3 --&gt; C3[Router] --&gt; D1\n    end</code></pre>"},{"location":"mc-viva/#questions-of-curiosity","title":"Questions of Curiosity","text":""},{"location":"mc-viva/#1-factors-affecting-call-blocking-probability","title":"1. Factors Affecting Call Blocking Probability","text":"<p>Call blocking probability (CBP) depends on network load, resource availability, user density, traffic patterns, call setup efficiency, and handover management. High congestion, inadequate spectrum, and poor mobility handling increase CBP, especially in urban areas.</p>"},{"location":"mc-viva/#2-impact-of-network-slicing-on-cbp","title":"2. Impact of Network Slicing on CBP","text":"<p>Network slicing optimizes resource allocation by creating virtual networks for different use cases. It ensures priority for critical services, enhances QoS, and isolates traffic, reducing congestion and minimizing call blocking.</p>"},{"location":"mc-viva/#3-industry-benchmarks-for-call-blocking","title":"3. Industry Benchmarks for Call Blocking","text":"<p>Industry standards, such as ITU and 3GPP, recommend CBP levels below 2% for quality service, with 4G networks aiming for ~1%. Newer technologies like 5G set even lower targets to ensure high reliability and low latency.</p>"},{"location":"mc-viva/#4-geographical-influence-on-cbp","title":"4. Geographical Influence on CBP","text":"<p>Urban areas experience high CBP due to user density, while rural areas suffer from inadequate infrastructure. Factors like peak-hour traffic, signal obstructions, and sparse cell coverage also impact call blocking rates.</p>"},{"location":"mc-viva/#5-strategies-to-reduce-cbp","title":"5. Strategies to Reduce CBP","text":"<p>Minimization strategies include: - Cell Load Balancing: Redistributes traffic to avoid congestion. - Advanced Technologies: Carrier aggregation, MIMO, and small cells improve spectrum efficiency. - Alternative Networks: Wi-Fi/5G offloading reduces macro network strain. - AI-Driven Optimization: Predictive resource management mitigates congestion proactively.</p>"},{"location":"mc-viva/#6-hidden-node-problem-and-avoidance-strategies","title":"6. Hidden Node Problem and Avoidance Strategies","text":""},{"location":"mc-viva/#what-are-the-ways-in-which-hidden-node-problems-are-avoided","title":"What are the ways in which hidden node problems are avoided?","text":"<ul> <li>RTS/CTS Mechanism: The Request To Send/Clear To Send (RTS/CTS) protocol helps prevent collisions by coordinating node transmissions.</li> <li>Increasing Transmission Power: Extending node communication range can reduce hidden node occurrences.</li> <li>Using Omnidirectional Antennas: Ensures better connectivity among nodes.</li> <li>Node Relocation: Physically repositioning nodes enhances communication.</li> <li>Protocol Enhancements: Polling and token-passing strategies improve medium access management.</li> </ul>"},{"location":"mc-viva/#what-is-the-exposed-node-problem-how-can-you-avoid-this","title":"What is the exposed node problem? How can you avoid this?","text":"<ul> <li>Definition: The exposed node problem occurs when a node refrains from sending packets due to perceived interference, even when transmission is possible.</li> <li>Avoidance Strategies:</li> <li>Carrier Sense Multiple Access (CSMA): Helps avoid unnecessary transmission blocking.</li> <li>RTS/CTS Mechanism: Coordinates transmissions to minimize interference.</li> </ul>"},{"location":"mc-viva/#how-does-the-hidden-node-problem-impact-different-types-of-network-traffic","title":"How does the hidden node problem impact different types of network traffic?","text":"<ul> <li>Real-time Video Traffic: Sensitive to delays and packet loss, leading to degraded quality.</li> <li>File Transfer Traffic: Collisions cause retransmissions, slowing down transfer rates.</li> </ul>"},{"location":"mc-viva/#what-role-does-transmission-power-control-play-in-mitigating-hidden-node-problems","title":"What role does transmission power control play in mitigating hidden node problems?","text":"<ul> <li>Adjusting power levels helps nodes detect each other, reducing hidden node occurrences while managing interference.</li> </ul>"},{"location":"mc-viva/#how-do-different-wireless-standards-80211abgnac-handle-the-hidden-node-problem-differently","title":"How do different wireless standards (802.11a/b/g/n/ac) handle the hidden node problem differently?","text":"Standard Handling Method 802.11a Uses RTS/CTS; limited effectiveness due to shorter range. 802.11b RTS/CTS implementation; better range but still vulnerable. 802.11g RTS/CTS support with backward compatibility; performance varies. 802.11n MIMO technology enhances range and mitigates hidden node issues. 802.11ac Beamforming and MIMO optimize signal paths, reducing problems."},{"location":"mc-viva/#1-if-we-modified-the-simulation-to-include-bidirectional-traffic-where-wireless-nodes-communicate-with-each-other-instead-of-just-with-node-1-how-would-this-affect-network-congestion-and-overall-performance-propose-a-test-setup-to-compare-unidirectional-vs-bidirectional-communication-patterns-with-the-same-number-of-nodes","title":"1. If we modified the simulation to include bidirectional traffic (where wireless nodes communicate with each other instead of just with Node 1), how would this affect network congestion and overall performance? Propose a test setup to compare unidirectional vs bidirectional communication patterns with the same number of nodes.","text":"<p>Effect: Bidirectional traffic would increase network congestion due to the added communication paths between nodes, leading to higher contention for bandwidth. It could also introduce delays due to more frequent collisions or retransmissions.</p> <p>Test Setup: Compare unidirectional vs. bidirectional traffic with the same number of nodes, measuring throughput, delay, and packet loss for both configurations. Use the same traffic load and node density in both setups to ensure a fair comparison.</p>"},{"location":"mc-viva/#2-how-would-implementing-different-packet-sizes-in-the-application-properties-affect-network-performance-across-the-varying-number-of-nodes-design-a-test-scenario-to-compare-the-impact-of-small-256-bytes-versus-large-1500-bytes-packet-sizes-on-throughput-and-delay","title":"2. How would implementing different packet sizes in the application properties affect network performance across the varying number of nodes? Design a test scenario to compare the impact of small (256 bytes) versus large (1500 bytes) packet sizes on throughput and delay.","text":"<p>Effect: Larger packet sizes (1500 bytes) would typically result in higher throughput but could cause longer delays due to increased transmission time. Smaller packets (256 bytes) result in lower throughput but might offer better responsiveness with reduced latency.</p> <p>Test Scenario: Set up two scenarios with varying packet sizes: one with 256-byte packets and the other with 1500-byte packets. Measure throughput and delay while increasing the number of nodes to observe the impact of packet size on network performance.</p>"},{"location":"mc-viva/#3-what-impact-would-environmental-factors-like-walls-interference-from-other-devices-or-distance-between-nodes-have-on-the-network-performance-if-we-had-enabled-path-loss-in-the-wireless-link-properties-how-does-this-relate-to-real-world-implementations","title":"3. What impact would environmental factors (like walls, interference from other devices, or distance between nodes) have on the network performance if we had enabled path loss in the wireless link properties? How does this relate to real-world implementations?","text":"<p>Effect: Environmental factors like walls, interference, or distance can significantly affect signal strength, leading to higher packet loss and reduced throughput. Path loss models simulate this by weakening the signal based on distance and obstacles.</p> <p>Real-world Relation: This mimics real-world wireless networks, where physical obstructions and interference reduce communication quality and range, thus affecting performance (e.g., in offices, homes, or urban areas).</p>"},{"location":"mc-viva/#4-why-was-tcp-disabled-in-the-transport-layer-for-this-experiment-how-would-the-results-differ-if-tcp-was-enabled-and-what-additional-network-characteristics-would-we-be-able-to-observe","title":"4. Why was TCP disabled in the transport layer for this experiment? How would the results differ if TCP was enabled, and what additional network characteristics would we be able to observe?","text":"<p>Reason for Disablement: TCP was likely disabled to focus on the raw performance of the network layer without the influence of transport layer reliability (e.g., retransmissions, congestion control).</p> <p>Effect of Enabling TCP: If TCP were enabled, we would see added overhead due to retransmissions and flow control, possibly lowering throughput but improving reliability and reducing packet loss. We could also observe network congestion more clearly.</p>"},{"location":"mc-viva/#5-if-we-enable-path-loss-in-wireless-link-properties-how-would-it-affect-the-number-of-successful-transmissions","title":"5. If we enable path loss in wireless link properties, how would it affect the number of successful transmissions?","text":"<p>Impact: Path loss would result in fewer successful transmissions as the signal strength diminishes with distance and obstacles. This would likely increase packet loss and require more retransmissions, reducing overall network efficiency.</p>"},{"location":"mc-viva/#network-performance-analysis","title":"Network Performance Analysis","text":""},{"location":"mc-viva/#1-effect-of-bidirectional-traffic-on-network-congestion-and-performance","title":"1. Effect of Bidirectional Traffic on Network Congestion and Performance","text":"<p>Effect: Bidirectional traffic would increase network congestion due to the added communication paths between nodes, leading to higher contention for bandwidth. It could also introduce delays due to more frequent collisions or retransmissions.</p> <p>Test Setup: Compare unidirectional vs. bidirectional traffic with the same number of nodes, measuring throughput, delay, and packet loss for both configurations. Use the same traffic load and node density in both setups to ensure a fair comparison.</p>"},{"location":"mc-viva/#2-impact-of-different-packet-sizes-on-network-performance","title":"2. Impact of Different Packet Sizes on Network Performance","text":"<p>Effect: Larger packet sizes (1500 bytes) would typically result in higher throughput but could cause longer delays due to increased transmission time. Smaller packets (256 bytes) result in lower throughput but might offer better responsiveness with reduced latency.</p> <p>Test Scenario: Set up two scenarios with varying packet sizes: one with 256-byte packets and the other with 1500-byte packets. Measure throughput and delay while increasing the number of nodes to observe the impact of packet size on network performance.</p>"},{"location":"mc-viva/#3-impact-of-environmental-factors-with-path-loss-enabled","title":"3. Impact of Environmental Factors with Path Loss Enabled","text":"<p>Effect: Environmental factors like walls, interference, or distance can significantly affect signal strength, leading to higher packet loss and reduced throughput. Path loss models simulate this by weakening the signal based on distance and obstacles.</p> <p>Real-world Relation: This mimics real-world wireless networks, where physical obstructions and interference reduce communication quality and range, thus affecting performance (e.g., in offices, homes, or urban areas).</p>"},{"location":"mc-viva/#4-disabling-tcp-in-the-transport-layer","title":"4. Disabling TCP in the Transport Layer","text":"<p>Reason for Disablement: TCP was likely disabled to focus on the raw performance of the network layer without the influence of transport layer reliability (e.g., retransmissions, congestion control).</p> <p>Effect of Enabling TCP: If TCP were enabled, we would see added overhead due to retransmissions and flow control, possibly lowering throughput but improving reliability and reducing packet loss. We could also observe network congestion more clearly.</p>"},{"location":"mc-viva/#5-effect-of-path-loss-on-successful-transmissions","title":"5. Effect of Path Loss on Successful Transmissions","text":"<p>Impact: Path loss would result in fewer successful transmissions as the signal strength diminishes with distance and obstacles. This would likely increase packet loss and require more retransmissions, reducing overall network efficiency.</p>"},{"location":"mobile-computing/","title":"Mobile computing","text":""},{"location":"mobile-computing/#unit-1","title":"Unit 1","text":""},{"location":"mobile-computing/#mobile-computing","title":"Mobile Computing","text":""},{"location":"mobile-computing/#what-is-mobile-computing","title":"What is Mobile Computing?","text":"<p>Mobile computing is a technology that enables the wireless transmission of data, voice, and video through mobile devices without relying on fixed physical connections.  </p>"},{"location":"mobile-computing/#main-components-of-mobile-computing","title":"Main Components of Mobile Computing","text":"<p>\u2705 Mobile Communication - Involves protocols, services, bandwidth, and portals that enable seamless connectivity. - Supports wireless communication over Wi-Fi, Cellular Networks (3G, 4G, 5G), and Bluetooth.  </p> <p>\u2705 Mobile Hardware - Includes portable devices that access mobility services:   - Smartphones \ud83d\udcf1   - Tablets   - Laptops   - Personal Digital Assistants (PDAs)   - Wearable Devices \u231a  </p> <p>\u2705 Mobile Software - Operating systems and applications that run on mobile devices. - Examples: Android, iOS, Windows Mobile, Mobile Web Browsers, and Apps. - Acts as the engine that powers mobile functionalities.  </p>"},{"location":"mobile-computing/#applications-of-mobile-computing","title":"Applications of Mobile Computing","text":"<p>\u2705 Web &amp; Internet Access \u2013 Enables browsing, cloud computing, and real-time communication. \u2705 Global Positioning System (GPS) \u2013 Provides navigation, tracking, and geolocation services. \u2705 Emergency Services \u2013 Supports disaster response, medical alerts, and real-time rescue coordination. \u2705 Entertainment Services \u2013 Powers mobile gaming, streaming platforms, and digital media. \u2705 Educational Services \u2013 Supports e-learning, mobile classrooms, and virtual collaboration.  </p>"},{"location":"mobile-computing/#evolution-of-mobile-computing","title":"Evolution of Mobile Computing","text":"<p>The evolution of mobile generations (G) marks advancements in speed, technology, frequency, data capacity, and latency, revolutionizing communication and connectivity.  </p>"},{"location":"mobile-computing/#first-generation-1g-analog-communication","title":"\ud83d\udce1 First Generation (1G) \u2013 Analog Communication","text":"<p>\ud83d\udcc5 Introduced: 1980s \u2013 1990s \ud83d\udccc Technology Used: AMPS (Advanced Mobile Phone System), based on FDMA \u26a1 Speed: 2.4 kbps </p> <p>\u2705 Features: - Allowed voice calls but limited to one country. - Used analog signals, leading to poor voice quality and frequent call drops. - Weak battery life and limited network capacity. - No data services, only voice communication.  </p>"},{"location":"mobile-computing/#second-generation-2g-digital-communication","title":"\ud83d\udcf2 Second Generation (2G) \u2013 Digital Communication","text":"<p>\ud83d\udcc5 Introduced: 1990s \ud83d\udccc Technology Used: GSM &amp; CDMA \u26a1 Speed: Up to 144 kbps </p> <p>\u2705 Features: - Digital signals replaced analog, improving voice clarity. - Introduced SMS &amp; MMS for text and picture messaging. - Enabled conference calling, call hold, and international roaming. - Used circuit-switched and packet-switched networks. - Introduced GPRS (General Packet Radio Service), achieving speeds of 50 kbps to 1 Mbps.  </p>"},{"location":"mobile-computing/#transition-from-2g-to-3g-25g-275g","title":"\ud83d\udcf6 Transition from 2G to 3G: 2.5G &amp; 2.75G","text":""},{"location":"mobile-computing/#25g-gprs-general-packet-radio-service","title":"2.5G (GPRS \u2013 General Packet Radio Service)","text":"<p>\ud83d\udccc Technology Used: GSM with GPRS \u26a1 Speed: 64 \u2013 144 kbps </p> <p>\u2705 Improvements Over 2G: - Introduced always-on internet access. - Enabled email, basic web browsing, and multimedia messaging (MMS). - Used packet-switched data, improving efficiency. - Lower latency than 2G. - Introduction of camera phones.  </p>"},{"location":"mobile-computing/#275g-edge-enhanced-data-rates-for-gsm-evolution","title":"2.75G (EDGE \u2013 Enhanced Data rates for GSM Evolution)","text":"<p>\ud83d\udccc Technology Used: GSM with EDGE (Enhanced GPRS) \u26a1 Speed: Up to 384 kbps </p> <p>\u2705 Enhancements Over 2.5G: - Faster data transmission, enabling video streaming and online gaming. - More efficient spectrum usage, improving network performance. - Served as a stepping stone to 3G, enhancing mobile internet and multimedia communication.  </p> <p>\ud83d\ude80 2.5G and 2.75G bridged the gap between traditional mobile calling and the era of high-speed mobile internet! </p>"},{"location":"mobile-computing/#third-generation-3g-high-speed-mobile-data","title":"\ud83d\udce1 Third Generation (3G) \u2013 High-Speed Mobile Data","text":"<p>\ud83d\udcc5 Introduced: 2000s \ud83d\udccc Technology Used: WCDMA, HSPA (High-Speed Packet Access) \u26a1 Speed: Up to 2 Mbps </p> <p>\u2705 Features: - Enabled web browsing, email, video downloads, and picture sharing. - Provided support for multimedia applications, including video calling. - Increased bandwidth and data transfer rates for improved web-based applications. - Improved voice clarity and reduced latency compared to 2G.  </p>"},{"location":"mobile-computing/#fourth-generation-4g-high-speed-broadband-connectivity","title":"\u26a1 Fourth Generation (4G) \u2013 High-Speed Broadband Connectivity","text":"<p>\ud83d\udcc5 Introduced: 2010s \ud83d\udccc Technology Used: LTE (Long-Term Evolution), WiMAX \u26a1 Speed: 10 Mbps \u2013 1 Gbps </p> <p>\u2705 Features: - Ultra-fast data speeds with peak downloads of 100 Mbps. - Supported high-quality streaming for HD video and VoIP (Voice over IP). - Introduced IP-based telephony (VoLTE) for improved call quality. - Combination of Wi-Fi and WiMAX for broader coverage. - Enhanced security and reliability in mobile communication.  </p>"},{"location":"mobile-computing/#fifth-generation-5g-the-future-of-mobile-connectivity","title":"\ud83d\udce1 Fifth Generation (5G) \u2013 The Future of Mobile Connectivity","text":"<p>\ud83d\udcc5 Introduced: 2020s \ud83d\udccc Technology Used: mmWave, Massive MIMO, Network Slicing \u26a1 Speed: Up to 20 Gbps </p> <p>\u2705 Key Advancements Over 4G: - Higher Data Rates \u2013 Up to 20 Gbps for ultra-fast downloads and real-time communication. - Lower Latency \u2013 Reduced response time, essential for real-time gaming, augmented reality (AR), and virtual reality (VR). - Increased Network Capacity \u2013 Supports massive IoT (Internet of Things) connections. - Improved Reliability \u2013 Network slicing allows dedicated networks for specific applications.  </p> <p>\ud83d\udccc Summary of Mobile Generations </p> Generation Technology Used Speed Key Features 1G AMPS (Analog) 2.4 kbps Voice calls, poor quality, no data 2G GSM &amp; CDMA 144 kbps Digital voice, SMS, MMS, basic internet 2.5G GPRS 64 \u2013 144 kbps Always-on internet, emails, camera phones 2.75G EDGE Up to 384 kbps Faster browsing, video streaming 3G WCDMA, HSPA Up to 2 Mbps Video calls, multimedia, web-based apps 4G LTE, WiMAX 10 Mbps \u2013 1 Gbps HD streaming, VoLTE, broadband connectivity 5G mmWave, MIMO Up to 20 Gbps Ultra-fast speeds, IoT, AI-powered networks"},{"location":"mobile-computing/#cellular-architecture","title":"\ud83d\udce1 Cellular Architecture","text":"<p>The cellular network is structured in a hierarchical way to provide efficient communication across large geographical areas.  </p> <p> </p> <p>\ud83d\udd39 Key Components: \u2705 Mobile Device (User Equipment) \u2013 Phones, tablets, or IoT devices. \u2705 Base Transceiver Station (BTS) \u2013 Handles wireless communication with mobile devices. \u2705 Base Station Controller (BSC) \u2013 Manages multiple BTS and assigns frequencies. \u2705 Mobile Switching Center (MSC) \u2013 Connects mobile calls and manages handovers. \u2705 Public Switched Telephone Network (PSTN) \u2013 Traditional wired telephone network. \u2705 Packet Data Network (Internet) \u2013 Allows mobile data access and browsing. \u2705 Cell Towers \u2013 Divides the service area into small cells to provide coverage.  </p> <p>Note</p> <p>Mode of Communications</p> <pre><code>graph TD;\n\n    subgraph Circuit-Switched Network\n        A((User A)) -- Dedicated Path --&gt; B((User B))\n    end\n\n    subgraph Packet-Switched Network\n        A1((User C)) -- Packet 1 --&gt; C1[Router] --&gt; D1((User D))\n        A1 -- Packet 2 --&gt; C2[Router] --&gt; D1\n        A1 -- Packet 3 --&gt; C3[Router] --&gt; D1\n    end</code></pre>"},{"location":"mobile-computing/#mobile-computing-architecture","title":"Mobile Computing Architecture","text":"<p>Mobile computing architecture ensures seamless communication, data management, and user interaction, making applications efficient and scalable.  </p>"},{"location":"mobile-computing/#location-based-services-lbs","title":"\ud83d\udccd Location-Based Services (LBS)","text":"<p>\u2705 Location-Aware Services - Identify available services like printers, fax machines, phones, and servers in the local environment.  </p> <p>\u2705 Follow-On Services - Automatic call forwarding and workspace transmission to the user\u2019s current location.  </p> <p>\u2705 Information Services - Push: Automatic alerts (e.g., special offers in a supermarket). - Pull: User-requested data (e.g., where can I find my favorite pastry?).  </p> <p>\u2705 Support Services - Maintains cache, session state, and intermediate results, allowing smooth mobility.  </p> <p>\u2705 Privacy Management - Controls who has access to location information.  </p>"},{"location":"mobile-computing/#three-tier-mobile-computing-architecture","title":"Three-Tier Mobile Computing Architecture","text":""},{"location":"mobile-computing/#breakdown-of-the-three-tier-architecture","title":"\ud83d\udee0 Breakdown of the Three-Tier Architecture","text":"<p>\u2705 \ud83d\udcf2 Tier-1: Presentation Layer (User Interface) - Where users interact with the mobile app. - Handles buttons, menus, forms, and screens. - Supports multiple users simultaneously.  </p> <p>\u2705 \ud83c\udf10 Connection Layer (Access Network) - Routes traffic between user devices and the backend system. - Adapts to different devices and network conditions. - Ensures efficient data transmission even if one route fails.  </p> <p>\u2705 \u2699\ufe0f Tier-2: Application Layer - Process Management \u2013 Organizes tasks and workflows (e.g., food ordering steps). - Business Logic \u2013 Enforces rules and decision-making (e.g., price calculations, discounts).  </p> <p>\u2705 \ud83d\udcbe Tier-3: Data Layer - Database Management \u2013 Organizes data storage and retrieval (like a librarian). - Data Store \u2013 The actual storage where information is kept (like a bookshelf).  </p>"},{"location":"mobile-computing/#why-this-design","title":"\ud83d\udccc Why This Design?","text":"<p>\u2705 Scalability: More users can be handled by expanding any layer. \u2705 Reliability: If one component fails, others continue functioning. \u2705 Flexibility: Different layers can be updated or fixed independently. \u2705 Adaptability: Works well on various devices and network conditions.  </p>"},{"location":"mobile-computing/#real-world-analogy-a-restaurant-setup","title":"\ud83d\udccd Real-World Analogy \u2013 A Restaurant Setup \ud83c\udf7d\ufe0f","text":"Layer Restaurant Example Presentation Tier The dining area where customers interact with waiters. Access Network The waiters who take orders to the kitchen. Application Tier The kitchen where food is prepared based on orders. Data Tier The pantry where ingredients are stored."},{"location":"mobile-computing/#unit-2","title":"Unit 2","text":""},{"location":"mobile-computing/#concept-of-multiplexing","title":"Concept of Multiplexing","text":"<p>Multiplexing is a key technique in communication systems that allows multiple users to share a single medium with minimal or no interference.</p>"},{"location":"mobile-computing/#real-life-analogy","title":"Real-Life Analogy","text":"<p>Highways as a Shared Medium: - Multiple vehicles (users) travel on the same highway (medium) without interference. - Space Division Multiplexing (SDM): Cars use separate lanes. - Time Division Multiplexing (TDM): Cars use the same lane at different times.</p>"},{"location":"mobile-computing/#medium-access-control-mac-protocols","title":"\ud83d\udee0 Medium Access Control (MAC) Protocols","text":"<ul> <li>\u2705 What is MAC?</li> </ul> <p>A sublayer of the Data Link Layer responsible for coordinating transmissions between multiple nodes.</p> <p></p> <p>Data link layer divided into two functionality-oriented sublayers</p>"},{"location":"mobile-computing/#the-mac-problem-in-wireless-networks","title":"\u26a1 The MAC Problem in Wireless Networks","text":"<p>\ud83d\udccc When multiple nodes transmit simultaneously, their signals collide, causing:</p> <p>Lost data and wasted bandwidth.</p> <p>Increased retransmissions, leading to higher delays and lower efficiency.</p> <p>\ud83d\udccc Solution? Use a protocol to manage access to the shared medium.</p> <p>\u2705 What MAC Protocols Must Do:</p> <ul> <li>Minimize Collisions to optimize bandwidth usage.</li> <li>Decide when a station can transmit to avoid conflicts.</li> <li>Handle busy channels by deciding whether to wait or retransmit.</li> <li>Resolve collisions efficiently to ensure smooth data transmission.</li> </ul> <p>Here is the Mermaid diagram representation of Multiple Access Protocols along with a brief explanation for each type:  </p> <pre><code>graph TD;\n\n    A[Multiple-Access Protocols] --&gt; B[Random Access Protocols]\n    A --&gt; C[Controlled-Access Protocols]\n    A --&gt; D[Channelization Protocols]\n\n    B --&gt; B1[ALOHA]\n    B --&gt; B2[CSMA]\n    B --&gt; B3[CSMA/CD]\n    B --&gt; B4[CSMA/CA]\n\n    C --&gt; C1[Reservation]\n    C --&gt; C2[Polling]\n    C --&gt; C3[Token Passing]\n\n    D --&gt; D1[FDMA]\n    D --&gt; D2[TDMA]\n    D --&gt; D3[CDMA]</code></pre> <p>\u2705 Random Access Protocols (No fixed control, contention-based): - ALOHA \u2013 Transmits data randomly; high collision rate. - CSMA (Carrier Sense Multiple Access) \u2013 Senses channel before sending data to reduce collisions. - CSMA/CD (Collision Detection) \u2013 Detects collisions and retransmits (used in Ethernet). - CSMA/CA (Collision Avoidance) \u2013 Avoids collisions before transmission (used in Wi-Fi).  </p> <p>\u2705 Controlled-Access Protocols (Centralized control, avoids collisions): - Reservation \u2013 Nodes reserve slots before transmission. - Polling \u2013 Central controller decides which node transmits. - Token Passing \u2013 A token circulates, granting transmission rights.  </p> <p>\u2705 Channelization Protocols (Divide channel into separate logical paths): - FDMA (Frequency Division Multiple Access) \u2013 Assigns different frequencies to users. - TDMA (Time Division Multiple Access) \u2013 Allocates time slots to users. - CDMA (Code Division Multiple Access) \u2013 Uses unique codes for simultaneous transmissions.  </p>"},{"location":"mobile-computing/#medium-access-in-wireline-vs-wireless-networks","title":"Medium Access in Wireline vs. Wireless Networks","text":""},{"location":"mobile-computing/#medium-access-in-wireline-networks-csmacd","title":"\ud83d\udce1 Medium Access in Wireline Networks (CSMA/CD)","text":"<p>\u2705 Assumptions: - Signal strength remains constant across the wire. - Same signal strength can be assumed throughout if the wire length is within standard limits. - Collisions can be detected by any node listening to the wire.  </p> <p>\u2705 CSMA/CD (Carrier Sense Multiple Access with Collision Detection) Operation: 1. Carrier Sense \u2013 Listen to the wire; if free, send data. 2. Collision Detection \u2013 If a collision is detected while transmitting, stop immediately and send a jam signal to notify all nodes.  </p> <p>\ud83d\udd39 Why CSMA/CD Works Well in Wired Networks? - The signal condition is the same across the medium. - Collisions are easily detectable, ensuring efficient retransmission.  </p>"},{"location":"mobile-computing/#medium-access-in-wireless-networks-csmaca","title":"\ud83d\udcf6 Medium Access in Wireless Networks (CSMA/CA)","text":"<p>\u2705 Challenges in Wireless Medium: - Signal strength varies due to distance and obstacles. - Attenuation follows the inverse square law (\\( 1/d^2 \\)), weakening signals over distance. - Collisions at the receiver cannot be detected by simply listening to the medium.  </p> <p>\u2705 CSMA/CD Issues in Wireless: 1. Carrier Sense \u2013 The sender may detect an idle medium, but the receiver may still experience a collision. 2. Collision Detection \u2013 The sender cannot always detect a collision at the receiver\u2019s end.  </p> <p>\ud83d\udd39 Why CSMA/CD Fails in Wireless? - Wireless nodes have different perspectives of the medium. - The hidden terminal problem causes undetected collisions. - Instead, wireless networks use CSMA/CA (Collision Avoidance) to prevent collisions before they happen.  </p>"},{"location":"mobile-computing/#wireless-medium-access-problems","title":"Wireless Medium Access Problems","text":"<p>Wireless networks face unique challenges due to signal interference, attenuation, and variable reception.  </p>"},{"location":"mobile-computing/#hidden-terminal-problem","title":"Hidden Terminal Problem","text":"<p>Scenario: - A and C cannot hear each other but are both within B\u2019s range. - A starts transmitting to B. - C senses the medium as free (since it cannot hear A) and starts transmitting to B at the same time. - A collision occurs at B, but neither A nor C detects it.  </p> <p>Cause: - Other senders are hidden from the current sender, leading to undetected collisions.  </p> <p>Solution: - The RTS/CTS (Request to Send / Clear to Send) mechanism helps avoid hidden terminal issues by coordinating access.  </p> <p></p>"},{"location":"mobile-computing/#exposed-terminal-problem","title":"Exposed Terminal Problem","text":"<p>Scenario: - B is transmitting to A. - C senses the medium as busy because B is transmitting. - However, C could have transmitted to D without causing a collision. - C unnecessarily defers its transmission, reducing network efficiency.  </p> <p>Cause: - The sender mistakenly assumes the medium is in use, leading to wasted transmission opportunities.  </p> <p>Solution: - Spatial reuse techniques allow simultaneous non-interfering transmissions.  </p> <pre><code>\nsequenceDiagram\n    participant R1 as R\u2081 (Receiver 1)\n    participant S1 as S\u2081 (Sender 1)\n    participant S2 as S\u2082 (Sender 2)\n    participant R2 as R\u2082 (Receiver 2)\n\n    Note over S1,R1: 1. Initial Communication Setup\n    S1-&gt;&gt;R1: Request to Send (RTS)\n    R1-&gt;&gt;S1: Clear to Send (CTS)\n\n    Note over S1,R1: 2. Data Transfer Begins\n    S1-&gt;&gt;R1: DATA Transmission\n\n    Note over S2,R2: Medium is Busy\n    S2--xR2: Waiting for Clear Medium\n\n    Note over S1,R1: 3. Successful Data Transfer\n    activate S1\n    activate R1\n    Note over S1,R1: Data Transfer Complete\n    deactivate S1\n    deactivate R1\n\n    Note over S2,R2: Medium is Now Clear</code></pre>"},{"location":"mobile-computing/#nearfar-terminal-problem","title":"Near/Far Terminal Problem","text":"<p>Scenario: - B is closer to C than A. - B\u2019s stronger signal overpowers A\u2019s weaker signal at C. - C cannot receive A\u2019s transmission properly, causing data loss.  </p> <p>Cause: - Signal strength imbalance leads to weaker signals being drowned out by stronger ones.  </p> <p>Solution: - Power control mechanisms ensure all terminals are detectable at the base station. - GSM avoids the problem by using time slots (TDMA), preventing simultaneous transmission. - CDMA uses power control so all signals arrive at the receiver with equal strength.  </p> <p></p>"},{"location":"mobile-computing/#multiplexing","title":"Multiplexing","text":"<p>Wireless channels can be multiplexed in four key dimensions:</p> <ol> <li>Time (t): A channel occupies the entire frequency spectrum for a specific time period.</li> <li>Space (s): The same frequency can be reused if base stations are sufficiently separated.</li> <li>Frequency (f): The spectrum is divided into smaller frequency bands.</li> <li>Code (c): Each channel is assigned a unique code for transmission.</li> </ol>"},{"location":"mobile-computing/#space-division-multiplexing-sdm","title":"Space Division Multiplexing (SDM)","text":"<ul> <li>SDM involves separating channels in three dimensions: Code, Time, and Frequency.</li> <li>The Space dimension is represented by circles indicating interference ranges.</li> <li>To prevent overlap, channels are mapped to separate spaces (s1 to s3). This creates \"guard space\" between channels.</li> <li>Channels k1 to k3 are clearly separated, while additional spaces are needed for channels k4 to k6.</li> <li>This principle is similar to how old analog phone systems provided separate copper wires for each subscriber.</li> </ul>"},{"location":"mobile-computing/#example-fm-radio","title":"Example: FM Radio","text":"<ul> <li>Multiple radio stations can use the same frequency without interference, as long as they are separated geographically.</li> </ul>"},{"location":"mobile-computing/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Guard space: Needed in all multiplexing schemes to prevent interference.</li> <li>SDM: Effective for localized transmissions like FM radio but not scalable for dense urban areas.</li> </ul> <p>Note</p> <p>If several radio stations want to broadcast in the same city - Solution?</p> <p>SDM not suitable </p> <p>Solution:</p> <p>Multiplexing through</p> <p>Frequency</p> <p>Time</p> <p>Code</p>"},{"location":"mobile-computing/#frequency-division-multiplexing-fdm","title":"Frequency Division Multiplexing (FDM)","text":"<p>Frequency Division Multiplexing (FDM) divides the frequency dimension into several non-overlapping frequency bands. Each channel \\(k_i\\) is assigned a specific frequency band, which can be used continuously by the sender.</p> <ul> <li>Guard Spaces: Essential to prevent frequency band overlap (also called adjacent channel interference).</li> <li>Example: Used by radio stations within the same region, where each station broadcasts on its own frequency.</li> </ul> <p></p>"},{"location":"mobile-computing/#how-fdm-works","title":"How FDM Works","text":"<ul> <li>Simple Scheme: The receiver only needs to tune into the specific frequency assigned to the sender.</li> <li>Usage: Common in systems like radio broadcasting, where multiple stations use different frequencies to avoid interference.</li> </ul>"},{"location":"mobile-computing/#advantages-of-fdm","title":"Advantages of FDM","text":"<ul> <li>Simplicity: Very simple to implement, as it requires minimal coordination between the sender and receiver.</li> <li>Continuous Use: Each sender can use its frequency band continuously, making it suitable for applications like radio broadcasting.</li> </ul>"},{"location":"mobile-computing/#disadvantages-of-fdm","title":"Disadvantages of FDM","text":"<ul> <li>Frequency Resource Waste: In mobile communication, where communication is short-term, dedicating an entire frequency band to each scenario would waste valuable frequency resources.</li> <li>Limited Flexibility: The fixed assignment of frequencies to senders makes the system inflexible, limiting the number of senders that can be supported.</li> </ul>"},{"location":"mobile-computing/#time-division-multiplexing-tdm","title":"Time Division Multiplexing (TDM)","text":"<p>In Time Division Multiplexing (TDM), each channel \\(k_i\\) is allocated the entire bandwidth for a specific time period. Multiple senders use the same frequency but at different points in time.</p> <ul> <li>Guard Space: Time gaps between transmissions are required to prevent overlap.</li> <li>Co-channel Interference: Occurs if transmissions overlap in time, similar to cars colliding on a highway.</li> </ul> <p></p>"},{"location":"mobile-computing/#how-tdm-works","title":"How TDM Works","text":"<ul> <li>Precise Synchronization: Senders must be precisely synchronized, which requires clocks or a method to distribute synchronization signals.</li> <li>Receiver Tuning: The receiver must not only adjust the frequency but also tune to the exact time slot for receiving data.</li> <li>Flexibility: TDM is flexible, allowing more time for senders with heavy traffic and less time for those with lighter loads.</li> </ul>"},{"location":"mobile-computing/#disadvantages-of-tdm","title":"Disadvantages of TDM","text":"<ul> <li>Synchronization Requirement: All senders need to be synchronized, which adds complexity.</li> <li>Time Slot Coordination: A receiver must adjust both the frequency and the correct time slot.</li> <li>Co-channel Interference: If multiple senders choose the same frequency at the same time, interference occurs.</li> </ul>"},{"location":"mobile-computing/#time-frequency-division-multiplexing-tdma-fdma","title":"Time + Frequency Division Multiplexing (TDMA + FDMA)","text":"<p>A combination of both TDM and FDM can be used, where each channel is allotted a specific frequency for a set time period.</p> <ul> <li>Guard Spaces: Required in both time and frequency dimensions.</li> <li>Example: GSM uses TDMA + FDMA for communication between mobile phones and base stations.</li> </ul>"},{"location":"mobile-computing/#advantages-of-tdma-fdma","title":"Advantages of TDMA + FDMA","text":"<ul> <li>Robustness: Provides some protection against frequency selective interference.</li> <li>Protection Against Tapping: The sequence of frequencies must be known to intercept data, providing some protection.</li> </ul>"},{"location":"mobile-computing/#disadvantages-of-tdma-fdma","title":"Disadvantages of TDMA + FDMA","text":"<ul> <li>Coordination: Coordination between senders is required for frequency and time management.</li> <li>Interference: If two senders use the same frequency at the same time, interference occurs. Frequency hopping can minimize this, reducing interference time.</li> </ul>"},{"location":"mobile-computing/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>TDM: Simple but requires precise synchronization, making it suitable for scenarios where each sender needs to transmit in defined time slots.</li> <li>TDMA + FDMA: Offers better robustness and protection, but requires complex coordination and management of both time and frequency.</li> </ul>"},{"location":"mobile-computing/#code-division-multiplexing-cdm","title":"Code Division Multiplexing (CDM)","text":"<p>Code Division Multiplexing (CDM) is a relatively new scheme used in commercial communication systems, having been initially used in military applications due to its built-in security features.</p> <ul> <li>Working Principle: All channels \\(k_i\\) use the same frequency at the same time. Separation is achieved by assigning each channel its own unique \"code.\"</li> <li>Guard Space: This is ensured by using codes with a sufficient \"distance\" in the code space, such as orthogonal codes.</li> </ul>"},{"location":"mobile-computing/#example-party-with-global-participants","title":"Example: Party with Global Participants","text":"<p>Imagine a party with many participants from different countries who communicate using the same frequency range (approx. 300\u20136000 Hz):</p> <ul> <li>Same Language (SDM): If everyone speaks the same language, space division multiplexing (SDM) is required to separate groups.</li> <li>Different Languages (CDM): As soon as another language is used, a new code (language) can be tuned into, clearly separating communication in different languages. Other languages appear as background noise.</li> </ul>"},{"location":"mobile-computing/#cdm-security","title":"CDM Security","text":"<ul> <li> <p>Built-in Security: If the receiver doesn\u2019t know the code (or language), the signals are received but are essentially useless. This creates a secure channel in a potentially \"hostile\" environment, much like using a secret language at the party.</p> </li> <li> <p>Guard Space: Codes must be sufficiently distinct (e.g., Swedish and Finnish are orthogonal enough, but Swedish and Norwegian are too similar for separation).</p> </li> </ul>"},{"location":"mobile-computing/#advantages-of-cdm","title":"Advantages of CDM","text":"<ul> <li> <p>Interference Protection: CDM provides strong protection against interference and tapping. The huge code space allows for easy assignment of unique codes to different senders without significant issues.</p> </li> <li> <p>Security: A secret code can create a secure channel, as only those with the correct code can decode the message.</p> </li> </ul>"},{"location":"mobile-computing/#disadvantages-of-cdm","title":"Disadvantages of CDM","text":"<ul> <li> <p>Complex Receiver: The receiver must know the code and be able to decode the signal amidst background noise. This increases the complexity of the receiver.</p> </li> <li> <p>Synchronization Requirement: The receiver must be precisely synchronized with the transmitter for accurate decoding.</p> </li> <li> <p>Power Control: Signals must reach the receiver with equal strength. If signals are uneven, such as someone speaking too loudly near the receiver, the loud signal could drain the others, making it difficult for the receiver to decode other channels.</p> </li> </ul>"},{"location":"mobile-computing/#key-takeaways_2","title":"Key Takeaways","text":"<ul> <li>CDM: Provides secure and interference-resistant communication but requires precise synchronization and power control.</li> <li>Security: Built-in security by using unique codes (or languages) for each communication channel.</li> <li>Complexity: High complexity due to the need for the receiver to decode signals accurately and maintain synchronization.</li> </ul>"},{"location":"mobile-computing/#comparison-of-multiplexing-techniques","title":"Comparison of Multiplexing Techniques","text":"Approach SDMA (Space Division) TDMA (Time Division) FDMA (Frequency Division) CDMA (Code Division) Idea Segment space into cells/sectors Segment sending time into disjoint time-slots Divide the frequency band into sub-bands Spread the spectrum using orthogonal codes Terminals Only one terminal active per cell/sector All terminals share the same frequency but transmit in time slots Each terminal has its own dedicated frequency All terminals can be active simultaneously using unique codes Signal Separation Cell structure with directed antennas Synchronization in the time domain Filtering in the frequency domain Codes and special receivers Advantages Simple, increases capacity per km\u00b2 Fully digital, very flexible Simple, robust, and well-established Highly flexible, less planning needed, supports soft handover Disadvantages Inflexible, fixed antennas required Guard space needed, synchronization is complex Inflexible, limited by available frequencies Complex receivers, requires precise power control Comment Used in combination with TDMA, FDMA, or CDMA Standard in fixed networks, often combined with FDMA/SDMA in mobile networks Often combined with TDMA (frequency hopping patterns) and SDMA (frequency reuse) Used in 3G systems, requires integration with TDMA/FDMA"},{"location":"mobile-computing/#unit-3","title":"Unit 3","text":""},{"location":"mobile-computing/#logical-mobility","title":"Logical Mobility","text":"<p>Logical Mobility refers to the ability to transfer software components, code, or computational elements between different systems or devices.</p>"},{"location":"mobile-computing/#types-of-logical-mobility","title":"Types of Logical Mobility:","text":"<ul> <li>Software Programs &amp; Applications: Moving entire applications between devices.</li> <li>Code Segments &amp; Modules: Transferring scripts or functions dynamically.</li> <li>Objects &amp; Data Structures: Migrating serialized objects or database entries.</li> <li>Computational Processes: Shifting active processing tasks to another system.</li> </ul>"},{"location":"mobile-computing/#examples","title":"Examples:","text":"<ul> <li>App Downloads: Installing an application from an app store.</li> <li>Web Execution: A browser fetching and executing JavaScript from a server.</li> </ul>"},{"location":"mobile-computing/#process-migration","title":"Process Migration","text":"<p>Process Migration is the transfer of an executing process from one computing system to another while maintaining its state.</p>"},{"location":"mobile-computing/#key-aspects-of-process-migration","title":"Key Aspects of Process Migration:","text":"<ul> <li>Process = Program Under Execution</li> <li>State Transfer Includes:</li> <li>Address Space: Memory and allocated resources.</li> <li>Execution Point: CPU register contents.</li> <li>Communication State: Open files, message channels.</li> <li>OS-dependent States: Any system-specific data.</li> </ul>"},{"location":"mobile-computing/#migration-process","title":"Migration Process:","text":"<ol> <li>Two Instances Exist: A source and destination process.</li> <li>Final Handoff: The destination instance takes over as the migrated process.</li> <li>Remote Execution: A process running on another machine is called a remote process.</li> </ol>"},{"location":"mobile-computing/#example","title":"Example:","text":"<ul> <li>Watching a movie on a smart TV, then continuing playback on a tablet while traveling.</li> </ul>"},{"location":"mobile-computing/#procss-migration","title":"Procss Migration:","text":""},{"location":"mobile-computing/#step-1-migration-request-issued","title":"Step 1: Migration Request Issued","text":"<ul> <li>A migration request is sent to a remote node.</li> <li>After negotiation, the migration is accepted.</li> </ul>"},{"location":"mobile-computing/#diagram","title":"Diagram:","text":""},{"location":"mobile-computing/#step-2-process-detachment","title":"Step 2: Process Detachment","text":"<ul> <li>The process is suspended on the source node.</li> <li>It is marked as \"migrating.\"</li> <li>Communication is temporarily redirected.</li> </ul>"},{"location":"mobile-computing/#diagram_1","title":"Diagram:","text":""},{"location":"mobile-computing/#_1","title":"Mobile computing","text":""},{"location":"mobile-computing/#step-3-temporary-communication-redirection","title":"Step 3: Temporary Communication Redirection","text":"<ul> <li>Incoming messages are queued.</li> <li>Messages are delivered after migration.</li> </ul>"},{"location":"mobile-computing/#diagram_2","title":"Diagram:","text":""},{"location":"mobile-computing/#step-4-process-state-extraction","title":"Step 4: Process State Extraction","text":"<ul> <li>The process's memory, registers, communication state, and kernel context are extracted.</li> </ul>"},{"location":"mobile-computing/#diagram_3","title":"Diagram:","text":""},{"location":"mobile-computing/#_2","title":"Mobile computing","text":""},{"location":"mobile-computing/#step-5-destination-process-instance-created","title":"Step 5: Destination Process Instance Created","text":"<ul> <li>A new process instance is initialized on the remote node.</li> </ul>"},{"location":"mobile-computing/#diagram_4","title":"Diagram:","text":""},{"location":"mobile-computing/#step-6-state-transfer","title":"Step 6: State Transfer","text":"<ul> <li>The extracted state is transferred to the destination node.</li> </ul>"},{"location":"mobile-computing/#diagram_5","title":"Diagram:","text":""},{"location":"mobile-computing/#step-7-forwarding-references","title":"Step 7: Forwarding References","text":"<ul> <li>References (e.g., file descriptors, network sockets) are updated to point to the new process instance.</li> </ul>"},{"location":"mobile-computing/#diagram_6","title":"Diagram:","text":""},{"location":"mobile-computing/#step-8-process-resumed","title":"Step 8: Process Resumed","text":"<ul> <li>The new process instance resumes execution on the remote node.</li> </ul>"},{"location":"mobile-computing/#diagram_7","title":"Diagram:","text":""},{"location":"mobile-computing/#advantages-of-process-migration","title":"Advantages of Process Migration","text":"<ol> <li>Dynamic Load Distribution </li> <li> <p>Balances the load by migrating processes from overloaded nodes to less loaded ones.</p> </li> <li> <p>Fault Resilience </p> </li> <li> <p>Ensures continuity by migrating processes from nodes that have partially failed or are at risk of failure.</p> </li> <li> <p>Improved System Administration </p> </li> <li> <p>Facilitates maintenance by moving processes from nodes that are about to be shut down or become unavailable.</p> </li> <li> <p>Data Access Locality </p> </li> <li> <p>Enhances efficiency by migrating processes closer to the data source, especially useful in mobile environments.</p> </li> <li> <p>Resource Sharing </p> </li> <li> <p>Allows access to specialized hardware by migrating a process to a node equipped with the required resources.</p> </li> <li> <p>Mobile Computing </p> </li> <li>Enables users to continue running applications seamlessly as they move between networks or devices.</li> </ol>"},{"location":"mobile-computing/#applications-of-process-migration","title":"Applications of Process Migration","text":"<ul> <li>Parallelizable Applications \u2013 Distributing computational tasks across multiple nodes.</li> <li>Long-running Applications \u2013 Allowing execution across different nodes without interruption.</li> <li>Generic Multiuser Workloads \u2013 Managing distributed workloads effectively.</li> <li>Pre-emptable Applications \u2013 Processes that can be temporarily suspended and resumed elsewhere.</li> <li>Migration-aware Applications \u2013 Applications designed to adapt to migration scenarios.</li> <li>Network &amp; Mobile Computing Applications \u2013 Ensuring service continuity as devices move across networks.</li> </ul>"},{"location":"mobile-computing/#alternatives-to-process-migration","title":"Alternatives to Process Migration","text":"<ol> <li>Remote Execution </li> <li>Executes code on a remote node instead of migrating the entire process.  </li> <li> <p>Faster than migration due to lower data transfer costs.</p> </li> <li> <p>Cloning </p> </li> <li>Creates a copy of the process on a different node using a remote fork mechanism.  </li> <li>Unlike migration, both instances continue running using distributed shared state.  </li> <li> <p>Higher complexity but useful when state inheritance is required.</p> </li> <li> <p>Mobile Agents </p> </li> <li>Uses Java, Tcl/Tk, or other technologies to move objects or scripts dynamically.  </li> <li>Achieved at the middleware level using frameworks like:<ul> <li>Common Object Request Broker Architecture (CORBA)</li> <li>Distributed Objects</li> </ul> </li> </ol>"},{"location":"mobile-computing/#mobile-agents","title":"\ud83e\udd16 Mobile Agents","text":"<p>Mobile Agents are software entities that autonomously move between computers and continue execution on the destination machine.</p> <ul> <li>\ud83c\udfc3\u200d\u2642\ufe0f Self-driven: Can function independently, even if the user disconnects from the network.</li> <li>\ud83d\ude80 Transportable: They move dynamically across systems.</li> <li>\ud83d\udd04 Data-Carriers: Store information and operate without requiring continuous communication.</li> </ul>"},{"location":"mobile-computing/#types-of-mobile-agents","title":"\ud83d\udee4\ufe0f Types of Mobile Agents","text":"<ol> <li>Agents with Pre-defined Path \ud83d\uddfa\ufe0f  </li> <li> <p>Follow a specific, predetermined route across nodes.</p> </li> <li> <p>Agents with Undefined Path (Roamer) \ud83c\udfde\ufe0f </p> </li> <li>Wander freely across the network, dynamically choosing destinations.</li> </ol>"},{"location":"mobile-computing/#properties-of-mobile-agents","title":"\ud83e\udde0 Properties of Mobile Agents","text":"<p>A Mobile Agent is a software object that exists within an execution environment and possesses these key traits:</p>"},{"location":"mobile-computing/#mandatory-properties","title":"\u2705 Mandatory Properties","text":"<ul> <li>\ud83d\udd04 Reactive \u2013 Responds to environmental changes.</li> <li>\ud83e\udd16 Autonomous \u2013 Controls its own actions.</li> <li>\ud83c\udfaf Goal-Driven \u2013 Works proactively towards objectives.</li> <li>\u23f3 Temporally Continuous \u2013 Runs indefinitely.</li> </ul>"},{"location":"mobile-computing/#optional-properties","title":"\ud83d\udce1 Optional Properties","text":"<ul> <li>\ud83d\udde3 Communicative \u2013 Can interact with other agents.</li> <li>\ud83d\ude80 Mobile \u2013 Can migrate between hosts.</li> <li>\ud83d\udcc8 Learning \u2013 Adapts based on past experiences.</li> </ul>"},{"location":"mobile-computing/#life-cycle-of-mobile-agents","title":"\ud83d\udd04 Life Cycle of Mobile Agents","text":"<p>\u2714\ufe0f Adapts to both home and foreign environments. \u2714\ufe0f Switches between nodes as needed. \u2714\ufe0f Focuses on achieving the final objective. \u2714\ufe0f Operates autonomously without external intervention.  </p>"},{"location":"mobile-computing/#mobile-agent-platforms","title":"\ud83d\udda5\ufe0f Mobile Agent Platforms","text":"<ul> <li>\ud83d\udedc Specialized Servers interpret agent behavior and handle communication.</li> <li>\ud83d\ude80 Autonomous Navigation \u2013 Agents can choose and request migration.</li> <li>\u2699 Platform-Independent Execution \u2013 Can run on any machine without pre-installation.</li> <li>\u2615 Java-Based Execution \u2013 Uses Java Virtual Machine (JVM) to dynamically load code.</li> </ul> <p>\ud83e\udded Types of Mobile Agents: \u2705 One-hop Agents \u2013 Migrate to a single destination. \ud83c\udf0d Multi-hop Agents \u2013 Roam across multiple locations dynamically.</p>"},{"location":"mobile-computing/#components-of-a-mobile-agent","title":"\ud83c\udfd7\ufe0f Components of a Mobile Agent","text":"<p>A Mobile Agent consists of two key components:</p> <ol> <li>\ud83d\udcdc Code \u2013 Instructions defining the agent\u2019s behavior.</li> <li>\ud83e\udde0 Execution State \u2013 The agent\u2019s progress and memory.</li> </ol> <p>\ud83d\udca1 Unlike regular programs where code is stored on disk and execution state is in RAM, mobile agents carry both together when migrating!  </p> <p>Migration Process: \ud83d\udd39 Agent moves \u2192 Carries both its code &amp; execution state \u2192 Resumes seamlessly at the new host.</p>"},{"location":"mobile-computing/#characteristics-of-mobile-agents","title":"\ud83c\udf1f Characteristics of Mobile Agents","text":"<p>\u2714\ufe0f Unique Identity \u2013 Each agent has a distinct presence. \ud83d\udd0d \u2714\ufe0f Aware of Other Agents \u2013 Can detect and interact with other agents. \ud83e\udd1d \u2714\ufe0f Message Handling \u2013 Sends &amp; receives structured messages. \u2709\ufe0f \u2714\ufe0f Host Communication \u2013 Can communicate with its hosting environment. \ud83c\udfe1 \u2714\ufe0f Concurrent Execution \u2013 Supports multiple agents running simultaneously. \ud83d\udd04  </p>"},{"location":"mobile-computing/#_3","title":"Mobile computing","text":"<p>Agent Architecture</p> <p></p>"},{"location":"mobile-computing/#mobile-agents-vs-process-migration","title":"\ud83e\udd16 Mobile Agents vs. Process Migration","text":"Aspect \ud83d\ude80 Mobile Agents \ud83d\udd04 Process Migration Control Autonomous decision-making; agents decide when and where to move. System-controlled; OS or network manager decides movement. Intelligence Built-in intelligence to adapt behavior. No built-in intelligence; follows system instructions. Decision Basis Moves based on programmed objectives and current needs. Moves based on system load and resource availability. Flexibility Can change destinations dynamically; supports multi-hop movement. Fixed source-to-destination migration; single-hop only. State Management Carries both code &amp; state together as a package. State must be captured, transferred, and reconstructed. Interaction Can communicate with other agents and systems. No inter-process communication during migration."},{"location":"mobile-computing/#clientserver-vs-mobile-agent-architectures","title":"\ud83c\udf10 Client/Server vs. Mobile Agent Architectures","text":"<p>\ud83d\udce1 Traditional Client/Server Model - Requires continuous communication between client and server. - Frequent request/response cycles increase network bandwidth usage.  </p> <p>\ud83d\ude80 Mobile Agent Architecture - Moves queries/transactions from client to server, reducing repetitive requests. - Works offline and syncs results when the system is back online. - Handles intermittent &amp; unreliable networks effectively.  </p>"},{"location":"mobile-computing/#requirements-for-mobile-agent-systems","title":"\u2705 Requirements for Mobile Agent Systems","text":"<ol> <li>Portability \u2013 Must run on different platforms without modifications. \ud83d\udcbb  </li> <li>Ubiquity \u2013 Should be available across multiple network environments. \ud83c\udf0d  </li> <li>Network Communication \u2013 Needs efficient mechanisms for sending/receiving data. \ud83d\udce1  </li> <li>Server Security \u2013 Must prevent unauthorized agent execution on a host. \ud83d\udd10  </li> <li>Agent Security \u2013 Protects agents from external threats or modifications. \ud83d\udee1\ufe0f  </li> <li>Resource Accounting \u2013 Tracks resource consumption for optimization. \u2699\ufe0f  </li> </ol>"},{"location":"mobile-computing/#diagram-mobile-agent-vs-client-server-communication","title":"\ud83d\udccc Diagram: Mobile Agent vs. Client-Server Communication","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Server\n    Client-&gt;&gt;Server: \ud83d\udd04 Request Data\n    Server--&gt;&gt;Client: \ud83d\udce9 Response\n    Client-&gt;&gt;Server: \ud83d\udd04 Another Request\n    Server--&gt;&gt;Client: \ud83d\udce9 Another Response\n</code></pre>"},{"location":"mobile-computing/#aglets-java-based-mobile-agent-platform","title":"\ud83d\ude80 Aglets: Java-Based Mobile Agent Platform","text":"<p>Aglets are a Java-based framework for mobile agents, designed by IBM. They allow objects to move between hosts on a network while maintaining their execution state.</p>"},{"location":"mobile-computing/#how-aglets-work","title":"\ud83d\udd04 How Aglets Work","text":"<ul> <li>\ud83c\udfc3 An Aglet can pause execution, move to a remote host, and resume execution.</li> <li>\ud83d\udce6 When an Aglet moves, it carries its code and object states to the new host.</li> <li>\ud83c\udf0d Multiple Aglets can run on a single node within different contexts.</li> </ul>"},{"location":"mobile-computing/#aglet-context","title":"\ud83c\udfe2 Aglet Context","text":"<p>An Aglet Context is the workspace where Aglets operate.</p> <ul> <li>\ud83c\udfe0 Stationary Object \u2013 Provides a uniform execution environment.</li> <li>\ud83c\udf10 Hosts Multiple Contexts \u2013 A single network node can run multiple contexts.</li> </ul>"},{"location":"mobile-computing/#aglet-proxy","title":"\ud83d\udee1 Aglet Proxy","text":"<p>A proxy is a representative of an Aglet that: - Shields the Aglet from direct access to its public methods. - Hides its real location, ensuring location transparency.</p>"},{"location":"mobile-computing/#aglet-life-cycle","title":"\ud83d\udd04 Aglet Life Cycle","text":"\ud83c\udfd7\ufe0f Stage \ud83d\udd0d Description Creation \u2728 Aglet is created within a context, assigned an identifier, and initialized. Execution starts immediately. Cloning \ud83e\uddec Produces an identical copy of the Aglet in the same context, with a new identifier. Dispatching \ud83d\ude80 Moves the Aglet to another context, removing it from the current one. Execution restarts at the destination. Retraction \ud83d\udd04 Pulls the Aglet back to its original context after being dispatched. Deactivation \u23f8\ufe0f Temporarily removes an Aglet from execution and stores it in secondary storage. Activation \u25b6\ufe0f Restores a deactivated Aglet back into a context. Disposal \ud83d\uddd1\ufe0f Stops execution and removes the Aglet from the context permanently."},{"location":"mobile-computing/#aglet-communication","title":"\ud83d\udce1 Aglet Communication","text":"<p>Aglets use messages and events to communicate and interact within their environment.</p> <ul> <li>\ud83d\udcec Message Communication </li> <li>\ud83d\udd04 Synchronous \u2013 Requires an immediate response.  </li> <li> <p>\ud83d\udce9 Asynchronous \u2013 Messages are sent without waiting for an immediate reply.  </p> </li> <li> <p>\ud83d\udce2 Event-Driven Communication </p> </li> <li>\ud83d\udd04 Used for reactive or proactive agents.  </li> <li> <p>\ud83d\udcc8 Example: An Aglet can listen for a stock price change event and act accordingly.</p> </li> <li> <p>\ud83d\udee0 Aglet API (Java-based) includes:  </p> </li> <li><code>Aglet</code> \u2013 Core agent class.  </li> <li><code>Aglet Proxy</code> \u2013 Ensures security and location transparency.  </li> <li><code>Aglet Context</code> \u2013 Execution environment for Aglets.  </li> <li><code>Message</code> \u2013 Communication mechanism.</li> </ul>"},{"location":"mobile-computing/#applications-of-aglets","title":"\ud83c\udfdb\ufe0f Applications of Aglets","text":"<ul> <li>\ud83d\uded2 E-commerce \u2013 Mobile shopping assistants.  </li> <li>\ud83c\udf0d E-marketplaces \u2013 Automated price comparison &amp; negotiation.  </li> <li>\u2708\ufe0f Travel &amp; Tourism \u2013 Dynamic air-ticket booking &amp; package tour planning.  </li> </ul>"},{"location":"mobile-computing/#aglet-event-model","title":"\ud83d\udd04 Aglet Event Model","text":"<p>Aglets operate in an event-driven programming model. There are three types of listeners that handle different events:</p> \ud83c\udfa7 Listener Type \ud83d\udccc Function Clone Listener \ud83e\uddec Handles events before, during, and after an Aglet is cloned. Mobility Listener \ud83d\ude80 Triggers actions before dispatching, during movement, and upon arrival at a new context. Persistence Listener \u23f3 Manages events before deactivation and after activation of an Aglet."},{"location":"mobile-computing/#diagram-aglet-event-flow","title":"\ud83d\udccc Diagram: Aglet Event Flow","text":"<pre><code>graph TD\n    A[Aglet Running] --&gt;|Clone Event| B[Clone Listener]\n    A --&gt;|Migration Event| C[Mobility Listener]\n    A --&gt;|Deactivation Event| D[Persistence Listener]</code></pre>"},{"location":"rpa/","title":"Rpa","text":""},{"location":"rpa/#robot-process-automation","title":"\ud83d\udcc4 Robot Process Automation","text":""},{"location":"rpa/#create-a-bot-to-check-whether-the-given-number-is-palindrome-or-not","title":"Create a bot to check whether the given number is Palindrome or not.","text":"<p>Palindrome</p> <p>1\ufe0f\u20e3 Input a number. 2\ufe0f\u20e3 Convert the input to a number. 3\ufe0f\u20e3 Initialize <code>reversedNumber = 0</code> and store the original number as <code>tempNumber</code>. 4\ufe0f\u20e3 Loop while <code>originalNumber</code> is not 0: - Extract the last digit (<code>originalNumber mod 10</code>). - Update <code>reversedNumber</code> (<code>reversedNumber * 10 + last digit</code>). - Remove the last digit from <code>originalNumber</code> (<code>originalNumber / 10</code>). 5\ufe0f\u20e3 Compare <code>reversedNumber</code> with <code>tempNumber</code>. 6\ufe0f\u20e3 If they are equal, print \"Palindrome\", else print \"Not a Palindrome\". </p> <p>palindrome.txt</p>"},{"location":"rpa/#create-pad-to-know-if-the-input-character-is-a-vowel-or-consonant","title":"Create PAD to know if the input character is a vowel or consonant.","text":"<p>Vowel-consonant</p> <p>1\ufe0f\u20e3 Take input from the user (a single character). 2\ufe0f\u20e3 Convert the character to lowercase to ensure case insensitivity. 3\ufe0f\u20e3 Check if the character is a vowel (<code>a, e, i, o, u</code>) using a switch case. - \u2705 If it matches any of these, display \"It is a vowel\". - \u274c Otherwise, display \"It is a consonant\". </p> <p>vowel-consonant.txt</p>"},{"location":"rpa/#create-a-pad-flow-to-move-pdf-and-docx-files-from-the-download-location-to-a-folder-created-in-the-drive-present-on-your-machine","title":"Create a PAD flow to move PDF and DOCX files from the download location to a folder created in the drive present on your machine.","text":"<p>Note</p> <p>Choose a filter to limit the files retrieved. This allows  wild cards, for example, .txt or document?.doc. To  allow for multiple file filters, separate the choices  with a semi-colon, for example, .txt;*.exe. </p> <p>move-files.txt</p>"},{"location":"rpa/#take-a-number-input-from-the-user-and-display-the-given-number-as-even-or-oddeg-if-even-display-the-message-even-number-else-odd-number","title":"Take a number input from the user and Display the given number as Even or odd(e.g. If Even Display the message \"Even Number\" else \"Odd Number\" )","text":"<p>Note</p> <p>% Number mod 2 %</p> <p>even-odd.txt</p>"},{"location":"rpa/#prompt-the-user-to-provide-their-complete-name-change-the-case-to-upper-count-the-characters-and-display-your-full-name-is-name-and-is-of-number-characters","title":"Prompt the user to provide their complete name, Change the case to UPPER, Count the characters and display Your full name is %name% and is of %number% characters","text":"<p>Note</p> <p>Your Full Name is %TextWithNewCase% and is of %TextWithNewCase.length% characters.</p> <p>name-uppercase.txt</p>"},{"location":"rpa/#ask-the-user-for-a-mobile-number-ensure-it-is-10-digits-and-print-it-using-a-message-dialogue-box","title":"Ask the user for a mobile number, ensure it is 10 digits and print it using a message dialogue box.","text":"<p>Note</p> <p>Use split text to separate the numbers and store into list, then iterate through the list</p> <p>verify-mobile-number.txt</p>"},{"location":"rpa/#take-two-number-inputs-from-the-user-and-derive-the-addition-subtraction-and-display-using-the-message-box","title":"Take two number inputs from the user and Derive the Addition, Subtraction and Display using the Message Box.","text":"<p>Note</p> <p>you are in 12 sem for fucks sake</p> <p>add-subtract.txt</p>"},{"location":"rpa/#read-the-employee-salary-data-from-the-excel-as-per-the-salary-value-divide-the-employee-into-the-groups-like-if-salary40k-then-group-a-else-group-b-write-the-group-into-the-excel-group-column-save-and-close-excel","title":"Read the Employee Salary Data from the Excel As per the Salary value divide the Employee into the groups like if Salary&gt;40k then Group A Else Group B Write the Group into the excel Group column Save and Close Excel","text":"<p>Note</p> <p>Open Excel, read salary data, and categorize employees into \"Group1\" (&gt;= 50k) or \"Group2\" (&lt; 50k) based on their salary.</p> <p>salary-excel.txt</p>"},{"location":"rpa/#create-a-bot-to-extract-all-the-tutorial-titles-from-httpswwwxelpluscomall-tutorials-and-display-the-extracted-data-in-a-message-box-did-it-for-web-to-excel","title":"Create a bot to Extract all the tutorial titles from https://www.xelplus.com/all-tutorials/ and display the extracted data in a message box. Did it for web to excel","text":"<p>Note</p> <p>Change link, enable power automate extension</p> <p>web-to-excel.txt</p>"},{"location":"rpa/#create-a-bot-that-collects-family-holiday-destination-suggestions-from-three-membersindia-us-uk-adds-them-to-a-list-randomly-removes-two-destinations-and-displays-the-remaining-options-through-a-message-box","title":"Create a bot that collects family holiday destination suggestions from three members(India, US, UK), adds them to a list, randomly removes two destinations, and displays the remaining options through a message box","text":"<p>Note</p> <p>Shuffle list used to remove destination randomly</p> <p>family-holiday.txt</p>"},{"location":"rpa/#create-a-bot-for-the-hr-onboarding-process-that-reads-data-from-an-excel-fileemployeeid-generationxlsx-fills-in-data-dynamically-on-the-portal-generates-employee-id-and-corporate-id-and-stores-them-in-an-excel-file-httphappybotsinemployee-onboarding-processhtml","title":"Create a bot for the HR onboarding process that reads data from an Excel file(EmployeeID Generation.xlsx), fills in data dynamically on the portal, generates Employee ID and Corporate ID, and stores them in an Excel file (http://happybots.in/Employee-Onboarding-Process.html).","text":"<p>Note</p> <p>Skip this question in exam if you can </p> <p>hr-main.txt</p> <p>hr-subflow.txt</p>"},{"location":"rpa/#multiplication-table","title":"Multiplication Table","text":"<p>Note</p> <p>Easy</p> <p>multiplication-table.txt</p>"},{"location":"trial/","title":"Trial","text":""},{"location":"trial/#machine-learning-types","title":"Machine Learning Types","text":"<pre><code>graph LR\n    subgraph Input\n    A[Data with Labels] --&gt; B[Supervised Learning]\n    C[Data without Labels] --&gt; D[Unsupervised Learning]\n    E[States + Actions] --&gt; F[Reinforcement Learning]\n    end\n\n    B --&gt; G[Mapping]\n    D --&gt; H[Classes]\n    F --&gt; I[Action]\n\n    B -.-&gt;|Error| B\n    F -.-&gt;|Reward| F\n\n</code></pre>"},{"location":"trial/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Input: Labeled data (features + target labels)</li> <li>Process: Learning from labeled examples</li> <li>Output: Prediction model</li> <li>Feedback: Error measurement against known labels</li> <li>Applications: </li> <li>Classification (spam detection, image recognition)</li> <li>Regression (price prediction, sales forecasting)</li> </ul>"},{"location":"trial/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Input: Unlabeled data</li> <li>Process: Pattern/structure discovery</li> <li>Output: Data grouping/structure</li> <li>Feedback: Internal validation metrics</li> <li>Applications:</li> <li>Clustering (customer segmentation)</li> <li>Dimensionality reduction (feature extraction)</li> </ul>"},{"location":"trial/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Input: States + possible actions</li> <li>Process: Trial and error learning</li> <li>Output: Action policy</li> <li>Feedback: Rewards/penalties</li> <li>Applications:</li> <li>Game AI (chess, Go)</li> <li>Robotics (navigation, control)</li> </ul>"},{"location":"trial/#common-algorithms","title":"Common Algorithms","text":"<ul> <li>Supervised: Linear Regression, Random Forest, Neural Networks</li> <li>Unsupervised: K-means, PCA, Autoencoders</li> <li>Reinforcement: Q-Learning, Policy Gradient, DQN</li> </ul>"},{"location":"trial/#reinforcement-learning_1","title":"Reinforcement Learning:","text":"<ul> <li>What to do</li> <li>How to map situations to actions</li> <li>Maximizing a numerical reward signal</li> </ul> <p>Reinforcement learning is an autonomous, self-teaching system that essentially learns by trial and error. It performs actions with the aim of maximizing rewards, or in other words, it is learning by doing in order to achieve the best outcomes.</p> <p>Reinforcement Learning is a feedback-based Machine learning technique in which an agent learns to behave in an environment by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty</p>"},{"location":"trial/#key-characteristics","title":"Key Characteristics","text":"<p>Reinforcement Learning is inspired by how humans and animals learn through interactions:</p> <ul> <li>Reward and Punishment: Encourages repeating actions that lead to rewards and avoiding punishments.</li> <li>Trial and Error: Similar to trying different methods until the correct one is found.</li> <li>Learning Over Time: Improvement occurs through continuous experience.</li> <li> <p>Rewards Come from a Sequence of Actions.</p> </li> <li> <p>The learner is not told which actions to take but must discover them through trial and error.</p> </li> <li>Actions affect not only the immediate reward but also future situations and rewards.</li> <li>Works well in problems where a sequence of decisions is important.</li> </ul> <p>RL is an autonomous, self-teaching system that learns by trial and error. The goal is to maximize rewards over time.</p> <p>Example Applications:     - Chess     - Maze solving     - Industrial robot arms     - Path planning     - Sweeper robots</p>"},{"location":"trial/#how-rl-differs-from-supervised-learning","title":"How RL Differs from Supervised Learning","text":"Feature Supervised Learning Reinforcement Learning Training Data Has labeled answers No labeled answers; learns from experience Decision Making Independent of past decisions Dependent on past decisions Learning Method Trained with a dataset Learns through trial and error"},{"location":"trial/#elements-of-rl","title":"Elements of RL","text":"<p>![[Pasted image 20250131000424.png|500]]</p>"},{"location":"trial/#agent","title":"Agent","text":"<ul> <li>Definition: An entity that interacts with the environment.</li> <li>Examples: Robot, human, software program.</li> </ul>"},{"location":"trial/#environment","title":"Environment","text":"<ul> <li>Definition: The external system in which the agent operates.</li> <li>Examples: Physical world, game simulation.</li> </ul>"},{"location":"trial/#learning-process","title":"Learning Process","text":"<ol> <li>The agent moves from the initial state to the goal state.</li> <li>The agent continually asks, \"What is the best action in each state?\"</li> </ol>"},{"location":"trial/#advantages-of-reinforcement-learning","title":"Advantages of Reinforcement Learning","text":"<p>\u2705 No need for predefined instructions or human intervention.  \u2705 Can adapt to both static and dynamic environments.  \u2705 Solves a wide range of problems (decision-making, prediction, optimization).  \u2705 Improves with experience and fine-tunes over time.</p>"},{"location":"trial/#disadvantages-of-reinforcement-learning","title":"Disadvantages of Reinforcement Learning","text":"<p>\u274c Performance depends on the quality of the reward function.  \u274c Designing and tuning RL models can be complex.</p> <p>[!NOTE]</p>"},{"location":"trial/#when-to-apply-reinforcement-learning","title":"When to Apply Reinforcement Learning?","text":"<p>Reinforcement Learning is most suitable when: - The problem environment is complex and uncertain, making traditional programming methods ineffective. - Feedback is sparse, delayed, and dependent on multiple decisions. - Decision-making (actions) follows a feedback loop.</p>"},{"location":"trial/#why-is-reinforcement-learning-difficult","title":"Why Is Reinforcement Learning Difficult?","text":"<p>The toughest parts of Reinforcement Learning are: - Mapping the Environment. - Including All Possible Actions.</p>"},{"location":"trial/#core-concepts","title":"Core Concepts","text":"<ul> <li>Goal-Oriented Learning: The agent learns by trying to achieve a goal.</li> <li>Learning from Consequences: The agent learns from the consequences of its actions.</li> <li>Active Research Area: RL is one of the most active fields in Artificial Intelligence (AI).</li> </ul>"},{"location":"trial/#rl-algorithm-steps","title":"RL Algorithm Steps","text":"<pre><code>graph TD;\n    A[Agent Observes Environment] --&gt; B[Agent Performs Action];\n    B --&gt; C[Agent Moves to New State];\n    C --&gt; D[Agent Receives Reward];\n    D --&gt; E[Agent Evaluates Action - Good or Bad];\n    E --&gt; F[Agent Adjusts Strategy to Maximize Reward];\n\n</code></pre>"},{"location":"trial/#learning-and-planning","title":"Learning and Planning","text":""},{"location":"trial/#two-fundamental-problems-in-sequential-decision-making","title":"Two Fundamental Problems in Sequential Decision Making","text":""},{"location":"trial/#reinforcement-learning-rl","title":"Reinforcement Learning (RL):","text":"<ul> <li>The environment is initially unknown.</li> <li>The agent interacts with the environment.<ul> <li>The agent improves its policy.</li> </ul> </li> </ul>"},{"location":"trial/#planning","title":"Planning:","text":"<ul> <li>A model of the environment is known.</li> <li>The agent performs computations with its model (without any external interaction).</li> <li>The agent improves its policy, also known as deliberation, reasoning, introspection, pondering, thought, search.</li> </ul>"},{"location":"trial/#model-of-the-environment","title":"Model of the Environment:","text":"<ul> <li>A model mimics the behavior of the environment. With the help of the model, one can make inferences about how the environment will behave. For example, if a state and an action are given, the model can predict the next state and reward.</li> <li>The model is used for planning, providing a way to take a course of action by considering all future situations before actually experiencing those situations.</li> <li>Approaches for solving RL problems with the help of the model are termed model-based approach.</li> <li>An approach without using a model is called a model-free approach.</li> </ul> <p>![[Pasted image 20250130232904.png]]</p>"},{"location":"trial/#types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based","title":"Types of Reinforcement Learning Algorithms ( on the basis of model based)","text":"<p>There are various algorithms used in reinforcement learning such as Q-learning, policy gradient methods, Monte Carlo method and many more. All these algorithms can be classified into two broad categories - </p>"},{"location":"trial/#model-free-reinforcement-learning","title":"Model-free Reinforcement Learning :","text":"<ul> <li>It is a category of reinforcement learning algorithms that learns to make decisions by</li> <li>interacting with the environment directly, without creating a model of the environment's</li> <li>dynamics.</li> <li>The agent performs different actions multiple times to learn the outcomes and creates a</li> <li>strategy (policy) that optimizes its reward points. This is ideal for changing, large or complex</li> <li>environments.</li> <li>Not applicable for some scenario like self driving car.</li> </ul>"},{"location":"trial/#model-based-reinforcement-learning","title":"Model-based Reinforcement Learning:","text":"<ul> <li>This category of reinforcement learning algorithms involves creating a model of the environment's dynamics to make decisions and improve performance.</li> <li>Ideal for environments that are static and well-defined, where real-world environment testing is difficult.</li> </ul>"},{"location":"trial/#key-differences-between-model-free-and-model-based-reinforcement-learning","title":"Key Differences Between Model-free and Model-based Reinforcement Learning","text":"Feature Model-Free RL Model-Based RL Learning Approach Direct learning from environment Indirect learning through model building Efficiency Requires more real-world interactions More sample-efficient Complexity Simpler implementation More complex due to model learning Environment Utilization No internal model Builds and uses a model Adaptability Slower to adapt to changes Faster adaptation with accurate model Computational Requirements Less intensive More computational resources needed Examples Q-Learning, SARSA, DQN, PPO Dyna-Q, Model-Based Value Iteration"},{"location":"trial/#rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state","title":"RL Framework - The RL Process: A Loop of State, Action, Reward, and Next State","text":"<p>![[Pasted image 20250130232937.png]]</p>"},{"location":"trial/#main-characteristics-of-rl","title":"Main Characteristics of RL","text":"<ul> <li>No supervisor while training.</li> <li>Environment is generally stochastic for real-world applications.</li> <li>Model of the environment can be incomplete.</li> <li>Feedback (Negative/Positive Reward) can be delayed or partial.</li> <li>The agent uses experience from the past to improve its performance over time.</li> <li>Actions that have fetched more rewards are preferred.</li> <li>The agent tries various actions and prefers those that are best or have fetched more rewards.</li> <li>RL uses Markov Decision Process (MDP) framework to define the interaction between a learning agent and its environment.</li> </ul>"},{"location":"trial/#reinforcement-learning-rl-problem-challenges-in-rl","title":"Reinforcement Learning (RL) Problem - Challenges in RL","text":""},{"location":"trial/#trade-off-between-exploration-and-exploitation","title":"Trade-off between Exploration and Exploitation:","text":"<ul> <li>To obtain rewards, an RL agent must prefer actions that it has tried in the past and found effective (Exploit).</li> <li>However, to discover such actions, it must try actions it has not selected before (Explore).</li> </ul> <p>[!NOTE] Neither exploration nor exploitation can be pursued exclusively without failing at the task. </p>"},{"location":"trial/#fundamental-components-of-rl","title":"Fundamental Components of RL","text":"<ul> <li>Policy: Defines the agent\u2019s behavior.</li> <li>Reward Function: Provides feedback on actions.</li> <li>Value Function: Evaluates future rewards.</li> <li>Model of the Environment: Simulates how the environment works.</li> </ul>"},{"location":"trial/#policy","title":"Policy:","text":"<p>A policy is a strategy or set of rules that defines the actions the agent should take in a given state.</p> <ul> <li>The policy can be deterministic (one action for a state) or stochastic (probabilistic actions for a state).</li> <li>The goal is to find an optimal policy that maximizes the total expected reward.</li> </ul> <p>Example:</p> <ul> <li>A robot navigating a maze may follow a policy that says, \"Always turn left unless there's an obstacle, then turn right.\"</li> </ul>"},{"location":"trial/#human-analogy","title":"Human Analogy:","text":"<ul> <li>A policy is like a person's habit or plan of action, such as the decision to exercise every morning or take an umbrella when it's cloudy.</li> </ul>"},{"location":"trial/#value-function","title":"Value function:","text":"<p>Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.  - Rewards determine the immediate, intrinsic desirability of environmental states.  - Values indicate the long-term desirability of states after considering the states likely to follow and the rewards available in those states.  - Example: - A state might always yield a low immediate reward but still have a high value because it is followed by states that yield high rewards. </p>"},{"location":"trial/#human-analog","title":"Human Analog","text":"<ul> <li>Rewards are somewhat like kepleasureur (if high) and ndpainai (if low). - Values correspond to a more rerefined and farsighted judgmenten of how pleased or displeased we are by the environment.nt.</li> </ul>"},{"location":"trial/#reward-function","title":"Reward Function:","text":"<p>The reward function provides feedback on the actions the agent takes, indicating whether an action was good or bad.</p> <ul> <li>It assigns a numeric value to the agent's actions, which the agent uses to evaluate the desirability of its actions in a given state.</li> <li>The goal of the agent is to maximize the cumulative reward over time.</li> </ul> <p>Example:</p> <ul> <li>In a game, winning a round might give a reward of +10, while losing gives a reward of -1.</li> </ul>"},{"location":"trial/#human-analogy_1","title":"Human Analogy:","text":"<ul> <li>The reward function is like the feedback a person gets from their actions, such as feeling happy after a good deed or guilty after a mistake.</li> </ul>"},{"location":"trial/#model-of-the-environment_1","title":"Model of the Environment:","text":"<p>The model of the environment simulates how the environment behaves, helping the agent predict the outcomes of actions.</p> <ul> <li>This model can be used for planning future actions by simulating potential outcomes.</li> <li>A model-free approach directly learns from experience, while a model-based approach uses a model to predict actions' results before performing them.</li> </ul> <p>Example:</p> <ul> <li>A self-driving car may use a model to simulate various driving scenarios and plan its route accordingly.</li> </ul>"},{"location":"trial/#human-analogy_2","title":"Human Analogy:","text":"<ul> <li>The model of the environment is like a mental map that a person forms, which helps them predict the likely outcomes of their actions, such as deciding to avoid a route with heavy traffic.</li> </ul>"},{"location":"trial/#final-element-of-rl-systems-model-of-the-environment","title":"Final Element of RL Systems: Model of the Environment","text":"<ul> <li>The model mimics the behavior of the environment, allowing inferences to be made about how the environment will behave.</li> <li>For example, given a state and action, the model might predict the resultant next state and next reward.</li> <li> <p>Models are used for planning, helping the agent decide on a course of action by considering future situations before they are experienced.</p> </li> <li> <p>Methods for solving RL problems that use models and planning are called model-based methods, while simpler trial-and-error learning methods are called model-free methods.</p> </li> </ul>"},{"location":"trial/#types-of-reinforcement-learning","title":"Types of Reinforcement Learning","text":"<p>There are three main types of Reinforcement Learning (RL): - Value-Based - Policy-Based - Model-Based</p> <p>Each approach has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem you are trying to solve.</p>"},{"location":"trial/#value-based-reinforcement-learning","title":"Value-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns to estimate the value of each state or action based on the rewards it receives.</li> <li>This value is known as Q-values.</li> <li>The agent then selects the actions with the highest Q-value in each state to maximize its long-term reward.</li> <li>The most commonly used algorithm for value-based reinforcement learning is Q-learning.</li> </ul>"},{"location":"trial/#policy-based-reinforcement-learning","title":"Policy-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns an optimal policy, which is a mapping from states to actions, without calculating the value function.</li> <li>The policy is updated based on the rewards received by the agent, with the goal of maximizing the expected reward over time.</li> <li>The most common algorithm used for policy-based reinforcement learning is the REINFORCE algorithm.</li> </ul>"},{"location":"trial/#model-based-reinforcement-learning_1","title":"Model-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns a model of the environment, which it can use to simulate different scenarios and plan its actions accordingly.</li> <li>The model can learn through supervised or unsupervised learning, and the agent can use it to predict the outcome of its actions before taking them.</li> <li>The most common model-based reinforcement learning algorithm is the Dyna algorithm.</li> </ul>"}]}