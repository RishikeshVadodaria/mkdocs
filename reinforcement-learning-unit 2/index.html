
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://rishikeshvadodaria.github.io/mkdocs/reinforcement-learning-unit%202/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Unit 2: Multi-Arm Bandits - Notes by Rishikesh</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2afb09e1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    
      <link rel="stylesheet" href="../css/navbar.css">
    
      <link rel="stylesheet" href="../css/divider.css">
    
      <link rel="stylesheet" href="../css/font.css">
    
      <link rel="stylesheet" href="../css/waves.css">
    
      <link rel="stylesheet" href="../css/extra.css">
    
      <link rel="stylesheet" href="../css/widgets.css">
    
      <link rel="stylesheet" href="../css/countdown.css">
    
      <link rel="stylesheet" href="../css/nav.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#unit-2-multi-arm-bandits" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Notes by Rishikesh" class="md-header__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes by Rishikesh
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Unit 2: Multi-Arm Bandits
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Notes by Rishikesh" class="md-nav__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    Notes by Rishikesh
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobile-computing.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mobile Computing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cv-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2-formula/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA formulas
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-the-k-armed-bandit-problem" class="md-nav__link">
    <span class="md-ellipsis">
      What Is the K-Armed Bandit Problem?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Is the K-Armed Bandit Problem?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#formal-definition" class="md-nav__link">
    <span class="md-ellipsis">
      Formal Definition
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-restaurant-choice" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Restaurant Choice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-world-analogies" class="md-nav__link">
    <span class="md-ellipsis">
      Real-World Analogies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Process Flow
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-greedy-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      What Is the ε-Greedy Algorithm?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Is the ε-Greedy Algorithm?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#detailed-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Detailed Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#numerical-example" class="md-nav__link">
    <span class="md-ellipsis">
      Numerical Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Numerical Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-by-step-execution" class="md-nav__link">
    <span class="md-ellipsis">
      Step-by-Step Execution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages and Limitations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-epsilon-decay-and-why-is-it-used" class="md-nav__link">
    <span class="md-ellipsis">
      What Is Epsilon Decay, and Why Is It Used?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Is Epsilon Decay, and Why Is It Used?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#detailed-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Detailed Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-its-used" class="md-nav__link">
    <span class="md-ellipsis">
      Why It’s Used
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-exponential-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Exponential Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derivation-why-decay-works" class="md-nav__link">
    <span class="md-ellipsis">
      Derivation: Why Decay Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-action-value-methods" class="md-nav__link">
    <span class="md-ellipsis">
      What Are Action-Value Methods?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Are Action-Value Methods?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-components" class="md-nav__link">
    <span class="md-ellipsis">
      Core Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-incremental-update" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Incremental Update
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-form" class="md-nav__link">
    <span class="md-ellipsis">
      General Form
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-we-track-a-non-stationary-problem-in-bandit-settings" class="md-nav__link">
    <span class="md-ellipsis">
      How Do We Track a Non-Stationary Problem in Bandit Settings?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How Do We Track a Non-Stationary Problem in Bandit Settings?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-stationary-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Non-Stationary Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-constant-step-size-update" class="md-nav__link">
    <span class="md-ellipsis">
      Solution: Constant Step-Size Update
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-tracking-changing-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Tracking Changing Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-upper-confidence-bound-ucb-approach" class="md-nav__link">
    <span class="md-ellipsis">
      What Is the Upper Confidence Bound (UCB) Approach?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Is the Upper Confidence Bound (UCB) Approach?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm-details" class="md-nav__link">
    <span class="md-ellipsis">
      Algorithm Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#numerical-example_1" class="md-nav__link">
    <span class="md-ellipsis">
      Numerical Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-example-early-steps" class="md-nav__link">
    <span class="md-ellipsis">
      Additional Example: Early Steps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare-ucb-with-greedy-which-balances-exploration-better" class="md-nav__link">
    <span class="md-ellipsis">
      Compare UCB with ε-Greedy: Which Balances Exploration Better?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Compare UCB with ε-Greedy: Which Balances Exploration Better?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#detailed-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Detailed Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#which-balances-exploration-better" class="md-nav__link">
    <span class="md-ellipsis">
      Which Balances Exploration Better?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10-armed-testbed-insights" class="md-nav__link">
    <span class="md-ellipsis">
      10-Armed Testbed Insights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-choose-each" class="md-nav__link">
    <span class="md-ellipsis">
      When to Choose Each
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conceptuallogical-questions" class="md-nav__link">
    <span class="md-ellipsis">
      Conceptual/Logical Questions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conceptual/Logical Questions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-greedy-algorithm_1" class="md-nav__link">
    <span class="md-ellipsis">
      What Is the ε-Greedy Algorithm?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#which-parameter-should-be-reduced-to-prevent-inefficient-exploration-why" class="md-nav__link">
    <span class="md-ellipsis">
      Which Parameter Should Be Reduced to Prevent Inefficient Exploration? Why?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#define-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Define Exploration vs. Exploitation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#does-a-higher-ucb-value-indicate-exploration-or-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Does a Higher UCB Value Indicate Exploration or Exploitation?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#revision-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Revision Checklist
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="unit-2-multi-arm-bandits">Unit 2: Multi-Arm Bandits<a class="headerlink" href="#unit-2-multi-arm-bandits" title="Permanent link">&para;</a></h1>
<p>Multi-Arm Bandit (MAB) problems are a cornerstone of reinforcement learning (RL), modeling decision-making under uncertainty. They focus on the <strong>exploration vs. exploitation</strong> trade-off, where an agent must choose between trying new options (exploration) to learn their value or selecting known high-reward options (exploitation) to maximize cumulative rewards. This unit provides an in-depth exploration of the k-armed bandit problem, key algorithms (ε-greedy, Upper Confidence Bound), action-value methods, non-stationary tracking, and comparative analyses, enriched with detailed examples and conceptual insights.</p>
<h2 id="what-is-the-k-armed-bandit-problem">What Is the K-Armed Bandit Problem?<a class="headerlink" href="#what-is-the-k-armed-bandit-problem" title="Permanent link">&para;</a></h2>
<p>The <strong>k-armed bandit problem</strong> is a fundamental RL problem inspired by a gambler facing <span class="arithmatex">\( k \)</span> slot machines ("bandits"), each with an unknown reward probability distribution. The agent’s goal is to maximize total rewards over a series of trials by strategically choosing which arm to pull, balancing the need to explore unknown arms and exploit known high-reward ones.</p>
<h3 id="formal-definition">Formal Definition<a class="headerlink" href="#formal-definition" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>:</li>
<li>At each time step <span class="arithmatex">\( t = 1, 2, \dots, T \)</span>, the agent selects one of <span class="arithmatex">\( k \)</span> arms, denoted <span class="arithmatex">\( A_t \in \{1, 2, \dots, k\} \)</span>.</li>
<li>The environment returns a reward <span class="arithmatex">\( R_t \)</span>, drawn from the arm’s unknown probability distribution.</li>
<li>Each arm <span class="arithmatex">\( a \)</span> has a <strong>true expected reward</strong> (action value) <span class="arithmatex">\( q_*(a) = E[R_t | A_t = a] \)</span>, which is stationary (unchanging) in the classic setting.</li>
<li><strong>Objective</strong>: Maximize the cumulative reward:</li>
</ul>
<div class="arithmatex">\[  \sum_{t=1}^T R_t  \]</div>
<ul>
<li><strong>Regret</strong>: The difference between the optimal reward (always choosing the best arm) and the actual reward:</li>
</ul>
<div class="arithmatex">\[  \text{Regret} = T \cdot \max_a q_*(a) - \sum_{t=1}^T R_t\]</div>
<ul>
<li><strong>Key Characteristics</strong>:</li>
<li><strong>Single-state problem</strong>: No state transitions, unlike general RL.</li>
<li><strong>Unknown distributions</strong>: The agent estimates <span class="arithmatex">\( q_*(a) \)</span> via observed rewards.</li>
<li><strong>Exploration vs. Exploitation</strong>: Trying new arms risks low rewards but improves estimates; sticking to the best-known arm maximizes short-term gains but may miss better options.</li>
</ul>
<h3 id="example-restaurant-choice">Example: Restaurant Choice<a class="headerlink" href="#example-restaurant-choice" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Scenario</strong>: You’re in a town with 4 restaurants (arms) for 1000 days. Each restaurant has an unknown probability of a "happy" meal: <span class="arithmatex">\( P(M_1) = 0.6 \)</span>, <span class="arithmatex">\( P(M_2) = 0.4 \)</span>, <span class="arithmatex">\( P(M_3) = 0.7 \)</span>, <span class="arithmatex">\( P(M_4) = 0.9 \)</span>. Reward = 1 for a happy meal, 0 otherwise.</li>
<li><strong>Strategies</strong>:</li>
<li><strong>Exploit Only</strong>: Always choose restaurant 1 (assuming it seems best initially). Expected reward: <span class="arithmatex">\( 1000 \times 0.6 = 600 \)</span>. Regret: <span class="arithmatex">\( 1000 \times 0.9 - 600 = 300 \)</span>.</li>
<li><strong>Explore Only</strong>: Choose each restaurant 250 times. Expected reward: <span class="arithmatex">\( 250 \times (0.6 + 0.4 + 0.7 + 0.9) = 650 \)</span>. Regret: <span class="arithmatex">\( 900 - 650 = 250 \)</span>.</li>
<li><strong>Optimal</strong>: Always choose restaurant 4 (if probabilities were known). Expected reward: <span class="arithmatex">\( 1000 \times 0.9 = 900 \)</span>.</li>
<li><strong>Challenge</strong>: Without knowing probabilities, the agent must balance trying all restaurants to find the best (exploration) with repeatedly visiting the best-known one (exploitation).</li>
</ul>
<h3 id="real-world-analogies">Real-World Analogies<a class="headerlink" href="#real-world-analogies" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Clinical Trials</strong>: Test different drugs (arms) to find the most effective, balancing patient outcomes (exploitation) with learning about new treatments (exploration).</li>
<li><strong>Online Advertising</strong>: Choose between ad campaigns to maximize clicks, testing new ads (exploration) vs. showing high-performing ads (exploitation).</li>
<li><strong>Game Playing</strong>: In chess, play known strong moves (exploitation) or experimental moves to learn their effectiveness (exploration).</li>
</ul>
<h3 id="process-flow">Process Flow<a class="headerlink" href="#process-flow" title="Permanent link">&para;</a></h3>
<p>The k-armed bandit interaction is a simple feedback loop, as shown below:</p>
<p><img alt="alt text" src="../image-37.png" /></p>
<div class="admonition note">
<p class="admonition-title">Definition: K-Armed Bandit</p>
<p>A k-armed bandit problem involves selecting one of <span class="arithmatex">\( k \)</span> actions with unknown reward distributions to maximize cumulative rewards, formalized as a single-state Markov Decision Process (MDP).</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Key Challenge</p>
<p>Over-exploration wastes trials on poor arms; over-exploitation risks missing the optimal arm, leading to high regret.</p>
</div>
<h2 id="what-is-the-greedy-algorithm">What Is the ε-Greedy Algorithm?<a class="headerlink" href="#what-is-the-greedy-algorithm" title="Permanent link">&para;</a></h2>
<p>The <strong>ε-greedy algorithm</strong> is a widely used method to balance exploration and exploitation in k-armed bandit problems. It predominantly selects the arm with the highest estimated reward (greedy action) but occasionally chooses a random arm to explore, controlled by a parameter <span class="arithmatex">\( \epsilon \)</span>.</p>
<h3 id="detailed-algorithm">Detailed Algorithm<a class="headerlink" href="#detailed-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Initialization</strong>:</li>
<li>For each arm <span class="arithmatex">\( a = 1, 2, \dots, k \)</span>:<ul>
<li>Set initial action-value estimate <span class="arithmatex">\( Q_1(a) = 0 \)</span> (or an optimistic value, e.g., 5).</li>
<li>Set selection count <span class="arithmatex">\( N_1(a) = 0 \)</span>.</li>
</ul>
</li>
<li>Choose exploration probability <span class="arithmatex">\( \epsilon \in [0, 1] \)</span>.</li>
<li><strong>For each time step <span class="arithmatex">\( t = 1, 2, \dots, T \)</span></strong>:</li>
<li>Generate a random number <span class="arithmatex">\( x \sim \text{Uniform}[0, 1] \)</span>.</li>
<li><strong>Action Selection</strong>:<ul>
<li>If <span class="arithmatex">\( x &gt; \epsilon \)</span> (probability <span class="arithmatex">\( 1 - \epsilon \)</span>):</li>
<li><strong>Exploit</strong>: Choose the greedy action <span class="arithmatex">\( A_t = \arg\max_a Q_t(a) \)</span>. Break ties randomly.</li>
<li>Else (probability <span class="arithmatex">\( \epsilon \)</span>):</li>
<li><strong>Explore</strong>: Choose a random arm <span class="arithmatex">\( A_t \)</span> with probability <span class="arithmatex">\( 1/k \)</span>.</li>
</ul>
</li>
<li><strong>Receive Reward</strong>: Pull arm <span class="arithmatex">\( A_t \)</span>, get reward <span class="arithmatex">\( R_t \)</span>.</li>
<li><strong>Update Counts</strong>: <span class="arithmatex">\( N_{t+1}(A_t) = N_t(A_t) + 1 \)</span>.</li>
<li><strong>Update Action-Value</strong>:</li>
</ol>
<div class="arithmatex">\[      Q_{t+1}(A_t) = Q_t(A_t) + \frac{1}{N_t(A_t)} [R_t - Q_t(A_t)]   \]</div>
<p>For other arms <span class="arithmatex">\( a \neq A_t \)</span>, <span class="arithmatex">\( Q_{t+1}(a) = Q_t(a) \)</span>, <span class="arithmatex">\( N_{t+1}(a) = N_t(a) \)</span>.</p>
<h3 id="numerical-example">Numerical Example<a class="headerlink" href="#numerical-example" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: 4-arm bandit with true win probabilities <span class="arithmatex">\( P(M_1) = 0.6 \)</span>, <span class="arithmatex">\( P(M_2) = 0.4 \)</span>, <span class="arithmatex">\( P(M_3) = 0.7 \)</span>, <span class="arithmatex">\( P(M_4) = 0.9 \)</span>. Use <span class="arithmatex">\( \epsilon = 0.2 \)</span>, run for 5 steps.</li>
<li><strong>Input</strong>:</li>
<li>Random numbers for action selection: [0.68, 0.55, 0.23, 0.95, 0.8].</li>
<li>Random numbers for rewards: [0.6, 0.2, 0.3, 0.4, 0.9].</li>
<li>Reward rule: If random number <span class="arithmatex">\( \leq P(M_i) \)</span>, <span class="arithmatex">\( R_t = 1 \)</span>; else <span class="arithmatex">\( R_t = 0 \)</span>.</li>
<li><strong>Initialization</strong>: <span class="arithmatex">\( Q_1(a) = 0 \)</span>, <span class="arithmatex">\( N_1(a) = 0 \)</span> for all arms.</li>
</ul>
<h4 id="step-by-step-execution">Step-by-Step Execution<a class="headerlink" href="#step-by-step-execution" title="Permanent link">&para;</a></h4>
<p><strong>Step 1</strong>:</p>
<ul>
<li><span class="arithmatex">\( x = 0.68 &gt; 0.2 \)</span>, greedy. All <span class="arithmatex">\( Q_1(a) = 0 \)</span>, pick <span class="arithmatex">\( M_1 \)</span> (random tie-break).</li>
<li>Reward: <span class="arithmatex">\( 0.6 \leq 0.6 \)</span>, <span class="arithmatex">\( R_1 = 1 \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_2(M_1) = 1 \)</span>, <span class="arithmatex">\( Q_2(M_1) = 0 + \frac{1}{1} (1 - 0) = 1 \)</span>.</li>
</ul>
<p><strong>Step 2</strong>:</p>
<ul>
<li><span class="arithmatex">\( x = 0.55 &gt; 0.2 \)</span>, greedy. <span class="arithmatex">\( Q_2(M_1) = 1 \)</span>, others 0, pick <span class="arithmatex">\( M_1 \)</span>.</li>
<li>Reward: <span class="arithmatex">\( 0.2 \leq 0.6 \)</span>, <span class="arithmatex">\( R_2 = 1 \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_3(M_1) = 2 \)</span>, <span class="arithmatex">\( Q_3(M_1) = 1 + \frac{1}{2} (1 - 1) = 1 \)</span>.</li>
</ul>
<p><strong>Step 3</strong>:</p>
<ul>
<li><span class="arithmatex">\( x = 0.23 &gt; 0.2 \)</span>, greedy. Pick <span class="arithmatex">\( M_1 \)</span>.</li>
<li>Reward: <span class="arithmatex">\( 0.3 \leq 0.6 \)</span>, <span class="arithmatex">\( R_3 = 1 \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_4(M_1) = 3 \)</span>, <span class="arithmatex">\( Q_4(M_1) = 1 + \frac{1}{3} (1 - 1) = 1 \)</span>.</li>
</ul>
<p><strong>Step 4</strong>:</p>
<ul>
<li><span class="arithmatex">\( x = 0.95 &gt; 0.2 \)</span>, greedy. Pick <span class="arithmatex">\( M_1 \)</span>.</li>
<li>Reward: <span class="arithmatex">\( 0.4 \leq 0.6 \)</span>, <span class="arithmatex">\( R_4 = 1 \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_5(M_1) = 4 \)</span>, <span class="arithmatex">\( Q_5(M_1) = 1 + \frac{1}{4} (1 - 1) = 1 \)</span>.</li>
</ul>
<p><strong>Step 5</strong>:</p>
<ul>
<li><span class="arithmatex">\( x = 0.8 &gt; 0.2 \)</span>, greedy. Pick <span class="arithmatex">\( M_1 \)</span>.</li>
<li>Reward: <span class="arithmatex">\( 0.9 &gt; 0.6 \)</span>, <span class="arithmatex">\( R_5 = 0 \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_6(M_1) = 5 \)</span>, <span class="arithmatex">\( Q_6(M_1) = 1 + \frac{1}{5} (0 - 1) = \frac{4}{5} = 0.8 \)</span>.</li>
</ul>
<h4 id="results">Results<a class="headerlink" href="#results" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Total Reward</strong>: <span class="arithmatex">\( 1 + 1 + 1 + 1 + 0 = 4 \)</span>.</li>
<li><strong>Optimal Reward</strong>: Always pick <span class="arithmatex">\( M_4 \)</span>: <span class="arithmatex">\( 0.9 \times 5 = 4.5 \)</span>.</li>
<li><strong>Regret</strong>: <span class="arithmatex">\( 4.5 - 4 = 0.5 \)</span>.</li>
<li><strong>Analysis</strong>: The algorithm was unlucky, as no exploration (<span class="arithmatex">\( x &lt; 0.2 \)</span>) occurred, missing <span class="arithmatex">\( M_4 \)</span>.</li>
</ul>
<h3 id="advantages-and-limitations">Advantages and Limitations<a class="headerlink" href="#advantages-and-limitations" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Advantages</strong>:</li>
<li>Simple to implement and understand.</li>
<li>Guarantees exploration of all arms with <span class="arithmatex">\( \epsilon &gt; 0 \)</span>, ensuring <span class="arithmatex">\( Q_t(a) \to q_*(a) \)</span> as <span class="arithmatex">\( t \to \infty \)</span>.</li>
<li>Effective in stationary environments with proper <span class="arithmatex">\( \epsilon \)</span>.</li>
<li><strong>Limitations</strong>:</li>
<li>Fixed <span class="arithmatex">\( \epsilon \)</span> leads to unnecessary exploration in later steps, increasing regret.</li>
<li>Random exploration may repeatedly select poor arms.</li>
<li>Requires tuning <span class="arithmatex">\( \epsilon \)</span>, which is problem-dependent.</li>
</ul>
<div class="admonition danger">
<p class="admonition-title">Pitfall</p>
<p>A high <span class="arithmatex">\( \epsilon \)</span> (e.g., 0.5) wastes trials on suboptimal arms, while <span class="arithmatex">\( \epsilon = 0 \)</span> (pure greedy) risks getting stuck on a suboptimal arm.</p>
</div>
<h2 id="what-is-epsilon-decay-and-why-is-it-used">What Is Epsilon Decay, and Why Is It Used?<a class="headerlink" href="#what-is-epsilon-decay-and-why-is-it-used" title="Permanent link">&para;</a></h2>
<p><strong>Epsilon decay</strong> is an enhancement to the ε-greedy algorithm where the exploration probability <span class="arithmatex">\( \epsilon \)</span> decreases over time, shifting focus from exploration to exploitation as the agent gains confidence in its action-value estimates.</p>
<h3 id="detailed-mechanism">Detailed Mechanism<a class="headerlink" href="#detailed-mechanism" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Initial <span class="arithmatex">\( \epsilon \)</span></strong>: Start with a high <span class="arithmatex">\( \epsilon_0 \)</span> (e.g., 0.5) to encourage exploration.</li>
<li><strong>Decay Strategies</strong>:</li>
<li><strong>Linear Decay</strong>: <span class="arithmatex">\( \epsilon_t = \epsilon_0 - \delta \cdot t \)</span>, where <span class="arithmatex">\( \delta \)</span> is a small constant (e.g., <span class="arithmatex">\( \delta = 0.0001 \)</span>).</li>
<li><strong>Exponential Decay</strong>: <span class="arithmatex">\( \epsilon_t = \epsilon_0 \cdot \gamma^t \)</span>, where <span class="arithmatex">\( 0 &lt; \gamma &lt; 1 \)</span> (e.g., <span class="arithmatex">\( \gamma = 0.999 \)</span>).</li>
<li><strong>Inverse Decay</strong>: <span class="arithmatex">\( \epsilon_t = \frac{\epsilon_0}{1 + \kappa \cdot t} \)</span>, where <span class="arithmatex">\( \kappa \)</span> controls decay speed.</li>
<li><strong>Minimum <span class="arithmatex">\( \epsilon \)</span></strong>: Set a floor <span class="arithmatex">\( \epsilon_{\text{min}} \)</span> (e.g., 0.01) to ensure some exploration persists.</li>
<li><strong>Implementation</strong>:</li>
<li>Update <span class="arithmatex">\( \epsilon_t \)</span> after each step or episode.</li>
<li>Example: <span class="arithmatex">\( \epsilon_t = \max(0.01, 0.5 \cdot 0.999^t) \)</span>.</li>
</ul>
<h3 id="why-its-used">Why It’s Used<a class="headerlink" href="#why-its-used" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Early Exploration</strong>: High initial <span class="arithmatex">\( \epsilon \)</span> ensures all arms are sampled, reducing the risk of missing the optimal arm.</li>
<li><strong>Late Exploitation</strong>: As <span class="arithmatex">\( Q_t(a) \)</span> becomes reliable, lower <span class="arithmatex">\( \epsilon \)</span> focuses on the best arm, maximizing rewards.</li>
<li><strong>Reduced Regret</strong>: Decreasing exploration minimizes trials wasted on suboptimal arms.</li>
<li><strong>Adaptability</strong>: Matches the learning process, where uncertainty decreases over time.</li>
<li><strong>Practicality</strong>: Avoids manual tuning of a fixed <span class="arithmatex">\( \epsilon \)</span>.</li>
</ol>
<h3 id="example-exponential-decay">Example: Exponential Decay<a class="headerlink" href="#example-exponential-decay" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: <span class="arithmatex">\( \epsilon_0 = 0.5 \)</span>, <span class="arithmatex">\( \gamma = 0.995 \)</span>, <span class="arithmatex">\( \epsilon_{\text{min}} = 0.01 \)</span>, <span class="arithmatex">\( T = 1000 \)</span>.</li>
<li><strong>Calculation</strong>:</li>
<li>Step 100: <span class="arithmatex">\( \epsilon_{100} = 0.5 \cdot 0.995^{100} \approx 0.303 \)</span>.</li>
<li>Step 500: <span class="arithmatex">\( \epsilon_{500} \approx 0.036 \)</span>.</li>
<li>Step 1000: <span class="arithmatex">\( \epsilon_{1000} \approx 0.01 \)</span> (hits minimum).</li>
<li><strong>Impact</strong>: Early steps explore ~50% of the time; by step 1000, exploration drops to ~1%, prioritizing the best arm.</li>
</ul>
<h3 id="derivation-why-decay-works">Derivation: Why Decay Works<a class="headerlink" href="#derivation-why-decay-works" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Early Phase</strong>: High <span class="arithmatex">\( \epsilon \)</span> increases <span class="arithmatex">\( N_t(a) \)</span> for all arms, improving <span class="arithmatex">\( Q_t(a) \)</span> accuracy.</li>
<li><strong>Late Phase</strong>: As <span class="arithmatex">\( Q_t(a) \to q_*(a) \)</span>, the optimal arm’s <span class="arithmatex">\( Q_t(a) \)</span> dominates, and exploration becomes less necessary.</li>
<li><strong>Regret Analysis</strong>:</li>
<li>Fixed <span class="arithmatex">\( \epsilon \)</span>: Linear regret (<span class="arithmatex">\( O(T) \)</span>), as exploration continues indefinitely.</li>
<li>Decaying <span class="arithmatex">\( \epsilon \)</span>: Sublinear regret (e.g., <span class="arithmatex">\( O(\sqrt{T}) \)</span>) with proper decay (e.g., <span class="arithmatex">\( \epsilon_t \propto 1/t \)</span>).</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Formula: Exponential Decay</p>
<div class="arithmatex">\[
\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_0 \cdot \gamma^t)
\]</div>
</div>
<div class="admonition danger">
<p class="admonition-title">Tuning Challenge</p>
<p>Too fast a decay (large <span class="arithmatex">\( \gamma \)</span>) may stop exploration prematurely; too slow a decay wastes trials. Tune <span class="arithmatex">\( \gamma \)</span> based on problem horizon <span class="arithmatex">\( T \)</span>.</p>
</div>
<h2 id="what-are-action-value-methods">What Are Action-Value Methods?<a class="headerlink" href="#what-are-action-value-methods" title="Permanent link">&para;</a></h2>
<p><strong>Action-value methods</strong> are techniques that estimate the expected reward (action value) of each arm based on observed rewards and use these estimates to guide action selection. They form the foundation of bandit algorithms like ε-greedy, UCB, and optimistic initial values.</p>
<h3 id="core-components">Core Components<a class="headerlink" href="#core-components" title="Permanent link">&para;</a></h3>
<p><strong>Expected Value <span class="arithmatex">\( q_*(a) \)</span></strong>:</p>
<ul>
<li>The true mean reward for arm <span class="arithmatex">\( a \)</span>: <span class="arithmatex">\( q_*(a) = E[R_t | A_t = a] \)</span>.</li>
<li>Example: For a Bernoulli bandit, <span class="arithmatex">\( q_*(a) = P(\text{win}) \)</span>.</li>
<li>Unknown to the agent, it’s estimated empirically.</li>
</ul>
<p><strong>Estimated Value <span class="arithmatex">\( Q_t(a) \)</span></strong>:
   - The agent’s approximation of <span class="arithmatex">\( q_*(a) \)</span> at time <span class="arithmatex">\( t \)</span>.
   - Computed via <strong>sample-average method</strong>:</p>
<div class="arithmatex">\[      Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i=a}}{N_t(a)}    \]</div>
<ul>
<li><span class="arithmatex">\( N_t(a) \)</span>: Number of times arm <span class="arithmatex">\( a \)</span> was chosen.</li>
<li><span class="arithmatex">\( \mathbb{1}_{A_i=a} \)</span>: Indicator (1 if <span class="arithmatex">\( A_i = a \)</span>, 0 otherwise).</li>
<li>If <span class="arithmatex">\( N_t(a) = 0 \)</span>, set <span class="arithmatex">\( Q_t(a) = 0 \)</span> or a default (e.g., optimistic value).
<strong>Incremental Update</strong>:<ul>
<li>To avoid storing all rewards, update <span class="arithmatex">\( Q_t(a) \)</span> incrementally:</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[ Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} [R_t - Q_t(a)]  \]</div>
<ul>
<li>Derivation:</li>
</ul>
<div class="arithmatex">\[  Q_{t+1}(a) = \frac{\sum_{i=1}^t R_i \cdot \mathbb{1}_{A_i=a}}{N_t(a)} = \frac{(N_t(a) - 1) Q_t(a) + R_t}{N_t(a)}   \]</div>
<div class="arithmatex">\[  = Q_t(a) + \frac{R_t - Q_t(a)}{N_t(a)}   \]</div>
<ul>
<li>Requires storing only <span class="arithmatex">\( Q_t(a) \)</span> and <span class="arithmatex">\( N_t(a) \)</span>, making it memory-efficient.</li>
</ul>
<p><strong>Action Selection</strong>:
   - Use <span class="arithmatex">\( Q_t(a) \)</span> to choose arms, e.g.:
     - Greedy: <span class="arithmatex">\( A_t = \arg\max_a Q_t(a) \)</span>.
     - ε-Greedy: Mix greedy with random exploration.
     - UCB: Incorporate uncertainty (see later).</p>
<h3 id="example-incremental-update">Example: Incremental Update<a class="headerlink" href="#example-incremental-update" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: Arm <span class="arithmatex">\( a \)</span> chosen 3 times, rewards: [1, 0, 1]. Initial <span class="arithmatex">\( Q_1(a) = 0 \)</span>, <span class="arithmatex">\( N_1(a) = 0 \)</span>.</li>
<li><strong>Updates</strong>:</li>
<li>After <span class="arithmatex">\( R_1 = 1 \)</span>: <span class="arithmatex">\( Q_2(a) = 0 + \frac{1}{1} (1 - 0) = 1 \)</span>, <span class="arithmatex">\( N_2(a) = 1 \)</span>.</li>
<li>After <span class="arithmatex">\( R_2 = 0 \)</span>: <span class="arithmatex">\( Q_3(a) = 1 + \frac{1}{2} (0 - 1) = 0.5 \)</span>, <span class="arithmatex">\( N_3(a) = 2 \)</span>.</li>
<li>After <span class="arithmatex">\( R_3 = 1 \)</span>: <span class="arithmatex">\( Q_4(a) = 0.5 + \frac{1}{3} (1 - 0.5) \approx 0.667 \)</span>, <span class="arithmatex">\( N_4(a) = 3 \)</span>.</li>
<li><strong>Verification</strong>: Sample-average: <span class="arithmatex">\( Q_4(a) = \frac{1 + 0 + 1}{3} = \frac{2}{3} \approx 0.667 \)</span>.</li>
</ul>
<h3 id="general-form">General Form<a class="headerlink" href="#general-form" title="Permanent link">&para;</a></h3>
<ul>
<li>The incremental update follows:</li>
</ul>
<div class="arithmatex">\[ \text{NewEstimate} = \text{OldEstimate} + \text{StepSize} \cdot [\text{Target} - \text{OldEstimate}]  \]</div>
<ul>
<li><strong>Target</strong>: The new reward <span class="arithmatex">\( R_t \)</span>.</li>
<li><strong>StepSize</strong>: <span class="arithmatex">\( \frac{1}{N_t(a)} \)</span>, which decreases as <span class="arithmatex">\( N_t(a) \)</span> grows.</li>
<li><strong>Error</strong>: <span class="arithmatex">\( R_t - Q_t(a) \)</span>, reduced by moving toward the target.</li>
</ul>
<h3 id="why-it-matters">Why It Matters<a class="headerlink" href="#why-it-matters" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Convergence</strong>: By the <strong>Law of Small Numbers</strong>, <span class="arithmatex">\( Q_t(a) \to q_*(a) \)</span> as <span class="arithmatex">\( N_t(a) \to \infty \)</span>.</li>
<li><strong>Efficiency</strong>: Incremental updates minimize computational and memory costs.</li>
<li><strong>Flexibility</strong>: Forms the basis for various algorithms (ε-greedy, UCB, etc.).</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Law of Large Numbers</p>
<p>As the number of samples <span class="arithmatex">\( N_t(a) \)</span> increases, the sample average <span class="arithmatex">\( Q_t(a) \)</span> converges to the true expected value <span class="arithmatex">\( q_*(a) \)</span>.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Exploration Dependency</p>
<p>Accurate <span class="arithmatex">\( Q_t(a) \)</span> requires sufficient exploration; if <span class="arithmatex">\( N_t(a) \)</span> is low, estimates are noisy, necessitating algorithms like ε-greedy or UCB.</p>
</div>
<h2 id="how-do-we-track-a-non-stationary-problem-in-bandit-settings">How Do We Track a Non-Stationary Problem in Bandit Settings?<a class="headerlink" href="#how-do-we-track-a-non-stationary-problem-in-bandit-settings" title="Permanent link">&para;</a></h2>
<p>In <strong>non-stationary</strong> bandit problems, the reward distributions (<span class="arithmatex">\( q_*(a) \)</span>) change over time, rendering traditional sample-average methods ineffective. Tracking these changes requires prioritizing recent rewards to adapt to evolving environments.</p>
<h3 id="non-stationary-challenges">Non-Stationary Challenges<a class="headerlink" href="#non-stationary-challenges" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Stationary Limitation</strong>: Sample-average methods assume <span class="arithmatex">\( q_*(a) \)</span> is fixed, so <span class="arithmatex">\( Q_t(a) \)</span> converges to a constant, ignoring shifts.</li>
<li><strong>Real-World Examples</strong>:</li>
<li>A restaurant’s quality changes due to a new chef.</li>
<li>A slot machine’s payout rate adjusts due to casino updates.</li>
<li>Ad click-through rates shift with user trends.</li>
<li><strong>Need for Adaptability</strong>: Old rewards become irrelevant, and the agent must track the current <span class="arithmatex">\( q_*(a) \)</span>.</li>
</ul>
<h3 id="solution-constant-step-size-update">Solution: Constant Step-Size Update<a class="headerlink" href="#solution-constant-step-size-update" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Modification</strong>: Replace the decreasing step-size <span class="arithmatex">\( \frac{1}{N_t(a)} \)</span> with a <strong>constant step-size</strong> <span class="arithmatex">\( \alpha \in (0, 1] \)</span>:</li>
</ul>
<div class="arithmatex">\[
Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]
\]</div>
<ul>
<li><strong>Alternative Form</strong>:</li>
</ul>
<div class="arithmatex">\[
Q_{t+1}(a) = (1 - \alpha) Q_t(a) + \alpha R_t
\]</div>
<ul>
<li><strong>Weighted Average</strong>:</li>
<li>After <span class="arithmatex">\( n \)</span> selections of arm <span class="arithmatex">\( a \)</span>, the estimate is:</li>
</ul>
<div class="arithmatex">\[
Q_{n+1}(a) = \alpha \sum_{i=1}^n (1 - \alpha)^{n-i} R_i + (1 - \alpha)^n Q_1(a)
\]</div>
<ul>
<li><strong>Weights</strong>: <span class="arithmatex">\( \alpha (1 - \alpha)^{n-i} \)</span> for reward <span class="arithmatex">\( R_i \)</span>, decaying exponentially with time since observation.</li>
<li><strong>Sum of Weights</strong>: <span class="arithmatex">\( \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} + (1 - \alpha)^n = 1 \)</span>, forming a weighted average.</li>
</ul>
<h3 id="example-tracking-changing-rewards">Example: Tracking Changing Rewards<a class="headerlink" href="#example-tracking-changing-rewards" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: Arm <span class="arithmatex">\( a \)</span>, initial <span class="arithmatex">\( Q_1(a) = 0 \)</span>, <span class="arithmatex">\( \alpha = 0.1 \)</span>. Rewards: [1, 1, 0, 2, 2] (assume <span class="arithmatex">\( q_*(a) \)</span> shifts from 1 to 2 after step 3).</li>
<li><strong>Updates</strong>:</li>
<li>Step 1: <span class="arithmatex">\( R_1 = 1 \)</span>, <span class="arithmatex">\( Q_2(a) = 0 + 0.1 (1 - 0) = 0.1 \)</span>.</li>
<li>Step 2: <span class="arithmatex">\( R_2 = 1 \)</span>, <span class="arithmatex">\( Q_3(a) = 0.1 + 0.1 (1 - 0.1) = 0.19 \)</span>.</li>
<li>Step 3: <span class="arithmatex">\( R_3 = 0 \)</span>, <span class="arithmatex">\( Q_4(a) = 0.19 + 0.1 (0 - 0.19) = 0.171 \)</span>.</li>
<li>Step 4: <span class="arithmatex">\( R_4 = 2 \)</span>, <span class="arithmatex">\( Q_5(a) = 0.171 + 0.1 (2 - 0.171) = 0.354 \)</span>.</li>
<li>Step 5: <span class="arithmatex">\( R_5 = 2 \)</span>, <span class="arithmatex">\( Q_6(a) = 0.354 + 0.1 (2 - 0.354) = 0.499 \)</span>.</li>
<li><strong>Analysis</strong>: <span class="arithmatex">\( Q_t(a) \)</span> starts near 1, then gradually tracks toward 2, reflecting the shift in <span class="arithmatex">\( q_*(a) \)</span>.</li>
</ul>
<h3 id="why-it-works">Why It Works<a class="headerlink" href="#why-it-works" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Recency Bias</strong>: <span class="arithmatex">\( \alpha \)</span> emphasizes recent rewards, with <span class="arithmatex">\( (1 - \alpha)^{n-i} \)</span> reducing the influence of older rewards.</li>
<li><strong>Exponential Decay</strong>: The weight <span class="arithmatex">\( \alpha (1 - \alpha)^{n-i} \)</span> decreases exponentially, ensuring adaptability.</li>
<li><strong>Tunable <span class="arithmatex">\( \alpha \)</span></strong>:</li>
<li>High <span class="arithmatex">\( \alpha \)</span> (e.g., 0.5): Fast adaptation, but noisy estimates.</li>
<li>Low <span class="arithmatex">\( \alpha \)</span> (e.g., 0.01): Smooth estimates, but slow to adapt.</li>
<li><strong>Applications</strong>: Non-stationary bandits are common in online advertising, recommendation systems, and dynamic pricing.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Exponentially Weighted Average</p>
<p>The constant <span class="arithmatex">\( \alpha \)</span> creates an <strong>exponentially recency-weighted average</strong>, ideal for tracking non-stationary reward distributions.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Mnemonic: Non-Stationary</p>
<p><strong>"ADAPT"</strong>: <strong>A</strong>lways <strong>D</strong>iscount <strong>A</strong>ncient rewards, <strong>P</strong>rioritize <strong>T</strong>oday’s.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Balancing Act</p>
<p>Choosing <span class="arithmatex">\( \alpha \)</span> is critical: too high causes instability; too low lags behind changes. Test multiple <span class="arithmatex">\( \alpha \)</span> values for optimal performance.</p>
</div>
<h2 id="what-is-the-upper-confidence-bound-ucb-approach">What Is the Upper Confidence Bound (UCB) Approach?<a class="headerlink" href="#what-is-the-upper-confidence-bound-ucb-approach" title="Permanent link">&para;</a></h2>
<p>The <strong>Upper Confidence Bound (UCB)</strong> approach is a sophisticated method for action selection in bandit problems, selecting arms by combining their estimated rewards with an uncertainty term to prioritize those with high potential (either due to high estimates or insufficient exploration).</p>
<h3 id="algorithm-details">Algorithm Details<a class="headerlink" href="#algorithm-details" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Action Selection</strong>:
  [
  A_t = \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
  ]</li>
<li><span class="arithmatex">\( Q_t(a) \)</span>: Estimated action value.</li>
<li><span class="arithmatex">\( c &gt; 0 \)</span>: Exploration parameter (typically <span class="arithmatex">\( c = 2 \)</span>) controls the trade-off between exploration and exploitation.</li>
<li><span class="arithmatex">\( \sqrt{\frac{\ln t}{N_t(a)}} \)</span>: Uncertainty term, high when <span class="arithmatex">\( N_t(a) \)</span> is small (few selections) or <span class="arithmatex">\( t \)</span> is large (many steps).</li>
<li>If <span class="arithmatex">\( N_t(a) = 0 \)</span>, assume infinite UCB (select the arm).</li>
<li><strong>Update</strong>:</li>
<li>Pull arm <span class="arithmatex">\( A_t \)</span>, receive <span class="arithmatex">\( R_t \)</span>.</li>
<li>Update: <span class="arithmatex">\( N_{t+1}(A_t) = N_t(A_t) + 1 \)</span>, <span class="arithmatex">\( Q_{t+1}(A_t) = Q_t(A_t) + \frac{1}{N_t(A_t)} [R_t - Q_t(A_t)] \)</span>.</li>
</ul>
<h3 id="intuition">Intuition<a class="headerlink" href="#intuition" title="Permanent link">&para;</a></h3>
<ul>
<li>The UCB value <span class="arithmatex">\( Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \)</span> estimates an <strong>upper bound</strong> on the true <span class="arithmatex">\( q_*(a) \)</span>, assuming optimism in uncertainty.</li>
<li><strong>High <span class="arithmatex">\( Q_t(a) \)</span></strong>: Indicates a promising arm (exploitation).</li>
<li><strong>High <span class="arithmatex">\( \sqrt{\frac{\ln t}{N_t(a)}} \)</span></strong>: Indicates an under-explored arm (exploration).</li>
<li><strong>Dynamic Balance</strong>:</li>
<li>Choosing an arm increases <span class="arithmatex">\( N_t(a) \)</span>, reducing its uncertainty term.</li>
<li>Not choosing an arm increases <span class="arithmatex">\( t \)</span>, raising its uncertainty term.</li>
<li><strong>Logarithmic Term</strong>: <span class="arithmatex">\( \ln t \)</span> ensures exploration diminishes over time but never stops.</li>
</ul>
<h3 id="numerical-example_1">Numerical Example<a class="headerlink" href="#numerical-example_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: 3 machines at <span class="arithmatex">\( t = 1000 \)</span>, <span class="arithmatex">\( c = 2 \)</span>. Current estimates:</li>
<li><span class="arithmatex">\( M_1 \)</span>: <span class="arithmatex">\( Q_t(M_1) = 0.8 \)</span>, <span class="arithmatex">\( N_t(M_1) = 250 \)</span>.</li>
<li><span class="arithmatex">\( M_2 \)</span>: <span class="arithmatex">\( Q_t(M_2) = 1.0 \)</span>, <span class="arithmatex">\( N_t(M_2) = 350 \)</span>.</li>
<li><span class="arithmatex">\( M_3 \)</span>: <span class="arithmatex">\( Q_t(M_3) = 1.2 \)</span>, <span class="arithmatex">\( N_t(M_3) = 400 \)</span>.</li>
<li><strong>Calculate UCB</strong>:</li>
<li><span class="arithmatex">\( \ln 1000 \approx 6.908 \)</span>.</li>
<li><span class="arithmatex">\( M_1 \)</span>: <span class="arithmatex">\( 0.8 + 2 \sqrt{\frac{6.908}{250}} = 0.8 + 2 \sqrt{0.027632} \approx 0.8 + 0.332 = 1.132 \)</span>.</li>
<li><span class="arithmatex">\( M_2 \)</span>: <span class="arithmatex">\( 1.0 + 2 \sqrt{\frac{6.908}{350}} \approx 1.0 + 0.281 = 1.281 \)</span>.</li>
<li><span class="arithmatex">\( M_3 \)</span>: <span class="arithmatex">\( 1.2 + 2 \sqrt{\frac{6.908}{400}} \approx 1.2 + 0.263 = 1.463 \)</span>.</li>
<li><strong>Choice</strong>: Select <span class="arithmatex">\( M_3 \)</span> (UCB = 1.463).</li>
<li><strong>Update</strong>: If <span class="arithmatex">\( R_t = 1 \)</span>:</li>
<li><span class="arithmatex">\( N_{t+1}(M_3) = 401 \)</span>.</li>
<li><span class="arithmatex">\( Q_{t+1}(M_3) = 1.2 + \frac{1}{401} (1 - 1.2) \approx 1.2 - 0.0005 = 1.1995 \)</span>.</li>
</ul>
<h3 id="additional-example-early-steps">Additional Example: Early Steps<a class="headerlink" href="#additional-example-early-steps" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: 2 arms, <span class="arithmatex">\( t = 3 \)</span>, <span class="arithmatex">\( c = 2 \)</span>, <span class="arithmatex">\( Q_1(a) = 0 \)</span>, <span class="arithmatex">\( N_1(a) = 0 \)</span>.</li>
<li><strong>Step 1</strong>: All <span class="arithmatex">\( N_1(a) = 0 \)</span>, pick <span class="arithmatex">\( M_1 \)</span>. Reward <span class="arithmatex">\( R_1 = 1 \)</span>. Update: <span class="arithmatex">\( Q_2(M_1) = 1 \)</span>, <span class="arithmatex">\( N_2(M_1) = 1 \)</span>.</li>
<li><strong>Step 2</strong>: <span class="arithmatex">\( \ln 2 \approx 0.693 \)</span>.</li>
<li><span class="arithmatex">\( M_1 \)</span>: <span class="arithmatex">\( 1 + 2 \sqrt{\frac{0.693}{1}} \approx 1 + 1.665 = 2.665 \)</span>.</li>
<li><span class="arithmatex">\( M_2 \)</span>: <span class="arithmatex">\( 0 + \infty = \infty \)</span>.</li>
<li>Pick <span class="arithmatex">\( M_2 \)</span>. Reward <span class="arithmatex">\( R_2 = 0 \)</span>. Update: <span class="arithmatex">\( Q_3(M_2) = 0 \)</span>, <span class="arithmatex">\( N_3(M_2) = 1 \)</span>.</li>
<li><strong>Step 3</strong>: <span class="arithmatex">\( \ln 3 \approx 1.099 \)</span>.</li>
<li><span class="arithmatex">\( M_1 \)</span>: <span class="arithmatex">\( 1 + 2 \sqrt{\frac{1.099}{1}} \approx 1 + 2.098 = 3.098 \)</span>.</li>
<li><span class="arithmatex">\( M_2 \)</span>: <span class="arithmatex">\( 0 + 2 \sqrt{\frac{1.099}{1}} \approx 2.098 \)</span>.</li>
<li>Pick <span class="arithmatex">\( M_1 \)</span>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">UCB Principle</p>
<p>UCB follows <strong>optimism in the face of uncertainty</strong>, assuming untested arms could be optimal, thus encouraging targeted exploration.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Mnemonic: UCB</p>
<p><strong>"UNCAP"</strong>: <strong>U</strong>ncertainty <strong>N</strong>udges <strong>C</strong>hoice, <strong>A</strong>iming for <strong>P</strong>otential.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Stationarity Assumption</p>
<p>UCB assumes stationary rewards; non-stationary settings require variants like discounted UCB or sliding-window UCB.</p>
</div>
<h2 id="compare-ucb-with-greedy-which-balances-exploration-better">Compare UCB with ε-Greedy: Which Balances Exploration Better?<a class="headerlink" href="#compare-ucb-with-greedy-which-balances-exploration-better" title="Permanent link">&para;</a></h2>
<h3 id="detailed-comparison">Detailed Comparison<a class="headerlink" href="#detailed-comparison" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>ε-Greedy</strong></th>
<th><strong>UCB</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Action Selection</strong></td>
<td>Greedy (<span class="arithmatex">\( \arg\max Q_t(a) \)</span>) with probability <span class="arithmatex">\( 1 - \epsilon \)</span>; random with <span class="arithmatex">\( \epsilon \)</span></td>
<td><span class="arithmatex">\( \arg\max \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right] \)</span></td>
</tr>
<tr>
<td><strong>Exploration Mechanism</strong></td>
<td>Random exploration with fixed or decaying <span class="arithmatex">\( \epsilon \)</span></td>
<td>Uncertainty-driven; prioritizes arms with low <span class="arithmatex">\( N_t(a) \)</span> or high potential</td>
</tr>
<tr>
<td><strong>Exploration Control</strong></td>
<td><span class="arithmatex">\( \epsilon \)</span> (fixed or decaying) controls exploration frequency</td>
<td><span class="arithmatex">\( c \)</span> scales uncertainty term, balancing exploration intensity</td>
</tr>
<tr>
<td><strong>Exploration Pattern</strong></td>
<td>Uniform random exploration, may oversample poor arms</td>
<td>Targeted exploration, favors near-optimal or under-explored arms</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>All arms sampled infinitely (<span class="arithmatex">\( \epsilon &gt; 0 \)</span>), <span class="arithmatex">\( Q_t(a) \to q_*(a) \)</span></td>
<td>All arms sampled, with suboptimal arms selected less over time</td>
</tr>
<tr>
<td><strong>Regret</strong></td>
<td>Linear regret (<span class="arithmatex">\( O(T) \)</span>) with fixed <span class="arithmatex">\( \epsilon \)</span>; sublinear with decay</td>
<td>Logarithmic regret (<span class="arithmatex">\( O(\ln T) \)</span>), theoretically optimal</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Low; simple random choice and updates</td>
<td>Moderate; requires computing <span class="arithmatex">\( \ln t \)</span> and <span class="arithmatex">\( \sqrt{\frac{\ln t}{N_t(a)}} \)</span></td>
</tr>
<tr>
<td><strong>Implementation Ease</strong></td>
<td>Very simple, minimal tuning</td>
<td>More complex, requires tuning <span class="arithmatex">\( c \)</span></td>
</tr>
<tr>
<td><strong>Suitability</strong></td>
<td>Stationary problems, quick prototypes</td>
<td>Stationary problems, long-term optimization</td>
</tr>
<tr>
<td><strong>Non-Stationary</strong></td>
<td>Poor; needs decay or other adaptations</td>
<td>Poor; needs variants like discounted UCB</td>
</tr>
</tbody>
</table>
<h3 id="which-balances-exploration-better">Which Balances Exploration Better?<a class="headerlink" href="#which-balances-exploration-better" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>UCB</strong> is superior for balancing exploration and exploitation in most stationary bandit problems because:</li>
<li><strong>Targeted Exploration</strong>: UCB selects arms based on their potential to be optimal, considering both <span class="arithmatex">\( Q_t(a) \)</span> and uncertainty. This avoids wasting trials on clearly suboptimal arms, unlike ε-greedy’s random exploration.</li>
<li><strong>Theoretical Optimality</strong>: UCB achieves logarithmic regret (<span class="arithmatex">\( O(\ln T) \)</span>), meaning regret grows slowly, compared to ε-greedy’s linear regret (<span class="arithmatex">\( O(T) \)</span>) with fixed <span class="arithmatex">\( \epsilon \)</span> or sublinear regret with decay.</li>
<li><strong>Dynamic Adjustment</strong>: The uncertainty term <span class="arithmatex">\( \sqrt{\frac{\ln t}{N_t(a)}} \)</span> naturally reduces exploration for well-tested arms and increases it for under-explored ones, without manual tuning.</li>
<li><strong>Empirical Evidence</strong>: On the 10-armed testbed (Sutton &amp; Barto, 2020), UCB outperformed ε-greedy (<span class="arithmatex">\( \epsilon = 0.1, 0.01 \)</span>) in average reward (~1.4 vs. ~1.2) and optimal arm selection (~80% vs. ~60%) after 1000 steps.</li>
<li><strong>ε-Greedy Strengths</strong>:</li>
<li><strong>Simplicity</strong>: Easier to implement and understand, ideal for quick prototypes or small-scale problems.</li>
<li><strong>Flexibility with Decay</strong>: Epsilon decay improves performance, though it requires careful tuning.</li>
<li><strong>Robustness</strong>: Performs adequately in noisy environments where UCB’s uncertainty estimates may be less reliable.</li>
<li><strong>ε-Greedy Weaknesses</strong>:</li>
<li>Random exploration can repeatedly select poor arms, increasing regret.</li>
<li>Fixed <span class="arithmatex">\( \epsilon \)</span> causes excessive exploration in later steps.</li>
<li>Tuning <span class="arithmatex">\( \epsilon \)</span> or decay parameters is problem-specific and error-prone.</li>
</ul>
<h3 id="10-armed-testbed-insights">10-Armed Testbed Insights<a class="headerlink" href="#10-armed-testbed-insights" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Setup</strong>: 2000 random 10-armed bandit problems, <span class="arithmatex">\( q_*(a) \sim N(0, 1) \)</span>, rewards <span class="arithmatex">\( R_t \sim N(q_*(a), 1) \)</span>.</li>
<li><strong>Results</strong> (Sutton &amp; Barto, 2020):</li>
<li><strong>Greedy (<span class="arithmatex">\( \epsilon = 0 \)</span>)</strong>: ~1.0 reward, ~33% optimal arm selection (gets stuck on suboptimal arms).</li>
<li><strong>ε-Greedy (<span class="arithmatex">\( \epsilon = 0.1 \)</span>)</strong>: ~1.2 reward, ~60% optimal arm selection.</li>
<li><strong>ε-Greedy (<span class="arithmatex">\( \epsilon = 0.01 \)</span>)</strong>: Slightly better early but plateaus lower than UCB.</li>
<li><strong>UCB (<span class="arithmatex">\( c = 2 \)</span>)</strong>: ~1.4 reward, ~80% optimal arm selection, excelling after initial exploration.</li>
</ul>
<h3 id="when-to-choose-each">When to Choose Each<a class="headerlink" href="#when-to-choose-each" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>UCB</strong>: Preferred for stationary problems with many trials, where low regret and targeted exploration are critical (e.g., ad optimization, clinical trials).</li>
<li><strong>ε-Greedy</strong>: Suitable for quick implementations, noisy environments, or when computational simplicity is prioritized (e.g., educational settings, small-scale tests).</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Winner: UCB</p>
<p>UCB’s uncertainty-driven exploration provides better regret bounds and efficiency, making it the go-to for stationary bandit problems.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Non-Stationary Limitation</p>
<p>Both algorithms struggle with non-stationary rewards. UCB requires adaptations like discounted UCB; ε-greedy needs decay or step-size adjustments.</p>
</div>
<h2 id="conceptuallogical-questions">Conceptual/Logical Questions<a class="headerlink" href="#conceptuallogical-questions" title="Permanent link">&para;</a></h2>
<h3 id="what-is-the-greedy-algorithm_1">What Is the ε-Greedy Algorithm?<a class="headerlink" href="#what-is-the-greedy-algorithm_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Answer</strong>: The ε-greedy algorithm is a bandit strategy that balances exploration and exploitation. It selects the arm with the highest estimated reward (<span class="arithmatex">\( \arg\max Q_t(a) \)</span>) with probability <span class="arithmatex">\( 1 - \epsilon \)</span> (exploitation) and a random arm with probability <span class="arithmatex">\( \epsilon \)</span> (exploration). Action values are updated incrementally using sample averages. It’s simple but may explore inefficiently with fixed <span class="arithmatex">\( \epsilon \)</span>, often improved with epsilon decay.</li>
</ul>
<h3 id="which-parameter-should-be-reduced-to-prevent-inefficient-exploration-why">Which Parameter Should Be Reduced to Prevent Inefficient Exploration? Why?<a class="headerlink" href="#which-parameter-should-be-reduced-to-prevent-inefficient-exploration-why" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Answer</strong>: The <strong>epsilon (<span class="arithmatex">\( \epsilon \)</span>)</strong> parameter should be reduced, typically via decay (e.g., <span class="arithmatex">\( \epsilon_t = \epsilon_0 \cdot \gamma^t \)</span>). High <span class="arithmatex">\( \epsilon \)</span> causes excessive random exploration, selecting suboptimal arms even when <span class="arithmatex">\( Q_t(a) \)</span> is reliable, increasing regret. Reducing <span class="arithmatex">\( \epsilon \)</span> over time shifts focus to exploitation, leveraging accurate estimates to maximize rewards.</li>
</ul>
<h3 id="define-exploration-vs-exploitation">Define Exploration vs. Exploitation<a class="headerlink" href="#define-exploration-vs-exploitation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Answer</strong>:</li>
<li><strong>Exploration</strong>: Choosing arms to learn their reward distributions, improving <span class="arithmatex">\( Q_t(a) \)</span> estimates. It risks short-term losses but enhances long-term performance by identifying the best arm. Example: Trying a new restaurant to assess its quality.</li>
<li><strong>Exploitation</strong>: Selecting the arm with the highest <span class="arithmatex">\( Q_t(a) \)</span> to maximize immediate rewards based on current knowledge. It ensures short-term gains but may miss better arms. Example: Repeatedly visiting your favorite restaurant.</li>
<li><strong>Conflict</strong>: Exploration sacrifices immediate rewards for knowledge; exploitation prioritizes immediate rewards but risks suboptimal long-term choices.</li>
</ul>
<h3 id="does-a-higher-ucb-value-indicate-exploration-or-exploitation">Does a Higher UCB Value Indicate Exploration or Exploitation?<a class="headerlink" href="#does-a-higher-ucb-value-indicate-exploration-or-exploitation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Answer</strong>: A higher UCB value (<span class="arithmatex">\( Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \)</span>) can indicate <strong>exploration</strong> if driven by a large uncertainty term (high <span class="arithmatex">\( \frac{\ln t}{N_t(a)} \)</span>) for under-explored arms, or <strong>exploitation</strong> if driven by a high <span class="arithmatex">\( Q_t(a) \)</span> for well-tested arms. UCB dynamically balances both, favoring arms with high potential to be optimal.</li>
</ul>
<h2 id="revision-checklist">Revision Checklist<a class="headerlink" href="#revision-checklist" title="Permanent link">&para;</a></h2>
<ul>
<li>[ ] Explain the k-armed bandit problem, including its definition, objective, regret, and a detailed restaurant example.</li>
<li>[ ] Describe the ε-greedy algorithm step-by-step and solve a 4-arm numerical example with random numbers.</li>
<li>[ ] Define epsilon decay, explain its necessity, and provide a numerical example with exponential decay.</li>
<li>[ ] Detail action-value methods, including expected vs. estimated values, incremental updates, and a sample calculation.</li>
<li>[ ] Explain tracking non-stationary problems using constant step-size updates, with a numerical example and derivation.</li>
<li>[ ] Describe the UCB approach, its formula, intuition, and solve a 3-arm numerical example at <span class="arithmatex">\( t = 1000 \)</span>.</li>
<li>[ ] Compare UCB and ε-greedy across action selection, regret, and exploration efficiency, citing testbed results.</li>
<li>[ ] Answer conceptual questions on ε-greedy, parameter tuning, exploration vs. exploitation, and UCB values.</li>
<li>[ ] Understand the 10-armed testbed and its implications for algorithm performance.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../js/extras.js"></script>
      
    
  </body>
</html>