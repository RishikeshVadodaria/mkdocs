
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://rishikeshvadodaria.github.io/mkdocs/reinforcement-learning-unit%201/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Unit 1: Basics of Reinforcement Learning - Notes by Rishikesh</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2afb09e1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    
      <link rel="stylesheet" href="../css/navbar.css">
    
      <link rel="stylesheet" href="../css/divider.css">
    
      <link rel="stylesheet" href="../css/font.css">
    
      <link rel="stylesheet" href="../css/waves.css">
    
      <link rel="stylesheet" href="../css/extra.css">
    
      <link rel="stylesheet" href="../css/widgets.css">
    
      <link rel="stylesheet" href="../css/countdown.css">
    
      <link rel="stylesheet" href="../css/nav.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#unit-1-basics-of-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Notes by Rishikesh" class="md-header__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes by Rishikesh
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Unit 1: Basics of Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Notes by Rishikesh" class="md-nav__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    Notes by Rishikesh
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobile-computing.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mobile Computing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cv-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2-formula/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA formulas
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-are-the-real-world-applications-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What Are the Real-World Applications of Reinforcement Learning?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#explain-the-rl-framework-what-are-its-core-components" class="md-nav__link">
    <span class="md-ellipsis">
      Explain the RL Framework. What Are Its Core Components?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Explain the RL Framework. What Are Its Core Components?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-components" class="md-nav__link">
    <span class="md-ellipsis">
      Core Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-process-flow" class="md-nav__link">
    <span class="md-ellipsis">
      RL Process Flow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-the-different-types-of-learning-in-ai" class="md-nav__link">
    <span class="md-ellipsis">
      What Are the Different Types of Learning in AI?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare-and-contrast-supervised-unsupervised-and-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Compare and Contrast Supervised, Unsupervised, and Reinforcement Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#list-and-explain-characteristics-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      List and Explain Characteristics of RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-the-key-elements-of-an-rl-problem" class="md-nav__link">
    <span class="md-ellipsis">
      What Are the Key Elements of an RL Problem?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What Are the Key Elements of an RL Problem?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-maze-navigation" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Maze Navigation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-immediate-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What Is Immediate Reinforcement Learning?
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="unit-1-basics-of-reinforcement-learning">Unit 1: Basics of Reinforcement Learning<a class="headerlink" href="#unit-1-basics-of-reinforcement-learning" title="Permanent link">&para;</a></h1>
<p>Reinforcement Learning (RL) is a dynamic AI paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. This unit explores RL’s applications, framework, comparison with other learning types, characteristics, problem elements, and immediate RL.</p>
<h2 id="what-are-the-real-world-applications-of-reinforcement-learning">What Are the Real-World Applications of Reinforcement Learning?<a class="headerlink" href="#what-are-the-real-world-applications-of-reinforcement-learning" title="Permanent link">&para;</a></h2>
<p>Reinforcement Learning powers systems that require adaptive decision-making in complex, dynamic environments. Its applications span multiple domains, leveraging trial-and-error learning to optimize outcomes. Key examples include:</p>
<ul>
<li><strong>Toddler Learning to Walk</strong>: A child experiments with steps, receiving feedback (stability or falling), and refines movements to walk steadily, mirroring RL’s reward-driven learning.</li>
<li><strong>Game-Playing Agents</strong>: Systems like DeepMind’s AlphaGo and OpenAI’s Dota 2 bot learn optimal strategies by playing millions of games, maximizing scores or wins.</li>
<li><strong>Robotics</strong>: Robots use RL to master tasks like grasping objects or navigating spaces, optimizing actions based on sensor feedback (e.g., +10 for a successful grasp).</li>
<li><strong>Autonomous Vehicles</strong>: Self-driving cars adjust speed, steering, or lane changes to maximize safety and efficiency, using rewards tied to traffic rules and collision avoidance.</li>
<li><strong>Healthcare</strong>: RL optimizes treatment plans, such as adjusting drug dosages for chronic diseases, maximizing patient recovery metrics.</li>
<li><strong>Finance</strong>: Trading algorithms learn to buy or sell stocks to maximize profits, with rewards based on portfolio growth.</li>
<li><strong>Smart Grids</strong>: RL manages energy distribution in power systems, optimizing for cost, demand, and sustainability.</li>
<li><strong>Recommendation Systems</strong>: Platforms like Netflix use RL to suggest content, maximizing user engagement (e.g., time spent watching).</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Mnemonic: RL Applications</p>
<p><strong>"GAMES"</strong>: <strong>G</strong>ames, <strong>A</strong>utonomous systems, <strong>M</strong>edicine, <strong>E</strong>nergy, <strong>S</strong>tock trading.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Why RL Excels</p>
<p>RL thrives in scenarios where explicit instructions are impractical, and learning from interaction is the only viable approach.</p>
</div>
<h2 id="explain-the-rl-framework-what-are-its-core-components">Explain the RL Framework. What Are Its Core Components?<a class="headerlink" href="#explain-the-rl-framework-what-are-its-core-components" title="Permanent link">&para;</a></h2>
<p>The RL framework models an agent learning optimal behavior through interaction with an environment. The agent observes the environment’s state, takes actions, receives rewards, and updates its strategy to maximize long-term rewards. This iterative process is formalized as a <strong>Markov Decision Process (MDP)</strong>.</p>
<h3 id="core-components">Core Components<a class="headerlink" href="#core-components" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Agent</strong>: The learner or decision-maker (e.g., a robot, game AI).</li>
<li><strong>Environment</strong>: The external system the agent interacts with (e.g., a maze, stock market).</li>
<li><strong>State (S)</strong>: A snapshot of the environment at a given time (e.g., the agent’s position in a grid).</li>
<li><strong>Action (A)</strong>: Choices the agent can make (e.g., move left, buy stock).</li>
<li><strong>Reward (R)</strong>: A scalar feedback signal indicating the action’s success (e.g., +100 for reaching a goal).</li>
<li><strong>Policy (π)</strong>: The agent’s strategy, mapping states to actions (deterministic: π(s) = a, or probabilistic: π(a|s)).</li>
<li><strong>Value Function (V or Q)</strong>: Estimates the expected cumulative reward for a state (V(s)) or state-action pair (Q(s, a)).</li>
<li><strong>Transition Function (P)</strong>: Defines the probability of moving to a new state given a state and action (P(s’|s, a)).</li>
<li><strong>Discount Factor (γ)</strong>: Balances immediate vs. future rewards (0 ≤ γ ≤ 1; e.g., γ = 0.9 prioritizes long-term gains).</li>
</ol>
<h3 id="rl-process-flow">RL Process Flow<a class="headerlink" href="#rl-process-flow" title="Permanent link">&para;</a></h3>
<p>The agent-environment interaction forms a loop, as shown in this Mermaid diagram:</p>
<p><img alt="alt text" src="../images/image2.png" /></p>
<h3 id="how-it-works">How It Works<a class="headerlink" href="#how-it-works" title="Permanent link">&para;</a></h3>
<ol>
<li>The agent observes the current state <span class="arithmatex">\( s_t \)</span>.</li>
<li>Based on its policy <span class="arithmatex">\( \pi \)</span>, it selects an action <span class="arithmatex">\( a_t \)</span>.</li>
<li>The environment responds with a reward <span class="arithmatex">\( r_{t+1} \)</span> and a new state <span class="arithmatex">\( s_{t+1} \)</span>.</li>
<li>The agent updates its policy or value function to improve future decisions.</li>
<li>The cycle repeats, aiming to maximize the cumulative reward:</li>
</ol>
<div class="arithmatex">\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots\]</div>
<div class="admonition note">
<p class="admonition-title">Definition: RL Framework</p>
<p>RL is a <strong>sequential decision-making process</strong> where an agent learns an optimal policy by trial and error to maximize expected discounted rewards.</p>
</div>
<h2 id="what-are-the-different-types-of-learning-in-ai">What Are the Different Types of Learning in AI?<a class="headerlink" href="#what-are-the-different-types-of-learning-in-ai" title="Permanent link">&para;</a></h2>
<p>AI learning paradigms vary based on data and feedback mechanisms. The three primary types are:</p>
<p><strong>Supervised Learning</strong>:</p>
<ul>
<li>Uses labeled datasets (input-output pairs) to train models.</li>
<li>Goal: Predict accurate outputs for new inputs.</li>
<li>Example: Classifying images as “cat” or “dog” using labeled images.</li>
<li>Applications: Spam detection, speech recognition, regression.</li>
</ul>
<p><strong>Unsupervised Learning</strong>:</p>
<ul>
<li>Works with unlabeled data to find hidden patterns or structures.</li>
<li>Goal: Group or organize data without predefined outputs.</li>
<li>Example: Clustering customers based on purchasing behavior.</li>
<li>Applications: Market segmentation, anomaly detection, data compression.</li>
</ul>
<p><strong>Reinforcement Learning</strong>:</p>
<ul>
<li>Learns through interaction with an environment, guided by rewards.</li>
<li>Goal: Maximize cumulative rewards through trial and error.</li>
<li>Example: A robot learning to navigate a maze by earning rewards for reaching the exit.</li>
<li>Applications: Game AI, robotics, resource optimization.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Mnemonic: AI Learning Types</p>
<p><strong>"SUP"</strong>: <strong>S</strong>upervised (predict), <strong>U</strong>nsupervised (pattern), <strong>P</strong> for RL (policy/reward).</p>
</div>
<h2 id="compare-and-contrast-supervised-unsupervised-and-reinforcement-learning">Compare and Contrast Supervised, Unsupervised, and Reinforcement Learning<a class="headerlink" href="#compare-and-contrast-supervised-unsupervised-and-reinforcement-learning" title="Permanent link">&para;</a></h2>
<p>The table below provides a detailed comparison of the three learning paradigms:</p>
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>Supervised Learning</strong></th>
<th><strong>Unsupervised Learning</strong></th>
<th><strong>Reinforcement Learning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Labeled (input-output pairs, e.g., images with labels)</td>
<td>Unlabeled (raw data, e.g., customer transactions)</td>
<td>No predefined dataset; generated via environment interaction</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>Minimize prediction error to match labels</td>
<td>Discover patterns, clusters, or representations</td>
<td>Maximize cumulative reward over time</td>
</tr>
<tr>
<td><strong>Feedback</strong></td>
<td>Direct feedback (correct labels)</td>
<td>No feedback; relies on data structure</td>
<td>Sparse, delayed rewards from environment</td>
</tr>
<tr>
<td><strong>Learning Process</strong></td>
<td>Train on static dataset, optimize loss (e.g., MSE)</td>
<td>Iterative pattern discovery (e.g., k-means)</td>
<td>Trial-and-error, policy optimization</td>
</tr>
<tr>
<td><strong>Example</strong></td>
<td>Predicting house prices from features</td>
<td>Grouping similar news articles</td>
<td>Training a drone to avoid obstacles</td>
</tr>
<tr>
<td><strong>Challenges</strong></td>
<td>Requires large labeled datasets</td>
<td>Hard to evaluate results without labels</td>
<td>Balancing exploration vs. exploitation</td>
</tr>
<tr>
<td><strong>Applications</strong></td>
<td>Image recognition, natural language processing</td>
<td>Dimensionality reduction, clustering</td>
<td>Autonomous systems, game AI, optimization</td>
</tr>
<tr>
<td><strong>Environment Interaction</strong></td>
<td>None; data is static</td>
<td>None; data is static</td>
<td>Dynamic; actions affect future states</td>
</tr>
<tr>
<td><strong>Time Horizon</strong></td>
<td>One-shot predictions</td>
<td>One-shot or iterative analysis             **</td>
<td>Sequential, long-term decision-making</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Key Difference</p>
<p>RL’s <strong>dynamic interaction</strong> and <strong>delayed rewards</strong> distinguish it from supervised and unsupervised learning, which rely on static datasets and immediate feedback.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Common Pitfall</p>
<p>Misapplying supervised learning to sequential decision tasks (like robotics) can fail, as it doesn’t account for action-state dependencies, unlike RL.</p>
</div>
<h2 id="list-and-explain-characteristics-of-rl">List and Explain Characteristics of RL<a class="headerlink" href="#list-and-explain-characteristics-of-rl" title="Permanent link">&para;</a></h2>
<p>RL’s unique characteristics define its approach to learning:</p>
<p><strong>Trial-and-Error Learning</strong>:</p>
<ul>
<li>Agents learn by experimenting with actions and observing outcomes.</li>
<li>Example: A robot tries different arm movements to pick up an object, refining based on success or failure.</li>
</ul>
<p><strong>Delayed Rewards</strong>:</p>
<ul>
<li>Rewards may come after a sequence of actions, requiring the agent to plan for long-term outcomes.</li>
<li>Example: In chess, the reward (win/loss) is only received at the game’s end.</li>
</ul>
<p><strong>Dynamic Environment</strong>:</p>
<ul>
<li>The environment changes based on the agent’s actions, creating a feedback loop.</li>
<li>Example: A self-driving car’s actions (e.g., braking) alter traffic conditions.</li>
</ul>
<p><strong>No Supervisor</strong>:</p>
<ul>
<li>Unlike supervised learning, RL relies on reward signals, not explicit instructions.</li>
<li>Example: A game AI learns strategies without being told the “correct” move.</li>
</ul>
<p><strong>Exploration vs. Exploitation</strong>:</p>
<ul>
<li>Agents must balance exploring new actions (to discover better strategies) with exploiting known rewarding actions.</li>
<li>Example: A trading bot tests new stocks (exploration) but focuses on profitable ones (exploitation).</li>
</ul>
<p><strong>Sequential Decision-Making</strong>:</p>
<ul>
<li>Actions influence future states and rewards, requiring foresight.</li>
<li>Example: In a maze, moving left now may block a future path.</li>
</ul>
<h2 id="what-are-the-key-elements-of-an-rl-problem">What Are the Key Elements of an RL Problem?<a class="headerlink" href="#what-are-the-key-elements-of-an-rl-problem" title="Permanent link">&para;</a></h2>
<p>An RL problem is formalized as a <strong>Markov Decision Process (MDP)</strong>, which includes the following elements:</p>
<p><strong>Set of States (S)</strong>:</p>
<ul>
<li>All possible configurations of the environment.</li>
<li>Example: In a grid world, states are the agent’s coordinates (x, y).</li>
</ul>
<p><strong>Set of Actions (A)</strong>:</p>
<ul>
<li>All possible decisions the agent can make.</li>
<li>Example: Move {up, down, left, right} in a maze.</li>
</ul>
<p><strong>Reward Function (R)</strong>:</p>
<ul>
<li>Maps state-action pairs (or states) to a scalar reward.</li>
<li>Example: R(s, a) = +100 for reaching the goal, -1 for hitting a wall.</li>
</ul>
<p><strong>Transition Probability (P)</strong>:</p>
<ul>
<li>Defines the likelihood of moving to a new state given a current state and action (P(s’|s, a)).</li>
<li>Example: Moving right in a deterministic grid moves to (x+1, y).</li>
</ul>
<p><strong>Discount Factor (γ)</strong>:</p>
<ul>
<li>A value (0 ≤ γ ≤ 1) that balances immediate vs. future rewards.</li>
<li>Example: γ = 0.9 discounts future rewards, prioritizing near-term gains.</li>
</ul>
<p><strong>Policy (π)</strong>:</p>
<ul>
<li>The agent’s strategy, mapping states to actions (π(s) = a or π(a|s) for probabilistic policies).</li>
<li>Example: A policy might dictate “always move toward the goal.”</li>
</ul>
<h3 id="example-maze-navigation">Example: Maze Navigation<a class="headerlink" href="#example-maze-navigation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>States</strong>: Grid cells (e.g., (2, 3)).</li>
<li><strong>Actions</strong>: {up, down, left, right}.</li>
<li><strong>Rewards</strong>: +100 for reaching the exit, -1 for each step.</li>
<li><strong>Transitions</strong>: Deterministic (move right from (1, 1) goes to (2, 1)).</li>
<li><strong>Discount Factor</strong>: γ = 0.95.</li>
<li><strong>Policy</strong>: Move toward the exit while avoiding walls.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Formula: Objective</p>
<p>The agent seeks to maximize the expected cumulative reward:
[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
]
where <span class="arithmatex">\( G_t \)</span> is the discounted return at time <span class="arithmatex">\( t \)</span>.</p>
</div>
<h2 id="what-is-immediate-reinforcement-learning">What Is Immediate Reinforcement Learning?<a class="headerlink" href="#what-is-immediate-reinforcement-learning" title="Permanent link">&para;</a></h2>
<p>Immediate Reinforcement Learning is a simplified RL setting where the agent receives rewards <strong>immediately</strong> after each action, eliminating the complexity of delayed rewards.</p>
<ul>
<li><strong>Characteristics</strong>:</li>
<li>Rewards are tied directly to the action just taken, providing instant feedback.</li>
<li>Simplifies learning, as the agent doesn’t need to account for future states or long-term consequences.</li>
<li>Often used in controlled or educational environments to teach RL basics.</li>
<li><strong>Example</strong>:</li>
<li>In a tic-tac-toe game, the agent gets +10 for placing a mark that wins the game immediately.</li>
<li>In a slot machine simulation, each pull yields an instant reward (+5 or 0).</li>
<li><strong>Advantages</strong>:</li>
<li>Faster learning due to clear action-reward associations.</li>
<li>Easier to implement and debug in simple environments.</li>
<li><strong>Limitations</strong>:</li>
<li>Rarely reflects real-world scenarios, where rewards are often delayed (e.g., winning a chess game requires many moves).</li>
<li>Less applicable to complex tasks requiring strategic planning.</li>
<li><strong>Comparison to Standard RL</strong>:</li>
<li>Standard RL handles delayed rewards, requiring algorithms like Q-learning or policy gradients to estimate future rewards.</li>
<li>Immediate RL is a special case, often used in bandit problems (e.g., choosing the best slot machine).</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Mnemonic: Immediate RL</p>
<p><strong>"AIR"</strong>: <strong>A</strong>ction, <strong>I</strong>nstant <strong>R</strong>eward.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Practical Note</p>
<p>Immediate RL is useful for prototyping or teaching but should transition to delayed reward models for real-world applications.</p>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../js/extras.js"></script>
      
    
  </body>
</html>