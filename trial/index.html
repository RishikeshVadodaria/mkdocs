
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://rishikeshvadodaria.github.io/mkdocs/trial/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Trial - Notes by Rishikesh</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2afb09e1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    
      <link rel="stylesheet" href="../css/navbar.css">
    
      <link rel="stylesheet" href="../css/divider.css">
    
      <link rel="stylesheet" href="../css/font.css">
    
      <link rel="stylesheet" href="../css/waves.css">
    
      <link rel="stylesheet" href="../css/extra.css">
    
      <link rel="stylesheet" href="../css/widgets.css">
    
      <link rel="stylesheet" href="../css/countdown.css">
    
      <link rel="stylesheet" href="../css/nav.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#machine-learning-types" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Notes by Rishikesh" class="md-header__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes by Rishikesh
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Trial
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Notes by Rishikesh" class="md-nav__button md-logo" aria-label="Notes by Rishikesh" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.36 2.64c1.64 0 3 1.36 3 3 0 1.65-1.36 3-3 3-1.65 0-3-1.35-3-3 0-.3.05-.58.14-.84-1.07-.51-2.25-.8-3.5-.8a8 8 0 0 0-8 8l.04.84-1.99.21L2 12A10 10 0 0 1 12 2c1.69 0 3.28.42 4.67 1.16.49-.33 1.07-.52 1.69-.52m0 2a1 1 0 0 0-1 1 1 1 0 0 0 1 1c.56 0 1-.45 1-1 0-.56-.44-1-1-1M5.64 15.36c1.65 0 3 1.35 3 3 0 .3-.05.58-.14.84 1.07.51 2.25.8 3.5.8a8 8 0 0 0 8-8l-.04-.84 1.99-.21L22 12a10 10 0 0 1-10 10c-1.69 0-3.28-.42-4.67-1.16-.49.33-1.07.52-1.69.52-1.64 0-3-1.36-3-3 0-1.65 1.36-3 3-3m0 2c-.56 0-1 .45-1 1 0 .56.44 1 1 1a1 1 0 0 0 1-1 1 1 0 0 0-1-1M12 8a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4"/></svg>

    </a>
    Notes by Rishikesh
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobile-computing.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mobile Computing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cv-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpa-m2-formula/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RPA formulas
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#machine-learning-types" class="md-nav__link">
    <span class="md-ellipsis">
      Machine Learning Types
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Machine Learning Types">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Supervised Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Common Algorithms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning_1" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      Key Characteristics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-rl-differs-from-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      How RL Differs from Supervised Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#elements-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Elements of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Elements of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#agent" class="md-nav__link">
    <span class="md-ellipsis">
      Agent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#environment" class="md-nav__link">
    <span class="md-ellipsis">
      Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Process
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advantages-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages of Reinforcement Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#disadvantages-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Disadvantages of Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Disadvantages of Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-apply-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      When to Apply Reinforcement Learning?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-is-reinforcement-learning-difficult" class="md-nav__link">
    <span class="md-ellipsis">
      Why Is Reinforcement Learning Difficult?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#core-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      Core Concepts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rl-algorithm-steps" class="md-nav__link">
    <span class="md-ellipsis">
      RL Algorithm Steps
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-and-planning" class="md-nav__link">
    <span class="md-ellipsis">
      Learning and Planning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning and Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#two-fundamental-problems-in-sequential-decision-making" class="md-nav__link">
    <span class="md-ellipsis">
      Two Fundamental Problems in Sequential Decision Making
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Two Fundamental Problems in Sequential Decision Making">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#planning" class="md-nav__link">
    <span class="md-ellipsis">
      Planning:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-of-the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Model of the Environment:
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based" class="md-nav__link">
    <span class="md-ellipsis">
      Types of Reinforcement Learning Algorithms ( on the basis of model based)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Types of Reinforcement Learning Algorithms ( on the basis of model based)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-free-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Model-free Reinforcement Learning :
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Model-based Reinforcement Learning:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-differences-between-model-free-and-model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Key Differences Between Model-free and Model-based Reinforcement Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state" class="md-nav__link">
    <span class="md-ellipsis">
      RL Framework - The RL Process: A Loop of State, Action, Reward, and Next State
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#main-characteristics-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Main Characteristics of RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl-problem-challenges-in-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL) Problem - Challenges in RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reinforcement Learning (RL) Problem - Challenges in RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#trade-off-between-exploration-and-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Trade-off between Exploration and Exploitation:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fundamental-components-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Fundamental Components of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fundamental Components of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy" class="md-nav__link">
    <span class="md-ellipsis">
      Policy:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Policy:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-analogy" class="md-nav__link">
    <span class="md-ellipsis">
      Human Analogy:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-function" class="md-nav__link">
    <span class="md-ellipsis">
      Value function:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-analog" class="md-nav__link">
    <span class="md-ellipsis">
      Human Analog
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Human Analog">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reward-function" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Function:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reward Function:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-analogy_1" class="md-nav__link">
    <span class="md-ellipsis">
      Human Analogy:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-of-the-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model of the Environment:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-analogy_2" class="md-nav__link">
    <span class="md-ellipsis">
      Human Analogy:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-element-of-rl-systems-model-of-the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Final Element of RL Systems: Model of the Environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#types-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Types of Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Types of Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#value-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Value-Based Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policy-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Policy-Based Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-reinforcement-learning_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model-Based Reinforcement Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h2 id="machine-learning-types">Machine Learning Types<a class="headerlink" href="#machine-learning-types" title="Permanent link">&para;</a></h2>
<pre class="mermaid"><code>graph LR
    subgraph Input
    A[Data with Labels] --&gt; B[Supervised Learning]
    C[Data without Labels] --&gt; D[Unsupervised Learning]
    E[States + Actions] --&gt; F[Reinforcement Learning]
    end

    B --&gt; G[Mapping]
    D --&gt; H[Classes]
    F --&gt; I[Action]

    B -.-&gt;|Error| B
    F -.-&gt;|Reward| F

</code></pre>
<h4 id="supervised-learning">Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Input</strong>: Labeled data (features + target labels)</li>
<li><strong>Process</strong>: Learning from labeled examples</li>
<li><strong>Output</strong>: Prediction model</li>
<li><strong>Feedback</strong>: Error measurement against known labels</li>
<li><strong>Applications</strong>: </li>
<li>Classification (spam detection, image recognition)</li>
<li>Regression (price prediction, sales forecasting)</li>
</ul>
<h4 id="unsupervised-learning">Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Input</strong>: Unlabeled data</li>
<li><strong>Process</strong>: Pattern/structure discovery</li>
<li><strong>Output</strong>: Data grouping/structure</li>
<li><strong>Feedback</strong>: Internal validation metrics</li>
<li><strong>Applications</strong>:</li>
<li>Clustering (customer segmentation)</li>
<li>Dimensionality reduction (feature extraction)</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Input</strong>: States + possible actions</li>
<li><strong>Process</strong>: Trial and error learning</li>
<li><strong>Output</strong>: Action policy</li>
<li><strong>Feedback</strong>: Rewards/penalties</li>
<li><strong>Applications</strong>:</li>
<li>Game AI (chess, Go)</li>
<li>Robotics (navigation, control)</li>
</ul>
<h5 id="common-algorithms">Common Algorithms<a class="headerlink" href="#common-algorithms" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Supervised</strong>: Linear Regression, Random Forest, Neural Networks</li>
<li><strong>Unsupervised</strong>: K-means, PCA, Autoencoders</li>
<li><strong>Reinforcement</strong>: Q-Learning, Policy Gradient, DQN</li>
</ul>
<hr />
<h4 id="reinforcement-learning_1">Reinforcement Learning:<a class="headerlink" href="#reinforcement-learning_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>What to do</strong></li>
<li><strong>How to map situations to actions</strong></li>
<li><strong>Maximizing a numerical reward signal</strong></li>
</ul>
<p><strong>Reinforcement learning</strong> is an autonomous, self-teaching system that essentially learns by trial and error. It performs actions with the aim of maximizing rewards, or in other words, it is learning by doing in order to achieve the best outcomes.</p>
<p><strong>Reinforcement Learning</strong> is a feedback-based Machine learning technique in which an agent
learns to behave in an environment by performing the actions and seeing the results of
actions. For each good action, the agent gets positive feedback, and for each bad action, the
agent gets negative feedback or penalty</p>
<h3 id="key-characteristics">Key Characteristics<a class="headerlink" href="#key-characteristics" title="Permanent link">&para;</a></h3>
<p>Reinforcement Learning is inspired by how <strong>humans and animals</strong> learn through interactions:</p>
<ul>
<li><strong>Reward and Punishment</strong>: Encourages repeating actions that lead to rewards and avoiding punishments.</li>
<li><strong>Trial and Error</strong>: Similar to trying different methods until the correct one is found.</li>
<li><strong>Learning Over Time</strong>: Improvement occurs through continuous experience.</li>
<li>
<p><strong>Rewards Come from a Sequence of Actions</strong>.</p>
</li>
<li>
<p>The learner is <strong>not told</strong> which actions to take but must <strong>discover</strong> them through trial and error.</p>
</li>
<li>Actions affect not only the <strong>immediate reward</strong> but also <strong>future situations and rewards</strong>.</li>
<li>Works well in problems where a <strong>sequence of decisions</strong> is important.</li>
</ul>
<p>RL is an <strong>autonomous, self-teaching</strong> system that learns by <strong>trial and error</strong>.
The goal is to <strong>maximize rewards</strong> over time.</p>
<p><strong>Example Applications</strong>:
    - Chess
    - Maze solving
    - Industrial robot arms
    - Path planning
    - Sweeper robots</p>
<h2 id="how-rl-differs-from-supervised-learning">How RL Differs from Supervised Learning<a class="headerlink" href="#how-rl-differs-from-supervised-learning" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Supervised Learning</th>
<th>Reinforcement Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Data</strong></td>
<td>Has labeled answers</td>
<td>No labeled answers; learns from experience</td>
</tr>
<tr>
<td><strong>Decision Making</strong></td>
<td>Independent of past decisions</td>
<td>Dependent on past decisions</td>
</tr>
<tr>
<td><strong>Learning Method</strong></td>
<td>Trained with a dataset</td>
<td>Learns through trial and error</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="elements-of-rl">Elements of RL<a class="headerlink" href="#elements-of-rl" title="Permanent link">&para;</a></h2>
<p>![[Pasted image 20250131000424.png|500]]</p>
<h4 id="agent">Agent<a class="headerlink" href="#agent" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Definition</strong>: An entity that interacts with the environment.</li>
<li><strong>Examples</strong>: Robot, human, software program.</li>
</ul>
<h4 id="environment">Environment<a class="headerlink" href="#environment" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Definition</strong>: The external system in which the agent operates.</li>
<li><strong>Examples</strong>: Physical world, game simulation.</li>
</ul>
<h4 id="learning-process">Learning Process<a class="headerlink" href="#learning-process" title="Permanent link">&para;</a></h4>
<ol>
<li>The agent moves from the <strong>initial state</strong> to the <strong>goal state</strong>.</li>
<li>The agent continually asks, <em>"What is the best action in each state?"</em></li>
</ol>
<hr />
<h2 id="advantages-of-reinforcement-learning">Advantages of Reinforcement Learning<a class="headerlink" href="#advantages-of-reinforcement-learning" title="Permanent link">&para;</a></h2>
<p>✅ No need for predefined instructions or human intervention. 
✅ Can adapt to <strong>both static and dynamic environments</strong>. 
✅ Solves a <strong>wide range of problems</strong> (decision-making, prediction, optimization). 
✅ Improves with <strong>experience</strong> and fine-tunes over time.</p>
<h2 id="disadvantages-of-reinforcement-learning">Disadvantages of Reinforcement Learning<a class="headerlink" href="#disadvantages-of-reinforcement-learning" title="Permanent link">&para;</a></h2>
<p>❌ Performance depends on the <strong>quality of the reward function</strong>. 
❌ <strong>Designing and tuning</strong> RL models can be <strong>complex</strong>.</p>
<hr />
<blockquote>
<p>[!NOTE]</p>
<h6 id="when-to-apply-reinforcement-learning">When to Apply Reinforcement Learning?<a class="headerlink" href="#when-to-apply-reinforcement-learning" title="Permanent link">&para;</a></h6>
<p>Reinforcement Learning is most suitable when:
- The problem environment is <strong>complex and uncertain</strong>, making traditional programming methods ineffective.
- Feedback is <strong>sparse, delayed, and dependent</strong> on multiple decisions.
- Decision-making (actions) follows a <strong>feedback loop</strong>.</p>
<h6 id="why-is-reinforcement-learning-difficult">Why Is Reinforcement Learning Difficult?<a class="headerlink" href="#why-is-reinforcement-learning-difficult" title="Permanent link">&para;</a></h6>
<p>The toughest parts of Reinforcement Learning are:
- <strong>Mapping the Environment</strong>.
- <strong>Including All Possible Actions</strong>.</p>
<h6 id="core-concepts">Core Concepts<a class="headerlink" href="#core-concepts" title="Permanent link">&para;</a></h6>
<ul>
<li><strong>Goal-Oriented Learning</strong>: The agent learns by trying to achieve a goal.</li>
<li><strong>Learning from Consequences</strong>: The agent learns from the consequences of its actions.</li>
<li><strong>Active Research Area</strong>: RL is one of the most <strong>active</strong> fields in Artificial Intelligence (AI).</li>
</ul>
</blockquote>
<h2 id="rl-algorithm-steps">RL Algorithm Steps<a class="headerlink" href="#rl-algorithm-steps" title="Permanent link">&para;</a></h2>
<pre class="mermaid"><code>graph TD;
    A[Agent Observes Environment] --&gt; B[Agent Performs Action];
    B --&gt; C[Agent Moves to New State];
    C --&gt; D[Agent Receives Reward];
    D --&gt; E[Agent Evaluates Action - Good or Bad];
    E --&gt; F[Agent Adjusts Strategy to Maximize Reward];

</code></pre>
<hr />
<h2 id="learning-and-planning">Learning and Planning<a class="headerlink" href="#learning-and-planning" title="Permanent link">&para;</a></h2>
<h4 id="two-fundamental-problems-in-sequential-decision-making">Two Fundamental Problems in Sequential Decision Making<a class="headerlink" href="#two-fundamental-problems-in-sequential-decision-making" title="Permanent link">&para;</a></h4>
<h5 id="reinforcement-learning-rl">Reinforcement Learning (RL):<a class="headerlink" href="#reinforcement-learning-rl" title="Permanent link">&para;</a></h5>
<ul>
<li>The environment is initially unknown.</li>
<li>The agent interacts with the environment.<ul>
<li>The agent improves its policy.</li>
</ul>
</li>
</ul>
<h5 id="planning">Planning:<a class="headerlink" href="#planning" title="Permanent link">&para;</a></h5>
<ul>
<li>A model of the environment is known.</li>
<li>The agent performs computations with its model (without any external interaction).</li>
<li>The agent improves its policy, also known as <strong>deliberation, reasoning, introspection, pondering, thought, search</strong>.</li>
</ul>
<hr />
<h2 id="model-of-the-environment">Model of the Environment:<a class="headerlink" href="#model-of-the-environment" title="Permanent link">&para;</a></h2>
<ul>
<li>A <strong>model</strong> mimics the behavior of the environment. With the help of the model, one can make inferences about how the environment will behave. For example, if a state and an action are given, the model can predict the next state and reward.</li>
<li>The model is used for <strong>planning</strong>, providing a way to take a course of action by considering all future situations before actually experiencing those situations.</li>
<li>Approaches for solving RL problems with the help of the model are termed <strong>model-based approach</strong>.</li>
<li>An approach without using a model is called a <strong>model-free approach</strong>.</li>
</ul>
<hr />
<p>![[Pasted image 20250130232904.png]]</p>
<h2 id="types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based">Types of Reinforcement Learning Algorithms ( on the basis of model based)<a class="headerlink" href="#types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based" title="Permanent link">&para;</a></h2>
<p>There are various algorithms used in reinforcement learning such as Q-learning, policy gradient
methods, Monte Carlo method and many more. All these algorithms can be classified into two broad categories - </p>
<h4 id="model-free-reinforcement-learning">Model-free Reinforcement Learning :<a class="headerlink" href="#model-free-reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>It is a category of reinforcement learning algorithms that learns to make decisions by</li>
<li>interacting with the environment directly, without creating a model of the environment's</li>
<li>dynamics.</li>
<li>The agent performs different actions multiple times to learn the outcomes and creates a</li>
<li>strategy (policy) that optimizes its reward points. This is ideal for changing, large or complex</li>
<li>environments.</li>
<li>Not applicable for some scenario like self driving car.</li>
</ul>
<h4 id="model-based-reinforcement-learning">Model-based Reinforcement Learning:<a class="headerlink" href="#model-based-reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>This category of reinforcement learning algorithms involves creating a <strong>model of the environment's dynamics</strong> to make decisions and improve performance.</li>
<li>Ideal for environments that are <strong>static and well-defined</strong>, where real-world environment testing is difficult.</li>
</ul>
<hr />
<h2 id="key-differences-between-model-free-and-model-based-reinforcement-learning">Key Differences Between Model-free and Model-based Reinforcement Learning<a class="headerlink" href="#key-differences-between-model-free-and-model-based-reinforcement-learning" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th><strong>Model-Free RL</strong></th>
<th><strong>Model-Based RL</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Learning Approach</strong></td>
<td>Direct learning from environment</td>
<td>Indirect learning through model building</td>
</tr>
<tr>
<td><strong>Efficiency</strong></td>
<td>Requires more real-world interactions</td>
<td>More sample-efficient</td>
</tr>
<tr>
<td><strong>Complexity</strong></td>
<td>Simpler implementation</td>
<td>More complex due to model learning</td>
</tr>
<tr>
<td><strong>Environment Utilization</strong></td>
<td>No internal model</td>
<td>Builds and uses a model</td>
</tr>
<tr>
<td><strong>Adaptability</strong></td>
<td>Slower to adapt to changes</td>
<td>Faster adaptation with accurate model</td>
</tr>
<tr>
<td><strong>Computational Requirements</strong></td>
<td>Less intensive</td>
<td>More computational resources needed</td>
</tr>
<tr>
<td><strong>Examples</strong></td>
<td>Q-Learning, SARSA, DQN, PPO</td>
<td>Dyna-Q, Model-Based Value Iteration</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state">RL Framework - The RL Process: A Loop of State, Action, Reward, and Next State<a class="headerlink" href="#rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state" title="Permanent link">&para;</a></h2>
<p>![[Pasted image 20250130232937.png]]</p>
<hr />
<h2 id="main-characteristics-of-rl">Main Characteristics of RL<a class="headerlink" href="#main-characteristics-of-rl" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>No supervisor</strong> while training.</li>
<li><strong>Environment</strong> is generally stochastic for real-world applications.</li>
<li><strong>Model of the environment</strong> can be incomplete.</li>
<li><strong>Feedback</strong> (Negative/Positive Reward) can be delayed or partial.</li>
<li>The agent uses experience from the past to improve its performance over time.</li>
<li>Actions that have fetched more rewards are preferred.</li>
<li>The agent tries various actions and prefers those that are best or have fetched more rewards.</li>
<li>RL uses <strong>Markov Decision Process (MDP)</strong> framework to define the interaction between a learning agent and its environment.</li>
</ul>
<hr />
<h2 id="reinforcement-learning-rl-problem-challenges-in-rl">Reinforcement Learning (RL) Problem - Challenges in RL<a class="headerlink" href="#reinforcement-learning-rl-problem-challenges-in-rl" title="Permanent link">&para;</a></h2>
<h3 id="trade-off-between-exploration-and-exploitation">Trade-off between Exploration and Exploitation:<a class="headerlink" href="#trade-off-between-exploration-and-exploitation" title="Permanent link">&para;</a></h3>
<ul>
<li>To obtain rewards, an RL agent must prefer actions that it has tried in the past and found effective (<strong>Exploit</strong>).</li>
<li>However, to discover such actions, it must try actions it has not selected before (<strong>Explore</strong>).</li>
</ul>
<blockquote>
<p>[!NOTE]
Neither exploration nor exploitation can be pursued exclusively without failing at the task.
</p>
</blockquote>
<hr />
<h2 id="fundamental-components-of-rl">Fundamental Components of RL<a class="headerlink" href="#fundamental-components-of-rl" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Policy</strong>: Defines the agent’s behavior.</li>
<li><strong>Reward Function</strong>: Provides feedback on actions.</li>
<li><strong>Value Function</strong>: Evaluates future rewards.</li>
<li><strong>Model of the Environment</strong>: Simulates how the environment works.</li>
</ul>
<h5 id="policy">Policy:<a class="headerlink" href="#policy" title="Permanent link">&para;</a></h5>
<p>A <strong>policy</strong> is a strategy or set of rules that defines the actions the agent should take in a given state.</p>
<ul>
<li>The policy can be deterministic (one action for a state) or stochastic (probabilistic actions for a state).</li>
<li>The <strong>goal</strong> is to find an optimal policy that maximizes the total expected reward.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>A robot navigating a maze may follow a policy that says, "Always turn left unless there's an obstacle, then turn right."</li>
</ul>
<h6 id="human-analogy">Human Analogy:<a class="headerlink" href="#human-analogy" title="Permanent link">&para;</a></h6>
<ul>
<li>A <strong>policy</strong> is like a person's <strong>habit or plan of action</strong>, such as the decision to exercise every morning or take an umbrella when it's cloudy.</li>
</ul>
<h4 id="value-function">Value function:<a class="headerlink" href="#value-function" title="Permanent link">&para;</a></h4>
<p>Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. 
- Rewards determine the <strong>immediate, intrinsic desirability</strong> of environmental states. 
- Values indicate the <strong>long-term desirability</strong> of states after considering the states likely to follow and the rewards available in those states. 
- <strong>Example</strong>: - A state might always yield a low immediate reward but still have a <strong>high value</strong> because it is followed by states that yield high rewards. </p>
<h4 id="human-analog">Human Analog<a class="headerlink" href="#human-analog" title="Permanent link">&para;</a></h4>
<ul>
<li>Rewards are somewhat like kepleasureur (if high) and ndpainai (if low). - Values correspond to a more rerefined and farsighted judgmenten of how pleased or displeased we are by the environment.nt.</li>
</ul>
<h5 id="reward-function">Reward Function:<a class="headerlink" href="#reward-function" title="Permanent link">&para;</a></h5>
<p>The <strong>reward function</strong> provides feedback on the actions the agent takes, indicating whether an action was good or bad.</p>
<ul>
<li>It assigns a <strong>numeric value</strong> to the agent's actions, which the agent uses to evaluate the desirability of its actions in a given state.</li>
<li>The goal of the agent is to maximize the cumulative reward over time.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>In a game, winning a round might give a reward of +10, while losing gives a reward of -1.</li>
</ul>
<h6 id="human-analogy_1">Human Analogy:<a class="headerlink" href="#human-analogy_1" title="Permanent link">&para;</a></h6>
<ul>
<li>The <strong>reward function</strong> is like the <strong>feedback</strong> a person gets from their actions, such as feeling <strong>happy</strong> after a good deed or <strong>guilty</strong> after a mistake.</li>
</ul>
<h5 id="model-of-the-environment_1">Model of the Environment:<a class="headerlink" href="#model-of-the-environment_1" title="Permanent link">&para;</a></h5>
<p>The <strong>model of the environment</strong> simulates how the environment behaves, helping the agent predict the outcomes of actions.</p>
<ul>
<li>This model can be used for <strong>planning</strong> future actions by simulating potential outcomes.</li>
<li>A model-free approach directly learns from experience, while a model-based approach uses a model to predict actions' results before performing them.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>A self-driving car may use a model to simulate various driving scenarios and plan its route accordingly.</li>
</ul>
<h5 id="human-analogy_2">Human Analogy:<a class="headerlink" href="#human-analogy_2" title="Permanent link">&para;</a></h5>
<ul>
<li>The <strong>model of the environment</strong> is like a <strong>mental map</strong> that a person forms, which helps them predict the likely outcomes of their actions, such as deciding to avoid a route with heavy traffic.</li>
</ul>
<hr />
<h2 id="final-element-of-rl-systems-model-of-the-environment">Final Element of RL Systems: <strong>Model of the Environment</strong><a class="headerlink" href="#final-element-of-rl-systems-model-of-the-environment" title="Permanent link">&para;</a></h2>
<ul>
<li>The model <strong>mimics the behavior of the environment</strong>, allowing inferences to be made about how the environment will behave.</li>
<li>For example, given a state and action, the model might predict the resultant next state and next reward.</li>
<li>
<p><strong>Models</strong> are used for <strong>planning</strong>, helping the agent decide on a course of action by considering future situations before they are experienced.</p>
</li>
<li>
<p>Methods for solving RL problems that use models and planning are called <strong>model-based methods</strong>, while simpler trial-and-error learning methods are called <strong>model-free methods</strong>.</p>
</li>
</ul>
<hr />
<h1 id="types-of-reinforcement-learning">Types of Reinforcement Learning<a class="headerlink" href="#types-of-reinforcement-learning" title="Permanent link">&para;</a></h1>
<p>There are three main types of Reinforcement Learning (RL):
- <strong>Value-Based</strong>
- <strong>Policy-Based</strong>
- <strong>Model-Based</strong></p>
<p>Each approach has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem you are trying to solve.</p>
<hr />
<h2 id="value-based-reinforcement-learning">Value-Based Reinforcement Learning<a class="headerlink" href="#value-based-reinforcement-learning" title="Permanent link">&para;</a></h2>
<ul>
<li>In this approach, the agent learns to estimate the value of each state or action based on the rewards it receives.</li>
<li>This value is known as <strong>Q-values</strong>.</li>
<li>The agent then selects the actions with the highest Q-value in each state to maximize its long-term reward.</li>
<li>The most commonly used algorithm for value-based reinforcement learning is <strong>Q-learning</strong>.</li>
</ul>
<hr />
<h2 id="policy-based-reinforcement-learning">Policy-Based Reinforcement Learning<a class="headerlink" href="#policy-based-reinforcement-learning" title="Permanent link">&para;</a></h2>
<ul>
<li>In this approach, the agent learns an optimal policy, which is a mapping from states to actions, without calculating the value function.</li>
<li>The policy is updated based on the rewards received by the agent, with the goal of maximizing the expected reward over time.</li>
<li>The most common algorithm used for policy-based reinforcement learning is the <strong>REINFORCE</strong> algorithm.</li>
</ul>
<hr />
<h2 id="model-based-reinforcement-learning_1">Model-Based Reinforcement Learning<a class="headerlink" href="#model-based-reinforcement-learning_1" title="Permanent link">&para;</a></h2>
<ul>
<li>In this approach, the agent learns a model of the environment, which it can use to simulate different scenarios and plan its actions accordingly.</li>
<li>The model can learn through supervised or unsupervised learning, and the agent can use it to predict the outcome of its actions before taking them.</li>
<li>The most common model-based reinforcement learning algorithm is the <strong>Dyna</strong> algorithm.</li>
</ul>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../js/extras.js"></script>
      
    
  </body>
</html>