## Unit 1 


### Introduction  
- **Image Processing**: Image in ‚Üí Image out.  
- **Computer Vision**: Image/Video in ‚Üí Interpretation out.  
- **Objective**: Enable computers to see and interpret objects like humans.  

### Human vs. Computer Vision  
- **Humans**: Quick recognition, intelligent decision-making.  
- **Computers**: Require complex processing, pattern recognition, and AI.  

### Features of Human Vision  
- **Stereo Vision**: Two eyes perceive depth using different distances.  
- **Texture & Color**: Identifies objects based on patterns and colors.  
- **Object Recognition**: Recognizes objects despite illumination, viewpoint, or expression changes.  
- **Context Awareness**: Infers key information from a scene.  

### Limitations of Human Vision  
- **Memory Constraints**: Limited ability to recall vast image data.  
- **Restricted Spectrum**: Limited to visible light.  
- **Optical Illusions**: Misinterpretations of size, shape, and perspective.  

### Computer Vision vs. Image Processing  
- **Image Processing**: Low-level operations (compression, restoration, enhancement).  
- **Computer Vision**: High-level understanding (object recognition, scene interpretation).  

### Applications of Computer Vision  
- **Self-driving Cars**: Navigation and obstacle detection.  
- **Facial Recognition**: Identity verification.  
- **Augmented Reality**: Merging virtual objects with reality.  
- **Medical Imaging**: Detecting anomalies in X-rays and MRIs.  
- **Manufacturing**: Detecting defective products.  
- **Retail & Banking**: OCR, fraud detection, and identity verification.

## **üìú Levels of processing for computer Vision**  

## **Computer Vision Processing Levels**  

### **Low-Level Processing**  
- **Enhances image quality** and extracts **basic features** like **edges, corners, textures**.  
- Works on **raw pixel data** without understanding objects.  
- **Steps for low level processing**:  
  - **Image Preprocessing**: Contrast enhancement, noise reduction.  
  - **Feature Extraction**: **Edge detection** (Canny, Sobel filters).  
  - **Segmentation**: Divides an image into meaningful parts.  
  - **Super-Resolution**: Converts **low-resolution** to **high-resolution** images.  

!!! Important
    Low-level processing extracts fundamental features such as edges, lines, corners, and salient points from an image. These features serve as the building blocks for higher-level processing, enabling pattern recognition, object detection, and scene understanding.

How Low-Level Processing Helps:

- ‚úÖ Feature Extraction: Identifies edges, textures, and key points in an image.
- ‚úÖ Segmentation: Divides images into meaningful regions and shapes.
- ‚úÖ Noise Reduction: Enhances image clarity by removing unwanted distortions.
- ‚úÖ Super-Resolution: Generates higher-resolution images from fewer pixels.
- ‚úÖ Depth Perception: Uses stereo vision (left & right cameras) to create disparity maps for estimating depth.

---

### **Mid-Level Processing**  
- Uses **patterns and object features** for recognition.  
- Works on **segmented objects** to **classify, track, and detect motion**.  
- **Examples**:  
  - **Object Detection**: Recognizes and labels objects.  
  - **Image Classification**: Assigns categories to images.  
  - **Image Matching & Stitching**: Used in **panoramas**.  
  - **Object Tracking**: **Predicts movement** using **optical flow**.  
  - **Clustering (K-Means)**: Segments objects into groups.  


---

### **High-Level Processing**  
- Involves **complex scene understanding** and **AI-based interpretations**.  
- Integrates **low-level features** for **semantic meaning**.  
- **Examples**:  
  - **Semantic Segmentation**: Classifies pixels into categories (e.g., road, car, tree).  
  - **Instance Segmentation**: Differentiates between multiple **objects of the same type**.  
  - **Panoptic Segmentation**: Combines **semantic + instance segmentation**.  
  - **Deep Learning for Segmentation**: Uses **CNNs & RNNs** for precise object recognition.  
  - **Theme Understanding**: Identifies scene context (e.g., **marketplace vs. garden**).  
  - **Visual Question Answering (VQA)**: AI interprets **images + text-based queries**.  

---

### **Applications of Computer Vision**  
- **Self-Driving Cars**: Detects lanes, pedestrians, and traffic signs.  
- **Facial Recognition**: Matches faces for identity verification.  
- **Medical Imaging**: Detects diseases in X-rays & MRIs.  
- **Manufacturing**: Identifies **defective products**.  
- **Retail & Banking**: Uses **OCR, fraud detection, and customer authentication**.


### Deep Learning for Image Segmentation
üîπ Convolutional Neural Networks (CNNs) play a crucial role in image segmentation, enabling computers to understand objects at a pixel level.

How Deep Learning Works for Image Segmentation:

- 1Ô∏è‚É£ Feature Extraction with CNNs: CNNs process images and detect key features for segmentation.
- 2Ô∏è‚É£ Object Localization with Region Proposal Network (RPN): Identifies potential object locations and generates bounding boxes.
- 3Ô∏è‚É£ Refining Features from the Region of Interest (ROI): CNN extracts features within the bounding box.
- 4Ô∏è‚É£ Instance Segmentation using Fully Convolutional Networks (FCN): The extracted features are passed into an FCN, which learns to segment the object from its surroundings.
- 5Ô∏è‚É£ Binary Mask Output: The FCN produces a binary mask, highlighting which pixels belong to the object and which do not.


üöÄ From raw images to object masks, deep learning models enhance precision in image segmentation by combining feature extraction, region detection, and pixel-level classification! üöÄ

### **Difference Between Low-Level and High-Level Features**

|**Feature Type**|**Low-Level Features** üñºÔ∏è|**High-Level Features** üß†|
|---|---|---|
|**Content**|Deals with **raw pixel data**, detecting **edges, textures, and simple patterns**.|Focuses on **object understanding, classification, and relationships**.|
|**Sensitivity**|More **sensitive to noise**, lighting, and orientation changes.|More **robust**, can interpret **complex scenes** despite variations.|
|**Scale**|Extracted from **local regions** (e.g., edges in a small part of the image).|**Global features**‚Äîconsiders the entire image for **context**.|
|**Resources**|Requires **fewer system resources**, as it focuses on simple features.|Uses **advanced AI models**, requiring **higher computation power**.|
|**Use Cases**|Applied in **image segmentation, feature matching, and object detection**.|Used in **image classification, object recognition, and scene understanding**.|

üîç **Low-level features** lay the foundation by detecting **shapes and patterns**, while **high-level features** bring **context and meaning** to an image. Together, they power modern **computer vision applications**! üöÄ



### Edge Detection

### **Sharpening Spatial Filters**  
üñºÔ∏è **Purpose**:  
- Removes **blurring** and **enhances edges** in images.  
- Highlights **intensity transitions** using **spatial differentiation**.  
- **Image gradients** measure the **rate of change in pixel intensity**, crucial for detecting edges.  

---

### **Image Gradients**  
üìå **Fundamental for computer vision & image processing**  
üîπ Used for:  
‚úÖ **Edge detection**  
‚úÖ **Finding object contours**  
‚úÖ **Outlining shapes**  

üîπ Computes:  
- **Gradient Magnitude** (Strength of the edge)  
- **Gradient Orientation** (Direction of the edge)  

‚ú® **Popular techniques built on image gradients**:  
- **Histogram of Oriented Gradients (HOG)**  
- **Scale-Invariant Feature Transform (SIFT)**  

---

### **Edge Detection Using Image Gradients**  
üí° **Gradient computation is a key pre-processing step** for edge detection.  

#### **Computing Image Gradients**  
The **gradient of an image** is calculated using finite differences:  
- **Gradient along the vertical direction ($G_y$):**  
  $$ G_y = f(r+1, c) - f(r-1, c) $$  
- **Gradient along the horizontal direction ($G_x$):**  
  $$ G_x = f(r, c+1) - f(r, c-1) $$  

üìå **Gradient masks (filters) for edge detection:**  

| **Filter** | **Mask** |
|------------|-------------|
| **Vertical ($G_y$) Sobel Filter** | \(\begin{bmatrix} -1 & 0 & 1 \end{bmatrix}\) |
| **Horizontal ($G_x$) Sobel Filter** | \(\begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}\) |


### **Image Gradient & Gradient Vector**  
üñºÔ∏è **Purpose**:  
- Computes **rate of change** in pixel intensity.  
- **Gradient Magnitude ($M$)** measures the **strength** of intensity change.  
- **Gradient Orientation ($\alpha$)** determines **direction** of the edge.  
- The **matrix size** for magnitude and angle is the **same** as the original image.  

üîπ **Mathematical Representation:**  
- **Gradient Angle ($\alpha$):**  
  $$ \alpha = \tan^{-1} \left(\frac{g_y}{g_x} \right) $$  
- **Gradient Magnitude ($M$):**  
  $$ M = \sqrt{g_x^2 + g_y^2} $$  

üìå **Where:**  
- $g_x$ = Gradient in the **horizontal direction**.  
- $g_y$ = Gradient in the **vertical direction**.  
- $\alpha$ = Angle between the **vertical axis** and the **edge direction**.  

---

### **Sobel Filter for Edge Detection**  
üí° **Sobel filters** compute **image gradients** using convolution masks.  

#### **Gradient Computation with Sobel Operator**  
üîπ **Sobel Operator for Horizontal ($G_x$) and Vertical ($G_y$) Gradients:**  

| **Gradient Direction** | **Filter Mask (Kernel)** |
|----------------|-----------------------------------|
| **$G_x$ (Horizontal Gradient)** | $\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$ |
| **$G_y$ (Vertical Gradient)** | $\begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}$ |

üîπ **Steps to Compute Edge Strength:**  
1Ô∏è‚É£ Convolve the **image** with **$G_x$** to compute **horizontal changes**.  
2Ô∏è‚É£ Convolve the **image** with **$G_y$** to compute **vertical changes**.  
3Ô∏è‚É£ Compute **gradient magnitude** and **angle** using:  
   $$ M = \sqrt{G_x^2 + G_y^2} $$  
   $$ \alpha = \tan^{-1} \left(\frac{G_y}{G_x} \right) $$  

üìå **Why Sobel Filters?**  
‚úÖ Enhances **edges** by detecting **gradients** in both directions.  
‚úÖ **Smooths noise** while emphasizing high-frequency intensity changes.  
‚úÖ Used in **edge detection algorithms** like **Canny Edge Detector**.  

### **Gaussian Filter for Image Smoothing**  

üîπ The **Gaussian filter** is a **smoothing filter** that reduces **noise and detail** in an image.  
üîπ It applies a **Gaussian function** to weight pixels, giving higher importance to the center pixel and gradually reducing weights outward.  

---

### **Mathematical Representation**  
The **Gaussian function** for a 2D image is:  
$$ G_\sigma(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}} $$
where:  
- \( \sigma \) = Standard deviation (controls blurring strength)  
- \( x, y \) = Pixel coordinates  

üìå **Higher \( \sigma \) ‚Üí More blur**  
üìå **Lower \( \sigma \) ‚Üí Less blur, preserves more details**  

---

### **Gaussian Filter Kernels**  

| **Filter Size** | **Kernel Matrix (Normalized)** |
|---------------|----------------------------|
| **3√ó3 (œÉ = 1)** | \( \frac{1}{16} \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \) |
| **5√ó5 (œÉ = 1)** | \( \frac{1}{330} \begin{bmatrix} 1 & 4 & 7 & 4 & 1 \\ 4 & 20 & 33 & 20 & 4 \\ 7 & 33 & 54 & 33 & 7 \\ 4 & 20 & 33 & 20 & 4 \\ 1 & 4 & 7 & 4 & 1 \end{bmatrix} \) |
| **5√ó5 (œÉ = 2)** | \( \frac{1}{34} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \) |

üìå **Larger filter size ‚Üí More blur & smoothing**  
üìå **Smaller filter size ‚Üí Preserves more details**  

---

### **Laplacian Filter for Edge Detection**  

üîπ The **Laplacian filter** is a **second-order derivative filter** used for **edge detection**.  
üîπ It highlights regions of **rapid intensity change** by computing the **second derivative** of an image.  

#### **Mathematical Representation**  
The Laplacian operator is given by:  

\[ \nabla^2 f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\]  

where \( f(x, y) \) is the image intensity at a given point.

---

### **Laplacian Filter Kernels**  

| **Filter Type** | **Kernel Matrix** |
|----------------|------------------|
| **4-Neighbor Laplacian** | \( \begin{bmatrix} 0 & -1 & 0 \\ -1 & 4 & -1 \\ 0 & -1 & 0 \end{bmatrix} \) |
| **8-Neighbor Laplacian** | \( \begin{bmatrix} -1 & -1 & -1 \\ -1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix} \) |

üîπ The **4-neighbor Laplacian** considers only direct neighbors, while the **8-neighbor Laplacian** accounts for diagonal edges as well.

---

## Unit 2

### **Image Features in Computer Vision**  

üîπ **Image features** are key elements that help in **object recognition, segmentation, and analysis**.  

#### **Types of Image Features**  
‚úÖ **Edges** ‚Äì Identifies boundaries between objects.  
‚úÖ **Color** ‚Äì Extracts information based on pixel intensity.  
‚úÖ **Texture** ‚Äì Analyzes surface patterns and structures.  
‚úÖ **Object Boundaries** ‚Äì Detects outlines and contours of objects.  
‚úÖ **Object Shape** ‚Äì Defines geometric properties of an object.  

üîπ **Good Features Should Be:**  
- ‚úÖ **Unique & Distinctive** ‚Äì Helps differentiate between objects.  
- ‚úÖ **Non-redundant** ‚Äì Avoids duplicate or unnecessary information.  
- ‚úÖ **Robust** ‚Äì Works well under noise and illumination changes.  
- ‚úÖ **Global Representation** ‚Äì Captures scene-wide characteristics, not just local details.  

---

### **Gradient-Based Features**  
Gradient-based techniques detect **changes in pixel intensity**, which highlight object edges and textures.  

üîπ **Popular Techniques:**  
- **DoG (Difference of Gaussian)**  
- **LoG (Laplacian of Gaussian)**  
- **HoG (Histogram of Oriented Gradients)**  
- **SIFT (Scale-Invariant Feature Transform)**  
- **SURF (Speeded-Up Robust Features)**  

üìå **Advantages:**  
‚úÖ **Invariant to small shifts & rotations** ‚Äì Ensures stability under transformations.  
‚úÖ **Localized histograms** ‚Äì Offers better spatial information compared to global histograms.  
‚úÖ **Contrast normalization** ‚Äì Reduces the impact of variable illumination.  

---

### **Difference of Gaussian (DoG)**  
üìå **A feature enhancement technique used for blob detection & SIFT descriptors**.  

#### **How DoG Works:**  
1Ô∏è‚É£ **Apply Gaussian Blur** ‚Äì Smoothens the image using **two Gaussian filters** with different sigma values (**œÉ‚ÇÅ & œÉ‚ÇÇ**).  
2Ô∏è‚É£ **Subtract the Two Blurred Images** ‚Äì Enhances regions with specific frequency details.  
3Ô∏è‚É£ **Suppress High-Frequency Details** ‚Äì Reduces random noise but preserves important structures.  

üîπ **Mathematical Representation:**  
$$ DoG = G_{\sigma_1} * I - G_{\sigma_2} * I $$

where:  
- \( I \) = Original grayscale image  
- \( G_{\sigma_1}, G_{\sigma_2} \) = Gaussian filters with different standard deviations  

üìå **Pros & Cons:**  
‚úÖ **Reduces noise while preserving edges**  
‚úÖ **Enhances spatial features**  
‚ùå **Reduces overall image contrast**  


### **Laplacian of Gaussian (LoG) ‚Äì Edge Detection & Feature Enhancement**  

üîπ **Laplacian of Gaussian (LoG)** is a feature detection technique that combines:  
1Ô∏è‚É£ **Gaussian Smoothing** ‚Äì Reduces noise in the image.  
2Ô∏è‚É£ **Laplacian Operator** ‚Äì Detects **edges and blobs** by identifying intensity changes.  

#### **How LoG Works:**  
1. **Apply a Gaussian filter** to smooth the image and suppress noise.  
2. **Compute the second derivative (Laplacian)** to highlight regions with rapid intensity changes (edges).  
3. **Detect zero-crossings** in the Laplacian response to identify edges.  

#### **Mathematical Representation:**  
The **LoG function** is given by:  
$$ LoG(x, y) = \nabla^2 G_{\sigma} (x, y) * I(x, y) $$  
where:  
- \( G_{\sigma} (x, y) \) = **Gaussian filter** with standard deviation \( \sigma \)  
- \( \nabla^2 \) = **Laplacian operator** (second derivative)  
- \( I(x, y) \) = **Input image**  

#### **Key Features of LoG:**  
‚úÖ **Combines smoothing & edge detection** in one step.  
‚úÖ **Detects both fine and coarse details** depending on \( \sigma \).  
‚úÖ **Useful for blob detection** in feature descriptors like **SIFT**.  
‚ùå **Sensitive to noise** ‚Äì Requires pre-smoothing for better results.  

üöÄ **LoG is commonly used in edge detection pipelines like the Marr-Hildreth operator and as a preprocessing step in Computer Vision applications!** üîç


### **Histogram of Oriented Gradients (HoG) ‚Äì Feature Descriptor for Object Detection**  

üîπ **Histogram of Oriented Gradients (HoG)** is a feature descriptor used for **object detection** and **image classification** by analyzing **gradient orientations** in localized regions of an image.  


### **Step-by-Step HoG Computation**  

‚úÖ **Step 1: Resize Image**  
- Resize the image to an **integer multiple of 8** (nearest to the original size).  
- Ensures uniform cell division and efficient computation.  

‚úÖ **Step 2: Divide Image into Cells**  
- Split the image into **small patches of equal size** (e.g., **8√ó8 pixels per cell**).  
- Each cell will have its own **gradient histogram**.  

‚úÖ **Step 3: Compute Gradients**  
- Calculate the **gradient magnitude** and **orientation** using **Sobel filters**:  
  \[
  M = \sqrt{G_x^2 + G_y^2}, \quad \theta = \tan^{-1} \left(\frac{G_y}{G_x} \right)
  \]  
  where \( G_x, G_y \) are gradients along horizontal and vertical directions.  

‚úÖ **Step 4: Compute Gradient Histograms (Per Cell)**  
- For **each 8√ó8 cell**, create a **histogram of gradients** (e.g., 9 bins for 0¬∞-180¬∞).  
- Assign gradient magnitudes to their corresponding **orientation bins**.  

‚úÖ **Step 5: Construct Feature Vector**  
- Normalize the histograms **across neighboring blocks** (e.g., **2√ó2 cells per block**) for better illumination invariance.  
- Flatten the computed HoG features into a **single feature vector** for classification.  

‚úÖ **Step 6: Visualize HoG**  
- HoG features are often **visualized as a grid of arrows**, where the **length and direction** represent gradient strength and orientation.  

‚úÖ **Step 7: Classify Images**  
- Use machine learning models (**SVM, Deep Learning**) to classify objects using the extracted **HoG feature vector**.  


#### **Mathematical Representation:**  
- **Gradient Magnitude ($M$):**  
$$   M = \sqrt{G_x^2 + G_y^2} $$ 

- **Gradient Orientation ($\theta$):**  
$$   \theta = \tan^{-1} \left(\frac{G_y}{G_x} \right) $$   
where:  
- \( G_x, G_y \) = Gradients in horizontal & vertical directions.  
- \( M \) = Strength of edge response.  
- \( \theta \) = Edge direction (0¬∞‚Äì180¬∞ or 0¬∞‚Äì360¬∞ bins).  

#### **Key Features of HoG:**  
‚úÖ **Invariance to Illumination & Shadows** ‚Äì Normalization removes intensity variations.  
‚úÖ **Captures Local Shape Information** ‚Äì Focuses on **edges and textures** rather than pixel intensity.  
‚úÖ **Robust to Small Translations & Rotations** ‚Äì Uses **histograms** instead of raw gradients.  
‚úÖ **Widely Used in Object Detection** ‚Äì Forms the basis of **Dalal-Triggs pedestrian detection** and is used in **SVM-based image recognition**.  
‚ùå **Computationally Expensive** ‚Äì Requires **dense gradient computations** across the entire image.  

üöÄ **HoG is widely used in Human & Object Detection (e.g., Pedestrian Detection in self-driving cars) and Machine Learning-based Image Classification!** üîç



### **Feature Descriptors in Computer Vision**  

üîπ **Feature descriptors** help identify **key points, edges, and corners** in an image.  
üîπ These descriptors are used for **object detection, image matching, and recognition**.  

---

### **Types of Feature Descriptors**  

#### **1. Global Descriptors üåç**  
- Represent the **entire image**.  
- Examples:  
  ‚úÖ **Histogram of Oriented Gradients (HoG)**  
  ‚úÖ **Difference of Gaussian (DoG)**  
  ‚úÖ **Histogram of Optical Flow (HOF)**  
- **Limitations**: Struggle with **occlusions and profile variations** since they analyze the **whole image**.  

#### **2. Local Descriptors üîç**  
- Describe **small patches** within an image.  
- More **accurate & robust** for **object detection, matching, and occlusion handling**.  
- Examples:  
  ‚úÖ **SIFT (Scale-Invariant Feature Transform)**  
  ‚úÖ **SURF (Speeded-Up Robust Features)**  
  ‚úÖ **LBP (Local Binary Pattern)**  
  ‚úÖ **BRISK (Binary Robust Invariant Scalable Keypoints)**  
  ‚úÖ **MSER (Maximally Stable Extremal Regions)**  
  ‚úÖ **FREAK (Fast Retina Keypoint)**  

üìå **Local descriptors outperform global ones in real-world applications like facial recognition and object tracking!** üöÄ  

---

### **How to Define an Interest Point?**  

üîπ **Interest points** are key locations (e.g., edges, corners) where **features can be extracted**.  

‚úÖ **Repeatability**:  
- A feature should be detected **consistently across multiple images**, despite **geometric & photometric transformations**.  

‚úÖ **Saliency**:  
- Features should be **distinct and unique** to avoid mismatches.  

‚úÖ **Compactness**:  
- Fewer features than the number of image pixels should **effectively represent the image**.  

‚úÖ **Efficiency**:  
- **Fast computation** is essential for **real-time applications** like tracking & detection.  

‚úÖ **Locality**:  
- Features should **occupy a small area** and remain **robust to clutter & occlusion**.  

‚úÖ **Covariance**:  
- Features should be **detectable despite geometric & photometric variations** (e.g., rotation, lighting changes).  

### **SIFT Algorithm ‚Äì Scale-Invariant Feature Transform**  

SIFT is a **feature detection** algorithm that extracts **scale and rotation-invariant keypoints** for object recognition, tracking, and image matching.  

---

## **Step 1: Construct a Scale Space**  

üìå **Why?**  
- Real-world objects appear different at **various distances (scales)**.  
- A feature must be **detectable at multiple scales** to be useful in recognition.  

üîπ **How it Works:**  
1. The **original image** is repeatedly **blurred using a Gaussian filter**.  
2. **Octaves** are created by **downsampling** the image (reducing its size by half).  
3. Within each octave, multiple blurred images are generated with increasing **sigma values (œÉ)**.  
4. This **scale-space representation** ensures features are **scale-independent**.  

üîπ **Mathematical Formulation (Gaussian Blur):**  
\[
G(x, y, \sigma) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}
\]  
where:  
- \( G(x, y, \sigma) \) = Gaussian function.  
- \( \sigma \) = Standard deviation (controls blurring).  
- \( x, y \) = Pixel coordinates.  

üîπ **Example:**  
- **Octave 1**: Original image + multiple blurred versions.  
- **Octave 2**: Image resized to **half** and blurred again.  
- **Repeats** for multiple octaves (typically **4-5** octaves).  

üìå **Outcome:**  
- A collection of images at **different scales and resolutions**.  

---

## **Step 2: Compute Difference of Gaussian (DoG)**  

üìå **Why?**  
- Identifies keypoints by enhancing **edges and texture features**.  
- The **Gaussian Blur removes noise**, and the **DoG highlights changes in intensity**.  

üîπ **How it Works:**  
1. **Compute DoG images** by subtracting two consecutive Gaussian-blurred images:  
   \[
   DoG(x, y, \sigma) = G(x, y, k\sigma) - G(x, y, \sigma)
   \]  
   where \( k \) is a constant (typically \( k = \sqrt{2} \)).  
2. This process is repeated across all octaves.  
3. The resulting **DoG images** enhance edges, blobs, and texture details.  

üìå **Outcome:**  
- A set of **DoG images** that highlight regions of interest (potential keypoints).  

---

## **Step 3: Keypoint Localization**  

üìå **Why?**  
- Identify **stable keypoints** while removing weak or false detections.  

üîπ **How it Works:**  
1. Each pixel in the **DoG images** is compared with **26 neighboring pixels** (8 in the same image, 9 in the scale above, and 9 in the scale below).  
2. If a pixel is the **local maximum or minimum**, it is marked as a **potential keypoint**.  
3. **Low-contrast keypoints** are discarded using a **threshold (typically 0.03)**.  
4. **Edges are removed** using the Hessian matrix determinant to avoid unstable keypoints.  

üîπ **Mathematical Filtering (Hessian Matrix):**  
\[
H = \begin{bmatrix} I_{xx} & I_{xy} \\ I_{xy} & I_{yy} \end{bmatrix}
\]  
- Compute **corner response**:  
  \[
  \frac{(\text{Trace}(H))^2}{\text{Det}(H)} < 12.1
  \]  
  If the value is **greater than 12.1**, the keypoint is rejected.  

üìå **Outcome:**  
- A set of **highly stable, contrast-rich keypoints** that can be used for further processing.  

