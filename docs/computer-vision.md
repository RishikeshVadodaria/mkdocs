## Unit 1 


### Introduction  
- **Image Processing**: Image in â†’ Image out.  
- **Computer Vision**: Image/Video in â†’ Interpretation out.  
- **Objective**: Enable computers to see and interpret objects like humans.  

### Human vs. Computer Vision  
- **Humans**: Quick recognition, intelligent decision-making.  
- **Computers**: Require complex processing, pattern recognition, and AI.  

### Features of Human Vision  
- **Stereo Vision**: Two eyes perceive depth using different distances.  
- **Texture & Color**: Identifies objects based on patterns and colors.  
- **Object Recognition**: Recognizes objects despite illumination, viewpoint, or expression changes.  
- **Context Awareness**: Infers key information from a scene.  

### Limitations of Human Vision  
- **Memory Constraints**: Limited ability to recall vast image data.  
- **Restricted Spectrum**: Limited to visible light.  
- **Optical Illusions**: Misinterpretations of size, shape, and perspective.  

### Computer Vision vs. Image Processing  
- **Image Processing**: Low-level operations (compression, restoration, enhancement).  
- **Computer Vision**: High-level understanding (object recognition, scene interpretation).  

### Applications of Computer Vision  
- **Self-driving Cars**: Navigation and obstacle detection.  
- **Facial Recognition**: Identity verification.  
- **Augmented Reality**: Merging virtual objects with reality.  
- **Medical Imaging**: Detecting anomalies in X-rays and MRIs.  
- **Manufacturing**: Detecting defective products.  
- **Retail & Banking**: OCR, fraud detection, and identity verification.

## **ğŸ“œ Levels of processing for computer Vision**  

## **Computer Vision Processing Levels**  

### **Low-Level Processing**  
- **Enhances image quality** and extracts **basic features** like **edges, corners, textures**.  
- Works on **raw pixel data** without understanding objects.  
- **Steps for low level processing**:  
  - **Image Preprocessing**: Contrast enhancement, noise reduction.  
  - **Feature Extraction**: **Edge detection** (Canny, Sobel filters).  
  - **Segmentation**: Divides an image into meaningful parts.  
  - **Super-Resolution**: Converts **low-resolution** to **high-resolution** images.  

!!! Important
    Low-level processing extracts fundamental features such as edges, lines, corners, and salient points from an image. These features serve as the building blocks for higher-level processing, enabling pattern recognition, object detection, and scene understanding.

How Low-Level Processing Helps:

- âœ… Feature Extraction: Identifies edges, textures, and key points in an image.
- âœ… Segmentation: Divides images into meaningful regions and shapes.
- âœ… Noise Reduction: Enhances image clarity by removing unwanted distortions.
- âœ… Super-Resolution: Generates higher-resolution images from fewer pixels.
- âœ… Depth Perception: Uses stereo vision (left & right cameras) to create disparity maps for estimating depth.

---

### **Mid-Level Processing**  
- Uses **patterns and object features** for recognition.  
- Works on **segmented objects** to **classify, track, and detect motion**.  
- **Examples**:  
  - **Object Detection**: Recognizes and labels objects.  
  - **Image Classification**: Assigns categories to images.  
  - **Image Matching & Stitching**: Used in **panoramas**.  
  - **Object Tracking**: **Predicts movement** using **optical flow**.  
  - **Clustering (K-Means)**: Segments objects into groups.  


---

### **High-Level Processing**  
- Involves **complex scene understanding** and **AI-based interpretations**.  
- Integrates **low-level features** for **semantic meaning**.  
- **Examples**:  
  - **Semantic Segmentation**: Classifies pixels into categories (e.g., road, car, tree).  
  - **Instance Segmentation**: Differentiates between multiple **objects of the same type**.  
  - **Panoptic Segmentation**: Combines **semantic + instance segmentation**.  
  - **Deep Learning for Segmentation**: Uses **CNNs & RNNs** for precise object recognition.  
  - **Theme Understanding**: Identifies scene context (e.g., **marketplace vs. garden**).  
  - **Visual Question Answering (VQA)**: AI interprets **images + text-based queries**.  

---

### **Applications of Computer Vision**  
- **Self-Driving Cars**: Detects lanes, pedestrians, and traffic signs.  
- **Facial Recognition**: Matches faces for identity verification.  
- **Medical Imaging**: Detects diseases in X-rays & MRIs.  
- **Manufacturing**: Identifies **defective products**.  
- **Retail & Banking**: Uses **OCR, fraud detection, and customer authentication**.


### Deep Learning for Image Segmentation
ğŸ”¹ Convolutional Neural Networks (CNNs) play a crucial role in image segmentation, enabling computers to understand objects at a pixel level.

How Deep Learning Works for Image Segmentation:

- 1ï¸âƒ£ Feature Extraction with CNNs: CNNs process images and detect key features for segmentation.
- 2ï¸âƒ£ Object Localization with Region Proposal Network (RPN): Identifies potential object locations and generates bounding boxes.
- 3ï¸âƒ£ Refining Features from the Region of Interest (ROI): CNN extracts features within the bounding box.
- 4ï¸âƒ£ Instance Segmentation using Fully Convolutional Networks (FCN): The extracted features are passed into an FCN, which learns to segment the object from its surroundings.
- 5ï¸âƒ£ Binary Mask Output: The FCN produces a binary mask, highlighting which pixels belong to the object and which do not.


ğŸš€ From raw images to object masks, deep learning models enhance precision in image segmentation by combining feature extraction, region detection, and pixel-level classification! ğŸš€

### **Difference Between Low-Level and High-Level Features**

|**Feature Type**|**Low-Level Features** ğŸ–¼ï¸|**High-Level Features** ğŸ§ |
|---|---|---|
|**Content**|Deals with **raw pixel data**, detecting **edges, textures, and simple patterns**.|Focuses on **object understanding, classification, and relationships**.|
|**Sensitivity**|More **sensitive to noise**, lighting, and orientation changes.|More **robust**, can interpret **complex scenes** despite variations.|
|**Scale**|Extracted from **local regions** (e.g., edges in a small part of the image).|**Global features**â€”considers the entire image for **context**.|
|**Resources**|Requires **fewer system resources**, as it focuses on simple features.|Uses **advanced AI models**, requiring **higher computation power**.|
|**Use Cases**|Applied in **image segmentation, feature matching, and object detection**.|Used in **image classification, object recognition, and scene understanding**.|

ğŸ” **Low-level features** lay the foundation by detecting **shapes and patterns**, while **high-level features** bring **context and meaning** to an image. Together, they power modern **computer vision applications**! ğŸš€



### Edge Detection

### **Sharpening Spatial Filters**  
ğŸ–¼ï¸ **Purpose**:  
- Removes **blurring** and **enhances edges** in images.  
- Highlights **intensity transitions** using **spatial differentiation**.  
- **Image gradients** measure the **rate of change in pixel intensity**, crucial for detecting edges.  

---

### **Image Gradients**  
ğŸ“Œ **Fundamental for computer vision & image processing**  
ğŸ”¹ Used for:  
âœ… **Edge detection**  
âœ… **Finding object contours**  
âœ… **Outlining shapes**  

ğŸ”¹ Computes:  
- **Gradient Magnitude** (Strength of the edge)  
- **Gradient Orientation** (Direction of the edge)  

âœ¨ **Popular techniques built on image gradients**:  
- **Histogram of Oriented Gradients (HOG)**  
- **Scale-Invariant Feature Transform (SIFT)**  

---

### **Edge Detection Using Image Gradients**  
ğŸ’¡ **Gradient computation is a key pre-processing step** for edge detection.  

#### **Computing Image Gradients**  
The **gradient of an image** is calculated using finite differences:  
- **Gradient along the vertical direction ($G_y$):**  
  $$ G_y = f(r+1, c) - f(r-1, c) $$  
- **Gradient along the horizontal direction ($G_x$):**  
  $$ G_x = f(r, c+1) - f(r, c-1) $$  

ğŸ“Œ **Gradient masks (filters) for edge detection:**  

| **Filter** | **Mask** |
|------------|-------------|
| **Vertical ($G_y$) Sobel Filter** | \(\begin{bmatrix} -1 & 0 & 1 \end{bmatrix}\) |
| **Horizontal ($G_x$) Sobel Filter** | \(\begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}\) |


### **Image Gradient & Gradient Vector**  
ğŸ–¼ï¸ **Purpose**:  
- Computes **rate of change** in pixel intensity.  
- **Gradient Magnitude ($M$)** measures the **strength** of intensity change.  
- **Gradient Orientation ($\alpha$)** determines **direction** of the edge.  
- The **matrix size** for magnitude and angle is the **same** as the original image.  

ğŸ”¹ **Mathematical Representation:**  
- **Gradient Angle ($\alpha$):**  
  $$ \alpha = \tan^{-1} \left(\frac{g_y}{g_x} \right) $$  
- **Gradient Magnitude ($M$):**  
  $$ M = \sqrt{g_x^2 + g_y^2} $$  

ğŸ“Œ **Where:**  
- $g_x$ = Gradient in the **horizontal direction**.  
- $g_y$ = Gradient in the **vertical direction**.  
- $\alpha$ = Angle between the **vertical axis** and the **edge direction**.  

---

### **Sobel Filter for Edge Detection**  
ğŸ’¡ **Sobel filters** compute **image gradients** using convolution masks.  

#### **Gradient Computation with Sobel Operator**  
ğŸ”¹ **Sobel Operator for Horizontal ($G_x$) and Vertical ($G_y$) Gradients:**  

| **Gradient Direction** | **Filter Mask (Kernel)** |
|----------------|-----------------------------------|
| **$G_x$ (Horizontal Gradient)** | $\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$ |
| **$G_y$ (Vertical Gradient)** | $\begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}$ |

ğŸ”¹ **Steps to Compute Edge Strength:**  
1ï¸âƒ£ Convolve the **image** with **$G_x$** to compute **horizontal changes**.  
2ï¸âƒ£ Convolve the **image** with **$G_y$** to compute **vertical changes**.  
3ï¸âƒ£ Compute **gradient magnitude** and **angle** using:  
   $$ M = \sqrt{G_x^2 + G_y^2} $$  
   $$ \alpha = \tan^{-1} \left(\frac{G_y}{G_x} \right) $$  

ğŸ“Œ **Why Sobel Filters?**  
âœ… Enhances **edges** by detecting **gradients** in both directions.  
âœ… **Smooths noise** while emphasizing high-frequency intensity changes.  
âœ… Used in **edge detection algorithms** like **Canny Edge Detector**.  

### **Gaussian Filter for Image Smoothing**  

ğŸ”¹ The **Gaussian filter** is a **smoothing filter** that reduces **noise and detail** in an image.  
ğŸ”¹ It applies a **Gaussian function** to weight pixels, giving higher importance to the center pixel and gradually reducing weights outward.  

---

### **Mathematical Representation**  
The **Gaussian function** for a 2D image is:  
\[
G_\sigma(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}
\]  
where:  
- \( \sigma \) = Standard deviation (controls blurring strength)  
- \( x, y \) = Pixel coordinates  

ğŸ“Œ **Higher \( \sigma \) â†’ More blur**  
ğŸ“Œ **Lower \( \sigma \) â†’ Less blur, preserves more details**  

---

### **Gaussian Filter Kernels**  

| **Filter Size** | **Kernel Matrix (Normalized)** |
|---------------|----------------------------|
| **3Ã—3 (Ïƒ = 1)** | \( \frac{1}{16} \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \) |
| **5Ã—5 (Ïƒ = 1)** | \( \frac{1}{330} \begin{bmatrix} 1 & 4 & 7 & 4 & 1 \\ 4 & 20 & 33 & 20 & 4 \\ 7 & 33 & 54 & 33 & 7 \\ 4 & 20 & 33 & 20 & 4 \\ 1 & 4 & 7 & 4 & 1 \end{bmatrix} \) |
| **5Ã—5 (Ïƒ = 2)** | \( \frac{1}{34} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \) |

ğŸ“Œ **Larger filter size â†’ More blur & smoothing**  
ğŸ“Œ **Smaller filter size â†’ Preserves more details**  

---

### **Laplacian Filter for Edge Detection**  

ğŸ”¹ The **Laplacian filter** is a **second-order derivative filter** used for **edge detection**.  
ğŸ”¹ It highlights regions of **rapid intensity change** by computing the **second derivative** of an image.  

#### **Mathematical Representation**  
The Laplacian operator is given by:  

\[ \nabla^2 f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\]  

where \( f(x, y) \) is the image intensity at a given point.

---

### **Laplacian Filter Kernels**  

| **Filter Type** | **Kernel Matrix** |
|----------------|------------------|
| **4-Neighbor Laplacian** | \( \begin{bmatrix} 0 & -1 & 0 \\ -1 & 4 & -1 \\ 0 & -1 & 0 \end{bmatrix} \) |
| **8-Neighbor Laplacian** | \( \begin{bmatrix} -1 & -1 & -1 \\ -1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix} \) |

ğŸ”¹ The **4-neighbor Laplacian** considers only direct neighbors, while the **8-neighbor Laplacian** accounts for diagonal edges as well.

---

## Unit 2

### **Image Features in Computer Vision**  

ğŸ”¹ **Image features** are key elements that help in **object recognition, segmentation, and analysis**.  

#### **Types of Image Features**  
âœ… **Edges** â€“ Identifies boundaries between objects.  
âœ… **Color** â€“ Extracts information based on pixel intensity.  
âœ… **Texture** â€“ Analyzes surface patterns and structures.  
âœ… **Object Boundaries** â€“ Detects outlines and contours of objects.  
âœ… **Object Shape** â€“ Defines geometric properties of an object.  

ğŸ”¹ **Good Features Should Be:**  
- âœ… **Unique & Distinctive** â€“ Helps differentiate between objects.  
- âœ… **Non-redundant** â€“ Avoids duplicate or unnecessary information.  
- âœ… **Robust** â€“ Works well under noise and illumination changes.  
- âœ… **Global Representation** â€“ Captures scene-wide characteristics, not just local details.  

---

### **Gradient-Based Features**  
Gradient-based techniques detect **changes in pixel intensity**, which highlight object edges and textures.  

ğŸ”¹ **Popular Techniques:**  
- **DoG (Difference of Gaussian)**  
- **LoG (Laplacian of Gaussian)**  
- **HoG (Histogram of Oriented Gradients)**  
- **SIFT (Scale-Invariant Feature Transform)**  
- **SURF (Speeded-Up Robust Features)**  

ğŸ“Œ **Advantages:**  
âœ… **Invariant to small shifts & rotations** â€“ Ensures stability under transformations.  
âœ… **Localized histograms** â€“ Offers better spatial information compared to global histograms.  
âœ… **Contrast normalization** â€“ Reduces the impact of variable illumination.  

---

### **Difference of Gaussian (DoG)**  
ğŸ“Œ **A feature enhancement technique used for blob detection & SIFT descriptors**.  

#### **How DoG Works:**  
1ï¸âƒ£ **Apply Gaussian Blur** â€“ Smoothens the image using **two Gaussian filters** with different sigma values (**Ïƒâ‚ & Ïƒâ‚‚**).  
2ï¸âƒ£ **Subtract the Two Blurred Images** â€“ Enhances regions with specific frequency details.  
3ï¸âƒ£ **Suppress High-Frequency Details** â€“ Reduces random noise but preserves important structures.  

ğŸ”¹ **Mathematical Representation:**  
$$ DoG = G_{\sigma_1} * I - G_{\sigma_2} * I $$

where:  
- \( I \) = Original grayscale image  
- \( G_{\sigma_1}, G_{\sigma_2} \) = Gaussian filters with different standard deviations  

ğŸ“Œ **Pros & Cons:**  
âœ… **Reduces noise while preserving edges**  
âœ… **Enhances spatial features**  
âŒ **Reduces overall image contrast**  

