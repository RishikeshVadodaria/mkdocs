<div class="nav-wrapper">
    <a href="https://rishikeshvadodaria.github.io/mkdocs/computer-vision-unit 1/" class="nav-item mobile-computing" data-title="Unit 1">Unit 1</a>
    <a href="https://rishikeshvadodaria.github.io/mkdocs/computer-vision-unit 2/" class="nav-item mobile-computing" data-title="Unit 2">Unit 2</a>
    <a href="https://rishikeshvadodaria.github.io/mkdocs/computer-vision-unit 3/" class="nav-item mobile-computing" data-title="Unit 3">Unit 3</a>
    <a href="https://rishikeshvadodaria.github.io/mkdocs/computer-vision-unit 4/" class="nav-item mobile-computing" data-title="Unit 4">Unit 4</a>
   </div>


# Introduction to Computer Vision

## Unit 1
 
## Introduction
- **Image Processing**: Image in â†’ Image out.
- **Computer Vision**: Image/Video in â†’ Interpretation out.
- **Objective**: Enable computers to see and interpret objects like humans.

## Human vs. Computer Vision
- **Humans**: Quick recognition, intelligent decision-making.
- **Computers**: Require complex processing, pattern recognition, and AI.

## Features of Human Vision
- **Stereo Vision**: Two eyes perceive depth using different distances.
- **Texture & Color**: Identifies objects based on patterns and colors.
- **Object Recognition**: Recognizes objects despite illumination, viewpoint, or expression changes.
- **Context Awareness**: Infers key information from a scene.

## Limitations of Human Vision
- **Memory Constraints**: Limited ability to recall vast image data.
- **Restricted Spectrum**: Limited to visible light.
- **Optical Illusions**: Misinterpretations of size, shape, and perspective.

## Computer Vision vs. Image Processing
- **Image Processing**: Low-level operations (compression, restoration, enhancement).
- **Computer Vision**: High-level understanding (object recognition, scene interpretation).

## Applications of Computer Vision
- **Self-driving Cars**: Navigation and obstacle detection.
- **Facial Recognition**: Identity verification.
- **Augmented Reality**: Merging virtual objects with reality.
- **Medical Imaging**: Detecting anomalies in X-rays and MRIs.
- **Manufacturing**: Detecting defective products.
- **Retail & Banking**: OCR, fraud detection, and identity verification.

## Computer Vision Processing Levels

### Low-Level Processing
- **Enhances image quality** and extracts **basic features** like **edges, corners, textures**.
- Works on **raw pixel data** without understanding objects.
- **Steps for low level processing**:
  - **Image Preprocessing**: Contrast enhancement, noise reduction.
  - **Feature Extraction**: **Edge detection** (Canny, Sobel filters).
  - **Segmentation**: Divides an image into meaningful parts.
  - **Super-Resolution**: Converts **low-resolution** to **high-resolution** images.

!!! Important
    Low-level processing extracts fundamental features such as edges, lines, corners, and salient points from an image. These features serve as the building blocks for higher-level processing, enabling pattern recognition, object detection, and scene understanding.

How Low-Level Processing Helps:
- âœ… Feature Extraction: Identifies edges, textures, and key points in an image.
- âœ… Segmentation: Divides images into meaningful regions and shapes.
- âœ… Noise Reduction: Enhances image clarity by removing unwanted distortions.
- âœ… Super-Resolution: Generates higher-resolution images from fewer pixels.
- âœ… Depth Perception: Uses stereo vision (left & right cameras) to create disparity maps for estimating depth.

### Mid-Level Processing
- Uses **patterns and object features** for recognition.
- Works on **segmented objects** to **classify, track, and detect motion**.
- **Examples**:
  - **Object Detection**: Recognizes and labels objects.
  - **Image Classification**: Assigns categories to images.
  - **Image Matching & Stitching**: Used in **panoramas**.
  - **Object Tracking**: **Predicts movement** using **optical flow**.
  - **Clustering (K-Means)**: Segments objects into groups.

### High-Level Processing
- Involves **complex scene understanding** and **AI-based interpretations**.
- Integrates **low-level features** for **semantic meaning**.
- **Examples**:
  - **Semantic Segmentation**: Classifies pixels into categories (e.g., road, car, tree).
  - **Instance Segmentation**: Differentiates between multiple **objects of the same type**.
  - **Panoptic Segmentation**: Combines **semantic + instance segmentation**.
  - **Deep Learning for Segmentation**: Uses **CNNs & RNNs** for precise object recognition.
  - **Theme Understanding**: Identifies scene context (e.g., **marketplace vs. garden**).
  - **Visual Question Answering (VQA)**: AI interprets **images + text-based queries**.

## Applications of Computer Vision
- **Self-Driving Cars**: Detects lanes, pedestrians, and traffic signs.
- **Facial Recognition**: Matches faces for identity verification.
- **Medical Imaging**: Detects diseases in X-rays & MRIs.
- **Manufacturing**: Identifies **defective products**.
- **Retail & Banking**: Uses **OCR, fraud detection, and customer authentication**.

## Deep Learning for Image Segmentation
ğŸ”¹ Convolutional Neural Networks (CNNs) play a crucial role in image segmentation, enabling computers to understand objects at a pixel level.

How Deep Learning Works for Image Segmentation:
- 1ï¸âƒ£ Feature Extraction with CNNs: CNNs process images and detect key features for segmentation.
- 2ï¸âƒ£ Object Localization with Region Proposal Network (RPN): Identifies potential object locations and generates bounding boxes.
- 3ï¸âƒ£ Refining Features from the Region of Interest (ROI): CNN extracts features within the bounding box.
- 4ï¸âƒ£ Instance Segmentation using Fully Convolutional Networks (FCN): The extracted features are passed into an FCN, which learns to segment the object from its surroundings.
- 5ï¸âƒ£ Binary Mask Output: The FCN produces a binary mask, highlighting which pixels belong to the object and which do not.

ğŸš€ From raw images to object masks, deep learning models enhance precision in image segmentation by combining feature extraction, region detection, and pixel-level classification! ğŸš€

## Difference Between Low-Level and High-Level Features

| **Feature Type** | **Low-Level Features** ğŸ–¼ï¸ | **High-Level Features** ğŸ§  |
|------------------|--------------------------|---------------------------|
| **Content** | Deals with **raw pixel data**, detecting **edges, textures, and simple patterns**. | Focuses on **object understanding, classification, and relationships**. |
| **Sensitivity** | More **sensitive to noise**, lighting, and orientation changes. | More **robust**, can interpret **complex scenes** despite variations. |
| **Scale** | Extracted from **local regions** (e.g., edges in a small part of the image). | **Global features**â€”considers the entire image for **context**. |
| **Resources** | Requires **fewer system resources**, as it focuses on simple features. | Uses **advanced AI models**, requiring **higher computation power**. |
| **Use Cases** | Applied in **image segmentation, feature matching, and object detection**. | Used in **image classification, object recognition, and scene understanding**. |

ğŸ” **Low-level features** lay the foundation by detecting **shapes and patterns**, while **high-level features** bring **context and meaning** to an image. Together, they power modern **computer vision applications**! ğŸš€

## Edge Detection

## Sharpening Spatial Filters
ğŸ–¼ï¸ **Purpose**:
- Removes **blurring** and **enhances edges** in images.
- Highlights **intensity transitions** using **spatial differentiation**.
- **Image gradients** measure the **rate of change in pixel intensity**, crucial for detecting edges.

## Image Gradients
ğŸ“Œ **Fundamental for computer vision & image processing**
ğŸ”¹ Used for:
âœ… **Edge detection**
âœ… **Finding object contours**
âœ… **Outlining shapes**

ğŸ”¹ Computes:
- **Gradient Magnitude** (Strength of the edge)
- **Gradient Orientation** (Direction of the edge)

âœ¨ **Popular techniques built on image gradients**:
- **Histogram of Oriented Gradients (HOG)**
- **Scale-Invariant Feature Transform (SIFT)**

## Edge Detection Using Image Gradients
ğŸ’¡ **Gradient computation is a key pre-processing step** for edge detection.

### Computing Image Gradients
The **gradient of an image** is calculated using finite differences:
- **Gradient along the vertical direction ($G_y$):**
  $$ G_y = f(r+1, c) - f(r-1, c) $$
- **Gradient along the horizontal direction ($G_x$):**
  $$ G_x = f(r, c+1) - f(r, c-1) $$

ğŸ“Œ **Gradient masks (filters) for edge detection:**

| **Filter** | **Mask** |
|------------|-------------|
| **Vertical ($G_y$) Sobel Filter** | \(\begin{bmatrix} -1 & 0 & 1 \end{bmatrix}\) |
| **Horizontal ($G_x$) Sobel Filter** | \(\begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}\) |

## Image Gradient & Gradient Vector
ğŸ–¼ï¸ **Purpose**:
- Computes **rate of change** in pixel intensity.
- **Gradient Magnitude ($M$)** measures the **strength** of intensity change.
- **Gradient Orientation ($\alpha$)** determines **direction** of the edge.
- The **matrix size** for magnitude and angle is the **same** as the original image.

ğŸ”¹ **Mathematical Representation:**
- **Gradient Angle ($\alpha$):**
  $$ \alpha = \tan^{-1} \left(\frac{g_y}{g_x} \right) $$
- **Gradient Magnitude ($M$):**
  $$ M = \sqrt{g_x^2 + g_y^2} $$

ğŸ“Œ **Where:**
- $g_x$ = Gradient in the **horizontal direction**.
- $g_y$ = Gradient in the **vertical direction**.
- $\alpha$ = Angle between the **vertical axis** and the **edge direction**.

## Sobel Filter for Edge Detection
ğŸ’¡ **Sobel filters** compute **image gradients** using convolution masks.

### Gradient Computation with Sobel Operator
ğŸ”¹ **Sobel Operator for Horizontal ($G_x$) and Vertical ($G_y$) Gradients:**

| **Gradient Direction** | **Filter Mask (Kernel)** |
|----------------|-----------------------------------|
| **$G_x$ (Horizontal Gradient)** | $\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$ |
| **$G_y$ (Vertical Gradient)** | $\begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}$ |

ğŸ”¹ **Steps to Compute Edge Strength:**
1ï¸âƒ£ Convolve the **image** with **$G_x$** to compute **horizontal changes**.
2ï¸âƒ£ Convolve the **image** with **$G_y$** to compute **vertical changes**.
3ï¸âƒ£ Compute **gradient magnitude** and **angle** using:
   $$ M = \sqrt{G_x^2 + G_y^2} $$
   $$ \alpha = \tan^{-1} \left(\frac{G_y}{G_x} \right) $$

ğŸ“Œ **Why Sobel Filters?**
âœ… Enhances **edges** by detecting **gradients** in both directions.
âœ… **Smooths noise** while emphasizing high-frequency intensity changes.
âœ… Used in **edge detection algorithms** like **Canny Edge Detector**.

## Gaussian Filter for Image Smoothing
ğŸ”¹ The **Gaussian filter** is a **smoothing filter** that reduces **noise and detail** in an image.
ğŸ”¹ It applies a **Gaussian function** to weight pixels, giving higher importance to the center pixel and gradually reducing weights outward.

### Mathematical Representation
The **Gaussian function** for a 2D image is:
$$ G_\sigma(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}} $$
where:
- \( \sigma \) = Standard deviation (controls blurring strength)
- \( x, y \) = Pixel coordinates

ğŸ“Œ **Higher \( \sigma \) â†’ More blur**
ğŸ“Œ **Lower \( \sigma \) â†’ Less blur, preserves more details**

### Gaussian Filter Kernels
| **Filter Size** | **Kernel Matrix (Normalized)** |
|---------------|----------------------------|
| **3Ã—3 (Ïƒ = 1)** | \( \frac{1}{16} \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \) |
| **5Ã—5 (Ïƒ = 1)** | \( \frac{1}{330} \begin{bmatrix} 1 & 4 & 7 & 4 & 1 \\ 4 & 20 & 33 & 20 & 4 \\ 7 & 33 & 54 & 33 & 7 \\ 4 & 20 & 33 & 20 & 4 \\ 1 & 4 & 7 & 4 & 1 \end{bmatrix} \) |
| **5Ã—5 (Ïƒ = 2)** | \( \frac{1}{34} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 2 & 2 & 2 & 1 \\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \) |

ğŸ“Œ **Larger filter size â†’ More blur & smoothing**
ğŸ“Œ **Smaller filter size â†’ Preserves more details**

## Laplacian Filter for Edge Detection
ğŸ”¹ The **Laplacian filter** is a **second-order derivative filter** used for **edge detection**.
ğŸ”¹ It highlights regions of **rapid intensity change** by computing the **second derivative** of an image.

### Mathematical Representation
The Laplacian operator is given by:

\[ \nabla^2 f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\]

where \( f(x, y) \) is the image intensity at a given point.

### Laplacian Filter Kernels
| **Filter Type** | **Kernel Matrix** |
|----------------|------------------|
| **4-Neighbor Laplacian** | \( \begin{bmatrix} 0 & -1 & 0 \\ -1 & 4 & -1 \\ 0 & -1 & 0 \end{bmatrix} \) |
| **8-Neighbor Laplacian** | \( \begin{bmatrix} -1 & -1 & -1 \\ -1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix} \) |

ğŸ”¹ The **4-neighbor Laplacian** considers only direct neighbors, while the **8-neighbor Laplacian** accounts for diagonal edges as well.