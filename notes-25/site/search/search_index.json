{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#notes-under-construction","title":"Notes Under Construction","text":"<p>The content on this page is currently under construction. Please check back later for more updates!</p>"},{"location":"RL-Unit%201/","title":"Reinforcement Learning","text":""},{"location":"RL-Unit%201/#machine-learning-types","title":"Machine Learning Types","text":"<pre><code>graph LR\n    subgraph Input\n    A[Data with Labels] --&gt; B[Supervised Learning]\n    C[Data without Labels] --&gt; D[Unsupervised Learning]\n    E[States + Actions] --&gt; F[Reinforcement Learning]\n    end\n\n    B --&gt; G[Mapping]\n    D --&gt; H[Classes]\n    F --&gt; I[Action]\n\n    B -.-&gt;|Error| B\n    F -.-&gt;|Reward| F\n\n\n</code></pre>"},{"location":"RL-Unit%201/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Input: Labeled data (features + target labels)</li> <li>Process: Learning from labeled examples</li> <li>Output: Prediction model</li> <li>Feedback: Error measurement against known labels</li> <li>Applications: </li> <li>Classification (spam detection, image recognition)</li> <li>Regression (price prediction, sales forecasting)</li> </ul>"},{"location":"RL-Unit%201/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Input: Unlabeled data</li> <li>Process: Pattern/structure discovery</li> <li>Output: Data grouping/structure</li> <li>Feedback: Internal validation metrics</li> <li>Applications:</li> <li>Clustering (customer segmentation)</li> <li>Dimensionality reduction (feature extraction)</li> </ul>"},{"location":"RL-Unit%201/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Input: States + possible actions</li> <li>Process: Trial and error learning</li> <li>Output: Action policy</li> <li>Feedback: Rewards/penalties</li> <li>Applications:</li> <li>Game AI (chess, Go)</li> <li>Robotics (navigation, control)</li> </ul>"},{"location":"RL-Unit%201/#common-algorithms","title":"Common Algorithms","text":"<ul> <li>Supervised: Linear Regression, Random Forest, Neural Networks</li> <li>Unsupervised: K-means, PCA, Autoencoders</li> <li>Reinforcement: Q-Learning, Policy Gradient, DQN</li> </ul>"},{"location":"RL-Unit%201/#reinforcement-learning_1","title":"Reinforcement Learning:","text":"<ul> <li>What to do</li> <li>How to map situations to actions</li> <li>Maximizing a numerical reward signal</li> </ul> <p>Reinforcement learning is an autonomous, self-teaching system that essentially learns by trial and error. It performs actions with the aim of maximizing rewards, or in other words, it is learning by doing in order to achieve the best outcomes.</p> <p>Reinforcement Learning is a feedback-based Machine learning technique in which an agent learns to behave in an environment by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty</p>"},{"location":"RL-Unit%201/#key-characteristics","title":"Key Characteristics","text":"<p>Reinforcement Learning is inspired by how humans and animals learn through interactions:</p> <ul> <li>Reward and Punishment: Encourages repeating actions that lead to rewards and avoiding punishments.</li> <li>Trial and Error: Similar to trying different methods until the correct one is found.</li> <li>Learning Over Time: Improvement occurs through continuous experience.</li> <li> <p>Rewards Come from a Sequence of Actions.</p> </li> <li> <p>The learner is not told which actions to take but must discover them through trial and error.</p> </li> <li>Actions affect not only the immediate reward but also future situations and rewards.</li> <li>Works well in problems where a sequence of decisions is important.</li> </ul> <p>RL is an autonomous, self-teaching system that learns by trial and error. The goal is to maximize rewards over time.</p> <p>Example Applications:     - Chess     - Maze solving     - Industrial robot arms     - Path planning     - Sweeper robots</p>"},{"location":"RL-Unit%201/#how-rl-differs-from-supervised-learning","title":"How RL Differs from Supervised Learning","text":"Feature Supervised Learning Reinforcement Learning Training Data Has labeled answers No labeled answers; learns from experience Decision Making Independent of past decisions Dependent on past decisions Learning Method Trained with a dataset Learns through trial and error"},{"location":"RL-Unit%201/#elements-of-rl","title":"Elements of RL","text":"<p>![[Pasted image 20250131000424.png|500]]</p>"},{"location":"RL-Unit%201/#agent","title":"Agent","text":"<ul> <li>Definition: An entity that interacts with the environment.</li> <li>Examples: Robot, human, software program.</li> </ul>"},{"location":"RL-Unit%201/#environment","title":"Environment","text":"<ul> <li>Definition: The external system in which the agent operates.</li> <li>Examples: Physical world, game simulation.</li> </ul>"},{"location":"RL-Unit%201/#learning-process","title":"Learning Process","text":"<ol> <li>The agent moves from the initial state to the goal state.</li> <li>The agent continually asks, \"What is the best action in each state?\"</li> </ol>"},{"location":"RL-Unit%201/#advantages-of-reinforcement-learning","title":"Advantages of Reinforcement Learning","text":"<p>\u2705 No need for predefined instructions or human intervention.  \u2705 Can adapt to both static and dynamic environments.  \u2705 Solves a wide range of problems (decision-making, prediction, optimization).  \u2705 Improves with experience and fine-tunes over time.</p>"},{"location":"RL-Unit%201/#disadvantages-of-reinforcement-learning","title":"Disadvantages of Reinforcement Learning","text":"<p>\u274c Performance depends on the quality of the reward function.  \u274c Designing and tuning RL models can be complex.</p> <p>[!NOTE]</p>"},{"location":"RL-Unit%201/#when-to-apply-reinforcement-learning","title":"When to Apply Reinforcement Learning?","text":"<p>Reinforcement Learning is most suitable when: - The problem environment is complex and uncertain, making traditional programming methods ineffective. - Feedback is sparse, delayed, and dependent on multiple decisions. - Decision-making (actions) follows a feedback loop.</p>"},{"location":"RL-Unit%201/#why-is-reinforcement-learning-difficult","title":"Why Is Reinforcement Learning Difficult?","text":"<p>The toughest parts of Reinforcement Learning are: - Mapping the Environment. - Including All Possible Actions.</p>"},{"location":"RL-Unit%201/#core-concepts","title":"Core Concepts","text":"<ul> <li>Goal-Oriented Learning: The agent learns by trying to achieve a goal.</li> <li>Learning from Consequences: The agent learns from the consequences of its actions.</li> <li>Active Research Area: RL is one of the most active fields in Artificial Intelligence (AI).</li> </ul>"},{"location":"RL-Unit%201/#rl-algorithm-steps","title":"RL Algorithm Steps","text":"<pre><code>graph TD;\n    A[Agent Observes Environment] --&gt; B[Agent Performs Action];\n    B --&gt; C[Agent Moves to New State];\n    C --&gt; D[Agent Receives Reward];\n    D --&gt; E[Agent Evaluates Action - Good or Bad];\n    E --&gt; F[Agent Adjusts Strategy to Maximize Reward];\n\n\n</code></pre>"},{"location":"RL-Unit%201/#learning-and-planning","title":"Learning and Planning","text":""},{"location":"RL-Unit%201/#two-fundamental-problems-in-sequential-decision-making","title":"Two Fundamental Problems in Sequential Decision Making","text":""},{"location":"RL-Unit%201/#reinforcement-learning-rl","title":"Reinforcement Learning (RL):","text":"<ul> <li>The environment is initially unknown.</li> <li>The agent interacts with the environment.<ul> <li>The agent improves its policy.</li> </ul> </li> </ul>"},{"location":"RL-Unit%201/#planning","title":"Planning:","text":"<ul> <li>A model of the environment is known.</li> <li>The agent performs computations with its model (without any external interaction).</li> <li>The agent improves its policy, also known as deliberation, reasoning, introspection, pondering, thought, search.</li> </ul>"},{"location":"RL-Unit%201/#model-of-the-environment","title":"Model of the Environment:","text":"<ul> <li>A model mimics the behavior of the environment. With the help of the model, one can make inferences about how the environment will behave. For example, if a state and an action are given, the model can predict the next state and reward.</li> <li>The model is used for planning, providing a way to take a course of action by considering all future situations before actually experiencing those situations.</li> <li>Approaches for solving RL problems with the help of the model are termed model-based approach.</li> <li>An approach without using a model is called a model-free approach.</li> </ul> <p>![[Pasted image 20250130232904.png]]</p>"},{"location":"RL-Unit%201/#types-of-reinforcement-learning-algorithms-on-the-basis-of-model-based","title":"Types of Reinforcement Learning Algorithms ( on the basis of model based)","text":"<p>There are various algorithms used in reinforcement learning such as Q-learning, policy gradient methods, Monte Carlo method and many more. All these algorithms can be classified into two broad categories - </p>"},{"location":"RL-Unit%201/#model-free-reinforcement-learning","title":"Model-free Reinforcement Learning :","text":"<ul> <li>It is a category of reinforcement learning algorithms that learns to make decisions by</li> <li>interacting with the environment directly, without creating a model of the environment's</li> <li>dynamics.</li> <li>The agent performs different actions multiple times to learn the outcomes and creates a</li> <li>strategy (policy) that optimizes its reward points. This is ideal for changing, large or complex</li> <li>environments.</li> <li>Not applicable for some scenario like self driving car.</li> </ul>"},{"location":"RL-Unit%201/#model-based-reinforcement-learning","title":"Model-based Reinforcement Learning:","text":"<ul> <li>This category of reinforcement learning algorithms involves creating a model of the environment's dynamics to make decisions and improve performance.</li> <li>Ideal for environments that are static and well-defined, where real-world environment testing is difficult.</li> </ul>"},{"location":"RL-Unit%201/#key-differences-between-model-free-and-model-based-reinforcement-learning","title":"Key Differences Between Model-free and Model-based Reinforcement Learning","text":"Feature Model-Free RL Model-Based RL Learning Approach Direct learning from environment Indirect learning through model building Efficiency Requires more real-world interactions More sample-efficient Complexity Simpler implementation More complex due to model learning Environment Utilization No internal model Builds and uses a model Adaptability Slower to adapt to changes Faster adaptation with accurate model Computational Requirements Less intensive More computational resources needed Examples Q-Learning, SARSA, DQN, PPO Dyna-Q, Model-Based Value Iteration"},{"location":"RL-Unit%201/#rl-framework-the-rl-process-a-loop-of-state-action-reward-and-next-state","title":"RL Framework - The RL Process: A Loop of State, Action, Reward, and Next State","text":"<p>![[Pasted image 20250130232937.png]]</p>"},{"location":"RL-Unit%201/#main-characteristics-of-rl","title":"Main Characteristics of RL","text":"<ul> <li>No supervisor while training.</li> <li>Environment is generally stochastic for real-world applications.</li> <li>Model of the environment can be incomplete.</li> <li>Feedback (Negative/Positive Reward) can be delayed or partial.</li> <li>The agent uses experience from the past to improve its performance over time.</li> <li>Actions that have fetched more rewards are preferred.</li> <li>The agent tries various actions and prefers those that are best or have fetched more rewards.</li> <li>RL uses Markov Decision Process (MDP) framework to define the interaction between a learning agent and its environment.</li> </ul>"},{"location":"RL-Unit%201/#reinforcement-learning-rl-problem-challenges-in-rl","title":"Reinforcement Learning (RL) Problem - Challenges in RL","text":""},{"location":"RL-Unit%201/#trade-off-between-exploration-and-exploitation","title":"Trade-off between Exploration and Exploitation:","text":"<ul> <li>To obtain rewards, an RL agent must prefer actions that it has tried in the past and found effective (Exploit).</li> <li>However, to discover such actions, it must try actions it has not selected before (Explore).</li> </ul> <p>[!NOTE] Neither exploration nor exploitation can be pursued exclusively without failing at the task. </p>"},{"location":"RL-Unit%201/#fundamental-components-of-rl","title":"Fundamental Components of RL","text":"<ul> <li>Policy: Defines the agent\u2019s behavior.</li> <li>Reward Function: Provides feedback on actions.</li> <li>Value Function: Evaluates future rewards.</li> <li>Model of the Environment: Simulates how the environment works.</li> </ul>"},{"location":"RL-Unit%201/#policy","title":"Policy:","text":"<p>A policy is a strategy or set of rules that defines the actions the agent should take in a given state.</p> <ul> <li>The policy can be deterministic (one action for a state) or stochastic (probabilistic actions for a state).</li> <li>The goal is to find an optimal policy that maximizes the total expected reward.</li> </ul> <p>Example:</p> <ul> <li>A robot navigating a maze may follow a policy that says, \"Always turn left unless there's an obstacle, then turn right.\"</li> </ul>"},{"location":"RL-Unit%201/#human-analogy","title":"Human Analogy:","text":"<ul> <li>A policy is like a person's habit or plan of action, such as the decision to exercise every morning or take an umbrella when it's cloudy.</li> </ul>"},{"location":"RL-Unit%201/#value-function","title":"Value function:","text":"<p>Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.  - Rewards determine the immediate, intrinsic desirability of environmental states.  - Values indicate the long-term desirability of states after considering the states likely to follow and the rewards available in those states.  - Example: - A state might always yield a low immediate reward but still have a high value because it is followed by states that yield high rewards. </p>"},{"location":"RL-Unit%201/#human-analog","title":"Human Analog","text":"<ul> <li>Rewards are somewhat like kepleasureur (if high) and ndpainai (if low). - Values correspond to a more rerefined and farsighted judgmenten of how pleased or displeased we are by the environment.nt.</li> </ul>"},{"location":"RL-Unit%201/#reward-function","title":"Reward Function:","text":"<p>The reward function provides feedback on the actions the agent takes, indicating whether an action was good or bad.</p> <ul> <li>It assigns a numeric value to the agent's actions, which the agent uses to evaluate the desirability of its actions in a given state.</li> <li>The goal of the agent is to maximize the cumulative reward over time.</li> </ul> <p>Example:</p> <ul> <li>In a game, winning a round might give a reward of +10, while losing gives a reward of -1.</li> </ul>"},{"location":"RL-Unit%201/#human-analogy_1","title":"Human Analogy:","text":"<ul> <li>The reward function is like the feedback a person gets from their actions, such as feeling happy after a good deed or guilty after a mistake.</li> </ul>"},{"location":"RL-Unit%201/#model-of-the-environment_1","title":"Model of the Environment:","text":"<p>The model of the environment simulates how the environment behaves, helping the agent predict the outcomes of actions.</p> <ul> <li>This model can be used for planning future actions by simulating potential outcomes.</li> <li>A model-free approach directly learns from experience, while a model-based approach uses a model to predict actions' results before performing them.</li> </ul> <p>Example:</p> <ul> <li>A self-driving car may use a model to simulate various driving scenarios and plan its route accordingly.</li> </ul>"},{"location":"RL-Unit%201/#human-analogy_2","title":"Human Analogy:","text":"<ul> <li>The model of the environment is like a mental map that a person forms, which helps them predict the likely outcomes of their actions, such as deciding to avoid a route with heavy traffic.</li> </ul>"},{"location":"RL-Unit%201/#final-element-of-rl-systems-model-of-the-environment","title":"Final Element of RL Systems: Model of the Environment","text":"<ul> <li>The model mimics the behavior of the environment, allowing inferences to be made about how the environment will behave.</li> <li>For example, given a state and action, the model might predict the resultant next state and next reward.</li> <li> <p>Models are used for planning, helping the agent decide on a course of action by considering future situations before they are experienced.</p> </li> <li> <p>Methods for solving RL problems that use models and planning are called model-based methods, while simpler trial-and-error learning methods are called model-free methods.</p> </li> </ul>"},{"location":"RL-Unit%201/#types-of-reinforcement-learning","title":"Types of Reinforcement Learning","text":"<p>There are three main types of Reinforcement Learning (RL): - Value-Based - Policy-Based - Model-Based</p> <p>Each approach has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem you are trying to solve.</p>"},{"location":"RL-Unit%201/#value-based-reinforcement-learning","title":"Value-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns to estimate the value of each state or action based on the rewards it receives.</li> <li>This value is known as Q-values.</li> <li>The agent then selects the actions with the highest Q-value in each state to maximize its long-term reward.</li> <li>The most commonly used algorithm for value-based reinforcement learning is Q-learning.</li> </ul>"},{"location":"RL-Unit%201/#policy-based-reinforcement-learning","title":"Policy-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns an optimal policy, which is a mapping from states to actions, without calculating the value function.</li> <li>The policy is updated based on the rewards received by the agent, with the goal of maximizing the expected reward over time.</li> <li>The most common algorithm used for policy-based reinforcement learning is the REINFORCE algorithm.</li> </ul>"},{"location":"RL-Unit%201/#model-based-reinforcement-learning_1","title":"Model-Based Reinforcement Learning","text":"<ul> <li>In this approach, the agent learns a model of the environment, which it can use to simulate different scenarios and plan its actions accordingly.</li> <li>The model can learn through supervised or unsupervised learning, and the agent can use it to predict the outcome of its actions before taking them.</li> <li>The most common model-based reinforcement learning algorithm is the Dyna algorithm.</li> </ul>"}]}